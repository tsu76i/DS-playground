{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdaec77d",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Frequency-Based Word Embeddings](#2-frequency-based-word-embeddings)\n",
    "    - [One-Hot Encoding](#one-hot-encoding)\n",
    "    - [Bag of Words (BoW)](#bag-of-words-bow)\n",
    "    - [Term Frequency-Inverse Document Frequency (TF-IDF)](#term-frequency-inverse-document-frequency-tf-idf)\n",
    "    - [Global Vectors for Word Representation (GloVe)](#global-vectors-for-word-representation-glove)\n",
    "        - [Co-Occurence Probability Ratio](#co-occurrence-probability-ratio)\n",
    "        - [Loss Function](#loss-function)\n",
    "        - [Weighting Function](#weighting-function)\n",
    "3. [Prediction-Based Word Embeddings](#3-prediction-based-word-embeddings)\n",
    "    - [Word2Vec](#word2vec)\n",
    "        - [Continuous Bag of Words (CBOW)](#continuous-bag-of-words-cbow)\n",
    "        - [Skip-Gram](#skip-gram)\n",
    "    - [fastText](#fasttext)\n",
    "4. [Contextual-Based Word Embeddings](#4-contextual-based-word-embeddings)\n",
    "    - [Bidirectional Encoder Representations from Transformers (BERT)](#bidirectional-encoder-representations-from-transformers-bert)\n",
    "    - [Generative Pre-trained Transformer (GPT)](#generative-pre-trained-transformer-gpt)\n",
    "    - [Embeddings from Language Models (ELMo)](#embeddings-from-language-models-elmo)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d592bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c4468",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Vectorisation in Natural Language Processing (NLP) is the process of converting text into numerical representations (vectors) that machine learning models can process and analyse natural language data. \n",
    "\n",
    "Word embeddings are a specific type of vectorisation technique that represents words as dense vectors in a continuous, high-dimensional space. Unlike basic vectorisation methods, word embeddings are designed to reflext semantic and syntactic relationships between words. Hence, words with similar meanings or usage contexts are mapped to vectors that are close together in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496f9f6",
   "metadata": {},
   "source": [
    "## 2. Frequency-Based Word Embeddings\n",
    "Frequency-based (or count-based) word embeddings represent words using statistics about their occurrences and co-occurrences in a cprpus. These methods do not use neural networks or prediction tasks. Instead, they rely on explicit counts and matrix manipulations to derive word vectors. Though they are typically simple, interpretable and easy to parallelise, they are unable to capture semantic similarity well.\n",
    "\n",
    "### One-Hot Encoding\n",
    "Each word in the vocabulary is represented by a sparse binary vector with a single $1$ at the position corresponding to the word and $0$ elsewhere. It does not capture semantic similarity between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27171d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "corpus = ['apple', 'banana', 'cherry', 'blueberry', 'apple']  # Sample\n",
    "\n",
    "corpus_reshaped = np.array(corpus).reshape(-1, 1)  # Reshape\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)  # Initialisation\n",
    "\n",
    "one_hot_encoded_corpus = ohe.fit_transform(corpus_reshaped)  # One Hot Encode\n",
    "\n",
    "print(one_hot_encoded_corpus)  # Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6154b0",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)\n",
    "Bag of Words is a simple technique that represents a document by the frequency of each word in the vocabulary, ignoring grammar and word order. Each document is a vector of word counts. We can implement BoW using `CountVectorizer` method from scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0e9917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]  # Sample\n",
    "\n",
    "vectoriser = CountVectorizer()  # Initialisation\n",
    "X = vectoriser.fit_transform(corpus)  # Apply CountVectorizer()\n",
    "\n",
    "print(vectoriser.get_feature_names_out())  # Output (features)\n",
    "print(X.toarray())  # Output (BoW matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55adddf",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) extends BoW by weighting each word based on their **frequency** in a document (TF) and their **rarity** across all documents (IDF). This helps highlight important words while reducing the influence of common ones such as 'the' or 'and'. The scikit-learn library provides `TfidfVectorizer` method to implement TF-IDF.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "$t$: Speficic term (word) being evaluated\n",
    "$d$: Single document within the corpus\n",
    "$D$: Set of all documents\n",
    "\n",
    "\n",
    "- **Term Frequency (TF)**: Measures how often term $t$ appears in document $d$, normalised by the document's length.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{TF}(t, d) = \\dfrac{\\text{Number of times } t \\text{ appears in } d}{\\text{Total terms in } d}\n",
    "\\end{align*}\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**: Penalises terms common across many documents.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IDF}(t, D) = \\text{log} \\left(\\dfrac{\\text{Total documents in corpus} (N)}{\\text{Documents containing } t} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "or, **smoothed IDF** (used in scikit-learn to avoid division by zero):\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IDF}(t, D) = \\text{log} \\left(\\dfrac{N + 1}{\\text{Documents containing } t + 1} \\right) + 1\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "*Example*: If the word 'vector' appears 8 times in a 200-word document, within 50 among 10000 documents:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{TF}(8, 200) = \\dfrac{8}{200} = 0.04\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IDF}(50, 10000) = \\text{log} \\left(\\dfrac{10000}{50}\\right) = \\text{log}(200) \\approx{2.30}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{TF-IDF} = \\text{TF} \\times \\text{IDF} = 0.04 \\times 2.30 = 0.092\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bdad7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]  # Sample\n",
    "\n",
    "vectoriser = TfidfVectorizer()  # Initialisation\n",
    "X = vectoriser.fit_transform(corpus)  # Apply TfidfVectorizer()\n",
    "\n",
    "print(vectoriser.get_feature_names_out())  # Output (features)\n",
    "print(X.toarray())  # Output (TF-IDF matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0fce8f",
   "metadata": {},
   "source": [
    "### Global Vectors for Word Representation (GloVe)\n",
    "Global Vectors for Word Representation (GloVe) is an unsupervised learning algorithm developed by Stanford to obtain vector representations of words. GloVe is a count-based method that leverages global word co-occurence statistics from a corpus. The key idea is that ratios of word-word co-occurence probabilities encode meaning, and by factorising a co-occurence matrix, GloVe learns word vectors such that word similarities and analogies are preserved.\n",
    "\n",
    "#### Co-Occurrence Probability Ratio\n",
    "GloVe uses the ratio $P_{ik}/P_{jk}$ of co-occurrence probabilities to capture semantic relationships, where: \\\n",
    "$ P_{ik} + \\dfrac{X_{ik}}{X_{i}}$ (probability of word $k$ occurring in context of word $i$).\n",
    "\n",
    "#### Loss Function\n",
    "The objective of GloVe is to minimise:\n",
    "\\begin{align*}\n",
    "J = \\sum_{i,j=1}^{V} f(X_{ij}) (w^{T}_{i} \\tilde w_{j} + b_{i} + \\tilde b_{j} - \\text{log} X_{ij})^2\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $w_{i}$, $\\tilde w_{j}$: Word and context vectors.\n",
    "- $X_{ij}$: Co-occurrence count of words $i$ and $j$.\n",
    "- $f(X_{ij})$: Weighting function that discounts frequent co-occurences.\n",
    "\n",
    "\n",
    "#### Weighting Function\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    " f(x) & = \\begin{cases}\n",
    "      \\left(\\dfrac{x}{x_{max}}\\right)^\\alpha   &   \\text{if } x < x_{max}     \\\\\n",
    "      1 &   \\text{otherwise}     \\\\\n",
    "      \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "$\\alpha$ is a hyperparameter, typically $=0.75$, and $x_{max} = 100$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99310e26",
   "metadata": {},
   "source": [
    "The developers of GloVe provide pre-trained word vectors trained on large corpora on their [official web page](https://nlp.stanford.edu/projects/glove/). \n",
    "A part of the following code was retrieved from [Spot Intelligence](https://spotintelligence.com/2023/11/27/glove-embedding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d24e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings into a dictionary\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14ddc123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings_path = '_datasets/glove.6B.100d.txt'\n",
    "glove_embeddings = load_embeddings(glove_embeddings_path)\n",
    "len(glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e66d61ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'wikipedia': [-0.2536    0.21885   0.53349  -0.52444   0.53655   0.62448  -0.12989\n",
      " -0.83826   0.89195   0.033484  0.42016   0.44988   0.094579 -0.92764\n",
      " -0.48991   0.75895   0.48858  -0.57347  -0.75298   0.53346  -0.72722\n",
      "  0.41164   0.049068  0.59324   0.028872 -1.4469    0.072449 -0.051847\n",
      "  0.36257   0.1662    0.022671  1.263    -0.634    -0.72939   0.29486\n",
      "  0.41603  -0.40253  -0.21218  -0.71229  -0.04464  -0.80034   0.83279\n",
      " -0.24826   0.61856  -0.26476   0.38703  -0.026548 -0.85908   0.34218\n",
      "  0.28381   0.79504   0.78182  -0.81676  -0.023553 -1.4282   -0.065081\n",
      " -0.36143  -0.38418   0.49508  -0.079691 -0.21495   0.3556   -0.55288\n",
      " -0.14088   1.3684    0.29986  -0.051735 -0.27049   0.65376  -0.31637\n",
      "  0.28904   1.4105    0.90976  -0.22609  -0.31961   0.036672  0.99641\n",
      "  0.50815  -0.35471  -0.56741  -0.58292  -0.41092   0.28246  -0.31194\n",
      " -0.50438  -0.1069    0.080875 -0.75075  -0.087019  0.22302   0.011673\n",
      "  0.70839   0.014801 -0.29071   1.0279   -0.27078  -0.17947  -0.66059\n",
      "  0.31799   0.32955 ]\n"
     ]
    }
   ],
   "source": [
    "# Accessing word embeddings\n",
    "word = 'wikipedia'\n",
    "if word in glove_embeddings:\n",
    "    embedding = glove_embeddings[word]\n",
    "    print(f\"Embedding for '{word}': {embedding}\")\n",
    "else:\n",
    "    print(f\"'{word}' not found in embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa9c25",
   "metadata": {},
   "source": [
    "For GloVe algorithm (and most word emedding algorithms), similar words are placed close together in the vector space. Thus, the standard way to measure how close (similar) the two words are is by computing cosine distance. Low value ($\\approx 0$) means high similarity, high value ($\\approx 2$) means high dissimilarity.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Cosine Distance} = 1 - \\text{Cosine Similarity}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05373589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.7508\n"
     ]
    }
   ],
   "source": [
    "# Finding similarity between word embeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "word1 = 'king'\n",
    "word2 = 'queen'\n",
    "similarity = 1 - cosine(glove_embeddings[word1], glove_embeddings[word2])\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acc75a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'bird' and 'seagull': 0.0718\n"
     ]
    }
   ],
   "source": [
    "word1 = 'bird'\n",
    "word2 = 'seagull'\n",
    "similarity = 1 - cosine(glove_embeddings[word1], glove_embeddings[word2])\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26861562",
   "metadata": {},
   "source": [
    "## 3. Prediction-Based Word Embeddings\n",
    "Prediction-based models are methods for learning word embeddings by predicting contextual relationships between words. These models rely on neural networks to capture the semantic and syntactic relationships between words based on their usage in a large corpus. The key idea is to learn dense, low-dimensional vectors for words by optimising a prediction task, such as:\n",
    "\n",
    "- Predicting a word given its context.\n",
    "\n",
    "- Predicting the context given a word.\n",
    "\n",
    "\n",
    "### Word2Vec\n",
    "Word2Vec is a family of neural network models that learn word embeddings by leveraging the distributional hypothesis: *words appearing in similar contexts tend to have similar mearning*. Word2Vec is designed to learn high-quality word embeddings by using two architectures:\n",
    "\n",
    "- **Continuous Bag of Words (CBOW)**: Predicts a target word from its surrounding context words.\n",
    "\n",
    "- **Skip-gram**: Predicts context words from a given target word.\n",
    "\n",
    "Both models use a shallow neural network with one hidden layer, and are trained on large text corpora to optimise word vectors such that words appearing in similar contexts have similar embeddings.\n",
    "\n",
    "#### Continuous Bag of Words (CBOW)\n",
    "Continuous Bag of Words (CBOW) is a Word2Vec architecture where the model predicts a target (centre) word $w_{t}$ based on the context words $w_{t-n}, \\cdots , w_{t-1}, w_{t+1}, \\cdots, w_{t+n}$. For example, given the context ['the', 'cat', 'on', 'the'], the model predicts the center word 'sat' in the phrase 'the cat sat on the mat'.\n",
    "\n",
    "- **Hidden Layer**:\n",
    "If $v_{w_{i}}$ is the embedding for context word $w_{i}$, the hidden layer output is:\n",
    "\n",
    "\\begin{align*}\n",
    "h = \\dfrac{1}{2m} \\sum_{-n \\leq j \\leq n, l \\neq 0} v_{w_{t+l}}\n",
    "\\end{align*}\n",
    "\n",
    "- **Activation Function (Softmax)**:\n",
    "The probability of prediction the centre word $w_{t}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(w_{t}| \\text{context}) = \\dfrac{\\text{exp}(u^{T}_{w_{t}}h)}{\\sum_{w \\in V} \\text{exp} (u^{T}_{w}h)}\n",
    "\\end{align*}\n",
    "\n",
    "- **Objective Function**:\n",
    "The objective is to maximise the log-likelihood over the corpus:\n",
    "\n",
    "\\begin{align*}\n",
    "J = \\dfrac{1}{T} \\sum_{t=1}^{T} \\text{log } p(w_{t}|w_{t-n}, \\cdots , w_{t-1}, w_{t+1}, \\cdots, w_{t+n})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "#### Skip-Gram\n",
    "Skip-Gram predicts context words given a centre word. For each word in the corpus, the model tries to predict the words within a window around it. Let the centre word be $w_{c}$, and context words $w_{O}$ within window size $n$:\n",
    "\n",
    "- **Hidden Layer**:\n",
    "The hidden layer output is the embedding vector $v_{w_{c}}$.\n",
    "\n",
    "- **Output Layer (Softmax)**:\n",
    "The probability of a context word $w_{O}$ given the centre word $w_{c}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "p(w_{O}|w_{c}) = \\dfrac{\\text{exp}(u^{⊤}_{w_{O}}v_{w_{c}})}{\\sum_{w \\in V} \\text{exp} (u^{⊤}_{w}v_{w_{c}})}\n",
    "\\end{align*}\n",
    "\n",
    "- **Objective Function**:\n",
    "The Skip-Gram model maximises the log probability of the context words given the centre word:\n",
    "\n",
    "\\begin{align*}\n",
    "J = \\dfrac{1}{T} \\sum_{t=1}^{T} \\sum_{-n \\leq j \\leq n, l \\neq 0} \\text{log } p(w_{t+j}|w_{t})\n",
    "\\end{align*}\n",
    "\n",
    "where $T$ is the number of words in the corpus.\n",
    "\n",
    "\n",
    "spaCy’s large (`en_core_web_lg`) and medium (`en_core_web_md`) models include 300-dimensional word vectors trained on massive corpora using Word2Vec-like algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7472ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # Load the large English model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00310689",
   "metadata": {},
   "source": [
    "Similarity between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b06a579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between two words: 0.7253\n"
     ]
    }
   ],
   "source": [
    "token_1 = nlp('king')[0]\n",
    "token_2 = nlp('queen')[0]\n",
    "print(f'Similarity between two words: {token_1.similarity(token_2):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f97d0d",
   "metadata": {},
   "source": [
    "Similarity between sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f85f09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between two sentences: 0.9074\n"
     ]
    }
   ],
   "source": [
    "doc_1 = nlp(\"The cat sat on the mat.\")\n",
    "doc_2 = nlp(\"A dog rested on the rug.\")\n",
    "print(f'Similarity between two sentences: {doc_1.similarity(doc_2):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18ebc9",
   "metadata": {},
   "source": [
    "### fastText\n",
    "fastText is an open-source library developed by Facebook's AI Research (FAIR) team for efficient learning of word representations (embeddings) and text classification in natural language processing. fastText operates at the subword level by breaking words into character n-grams (subwords), allowing it to capture morphological information and handle out-of-vocabulary (OOV) words more effectively. It incorporates two key techniques, Hierarchical Softmax and Negative Sampling to optimise training efficiency:\n",
    "\n",
    "- **Hierarchical Softmax**:\n",
    "Instead of computing the probability distribution over all possible words, hierarchical softmax organises the vocabulary into a binary tree (often a Huffman tree). Each word becomes a leaf node in this tree.\n",
    "\n",
    "- **Negative Sampling**:\n",
    "Negative sampling updates the model with respect to only a few negative (incorrect) samples per training instance for a faster training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f9490aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  62\n",
      "Number of labels: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'prince': [(0.9920476078987122, 'princess'), (0.970518171787262, 'throne'), (0.969997763633728, 'queen')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:   95002 lr:  0.000000 avg.loss:  4.118048 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import tempfile\n",
    "\n",
    "sample_data = \"\"\"king queen prince princess royal throne monarch emperor empress\n",
    "apple fruit banana orange peel citrus\n",
    "car vehicle bike truck highway road\n",
    "dog cat animal pet wolf fox\n",
    "paris france berlin germany london rome madrid\n",
    "computer laptop tablet smartphone device\n",
    "river lake ocean sea pond bay\n",
    "happy sad angry joyful excited\n",
    "run walk jump swim fly\n",
    "red blue green yellow purple violet\"\"\"\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode='w') as f:\n",
    "    f.write(sample_data)\n",
    "    f.flush()\n",
    "    model = fasttext.train_unsupervised(f.name, minCount=1, epoch=1000)\n",
    "\n",
    "similar = model.get_nearest_neighbors('prince', k=3)\n",
    "print(\"Most similar words to 'prince':\", similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30857444",
   "metadata": {},
   "source": [
    "## 4. Contextual-Based Word Embeddings\n",
    "Contextual-Based Word Embeddings are representations of words in a high-dimensional space where the meaning of a word depends on its context in a sentence. Unlike traditional word embeddings, contextual embeddings capture the semantic nuances of words based on the surrounding text. For example, the word 'bank' in 'river bank' and 'bank account' will have different embeddings because their meanings differ based on context. This dynamic nature allows for a more nuanced and accurate understanding of language, which is crucial for advanced NLP tasks such as sentiment analysis, machine translation, and information extraction.\n",
    "\n",
    "They are typically generated using deep learning models such as recurrent neural network (RNNs), Transformers, or pre-trained language models (BERT, GPT, RoBERTa, etc.).\n",
    "\n",
    "\n",
    "### Bidirectional Encoder Representations from Transformers (BERT)\n",
    "Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based language model developed by Google. It is pre-trained using Masked Language Modelling (MLM) and Next Sentence Prediction (NSP). BERT can capture contexts from both directions and understand the relationships between words.\n",
    "\n",
    "- **Applications**: Question answering, text classification, named entitiy recognition, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c46f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS]\tEmbedding shape: torch.Size([768])\n",
      "Token: the\tEmbedding shape: torch.Size([768])\n",
      "Token: bank\tEmbedding shape: torch.Size([768])\n",
      "Token: will\tEmbedding shape: torch.Size([768])\n",
      "Token: not\tEmbedding shape: torch.Size([768])\n",
      "Token: lend\tEmbedding shape: torch.Size([768])\n",
      "Token: money\tEmbedding shape: torch.Size([768])\n",
      "Token: to\tEmbedding shape: torch.Size([768])\n",
      "Token: the\tEmbedding shape: torch.Size([768])\n",
      "Token: river\tEmbedding shape: torch.Size([768])\n",
      "Token: bank\tEmbedding shape: torch.Size([768])\n",
      "Token: .\tEmbedding shape: torch.Size([768])\n",
      "Token: [SEP]\tEmbedding shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokeniser = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased')  # Load tokeniser\n",
    "model = BertModel.from_pretrained(\n",
    "    'bert-base-uncased')  # Load pre-trained BERT model\n",
    "model.eval()\n",
    "\n",
    "text = \"The bank will not lend money to the river bank.\"  # Example\n",
    "\n",
    "inputs = tokeniser(text, return_tensors='pt')  # Tokenise and encode\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # Shape: (1, seq_len, hidden_size)\n",
    "\n",
    "tokens = tokeniser.convert_ids_to_tokens(\n",
    "    inputs['input_ids'][0])  # Map tokens to embeddings\n",
    "for token, embedding in zip(tokens, embeddings[0]):\n",
    "    print(f\"Token: {token}\\tEmbedding shape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990c65e",
   "metadata": {},
   "source": [
    "### Generative Pre-trained Transformer (GPT)\n",
    "Generative Pre-trained Transformer (GPT) is a unidirectional language model that predicts the next token in a sequence (left-to-right), trained on a large corpus of text and can easily be fine-tuned. It uses the decoder part of the Transformer architecture.\n",
    "\n",
    "- **Applications**: Text generation, code generation, text summarisation, etc.\n",
    "\n",
    "To get embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cb996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokeniser = GPT2Tokenizer.from_pretrained('gpt2')  # Load tokeniser\n",
    "model = GPT2Model.from_pretrained('gpt2')  # Load pre-trained model\n",
    "model.eval()\n",
    "\n",
    "prompt = 'Once upon a time in a land far away'  # Example text as a prompt\n",
    "\n",
    "inputs = tokeniser(prompt,\n",
    "                   return_tensors='pt')  # Tokenise prompt\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Get embeddings\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "tokens = tokeniser.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "for token, embedding in zip(tokens, embeddings[0]):\n",
    "    print(f\"Token: {token}\\tEmbedding shape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424d3ff",
   "metadata": {},
   "source": [
    "To generate a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f1b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a land far away, the sun was shining, and the moon was rising. The sun had risen, but the earth was still.\n",
      "\n",
      "The sun rose, then, as it had rose before, to the sky,\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokeniser = GPT2Tokenizer.from_pretrained('gpt2')  # Load tokeniser\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # Load pre-trained model\n",
    "model.eval()\n",
    "\n",
    "prompt = 'Once upon a time in a land far away'  # Example text as a prompt\n",
    "\n",
    "inputs = tokeniser(prompt,\n",
    "                   return_tensors='pt')  # Tokenise prompt\n",
    "\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,  # Prevent repeated phrases\n",
    "    pad_token_id=tokeniser.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokeniser.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a02a31",
   "metadata": {},
   "source": [
    "### Embeddings from Language Models (ELMo)\n",
    "Embeddings from Language Models (ELMo) is a contextual word embedding method that generates dynamic representations of words based on their context in a sentence. Unlike static embeddings (e.g., Word2Vec), ELMo uses a bidirectional LSTM architecture to capture syntactic and semantic nuances, allowing words such as 'bank' to have different embeddings in 'river bank' or 'financial bank'. It provides following features:\n",
    "\n",
    "- **Bidirectional Context**: Processes text in both forward and backward directions to capture context from surrounding words.\n",
    "- **Layer Aggregation**: Combines embeddings from multiple LSTM layers to form rich representations.\n",
    "\n",
    "ELMo uses a two-layer bidirectional LSTM:\n",
    "- **Forward Pass**: Predicts the next word in a sequence.\n",
    "- **Backward Pass**: Predicts the previous word.\n",
    "- **Layer Concatenation**: Outputs from both layers and the initial embedding layer are combined into a final embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
