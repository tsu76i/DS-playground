{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb3b7d3",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "1. [Device Agnostic Code](#2-device-agnostic-code)\n",
    "1. [Loading Pre-Trained Model](#3-loading-pre-trained-model)\n",
    "    - [GPT-2](#gpt-2)\n",
    "1. [Repetition](#4-repetition)\n",
    "1. [Basic Generation Strategies](#5-basic-generation-strategies)\n",
    "    - [Greedy Search](#greedy-search)\n",
    "    - [Sampling](#sampling)\n",
    "    - [Beam Search](#beam-search)\n",
    "    - [Top-k Sampling](#top-k-sampling)\n",
    "    - [Top-p (Nucleus) Sampling](#top-p-nucleus-sampling)\n",
    "1. [Advanced Generation Strategies](#6-advanced-generation-strategies)\n",
    "    - [Speculative Decoding](#speculative-decoding)\n",
    "    - [Prompt Lookup Decoding](#prompt-lookup-decoding)\n",
    "    - [Self-Speculative Decoding](#self-speculative-decoding)\n",
    "    - [Universal Assisted Decoding](#universal-assisted-decoding)\n",
    "    - [Contrastive Search](#contrastive-search)\n",
    "    - [Decoding by Contrasting Layers (DoLa)](#decoding-by-contrasting-layers-dola)\n",
    "    - [Diverse Beam Search](#diverse-beam-search)\n",
    "1. [References](#7-references)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f809ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c49ec7",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Text generation is the task of automatically producing coherent and contextually relevant text using machine learning models. It is a fundamental component of natural language processing (NLP), enabling applications such as conversational agents, content creation, and summarisation. \n",
    "\n",
    "The purpose of this project is to explore various text generation strategies, both basic and advanced, using a range of pretrained transformer models (i.e., GPT-2, GPT-2 Large, layerskip-llama3.2-1B, and double7/vicuna-68m) to gain deeper insights into their capabilities and performance across different generation techniques.\n",
    "\n",
    "\n",
    "## 2. Device Agnostic Code\n",
    "GPU acceleration delivers significant speed-up over CPU for deep learning tasks, especially for large models and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cc87fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    device=\"cuda\"  # GPU\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"  # MPS (MacOS)\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"  # No GPU Available\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8ddb7",
   "metadata": {},
   "source": [
    "## 3. Loading Pre-Trained Model\n",
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e524fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e0a319b6ba41fe82b3f6e800356df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7a032d64004daab22b6b98511abd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bb1e73a3a8447084681f4caeb12046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c18fe3ce3e74aacac817e195a934a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f33b916c5d14da58c6214a80f212df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172352cd357d410a9a642d38bd52fb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612105cf02bf49a0b2915eb6093084ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2-large\"\n",
    "MAX_TOKENS = 50\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddfada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855e8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Hello! Today I\"\n",
    "\n",
    "model_inputs = tokeniser(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f85467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,     0,  6288,   314]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f2dcc",
   "metadata": {},
   "source": [
    "## 4. Repetition\n",
    "Phrases being repeated in the generated text is a common phenomenon in augoregressive language models. If a token or phrase has a high probability continuation in training data, the model's predictions tend to loop, generating the same phrase repeatedly. \n",
    "To avoid or reduce repetition, several techniques can be applied:\n",
    "\n",
    "- Employ nucleus sampling or top-k sampling (explained in detail below).\n",
    "- Increase `temperature` above 1.0 to flatten probability distribution, encouraging more random choices.\n",
    "- Use `repetition_penalty` (typically between 1.1 and 1.5) to reduce the probability of tokens that have already been generated.\n",
    "- Apply `no_repeat_ngram_size` to strictly prevent the model from generating the same n-gram repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb9634",
   "metadata": {},
   "source": [
    "## 5. Basic Generation Strategies\n",
    "### Greedy Search\n",
    "Greedy Search is the default setting of decoding strategy used by `.generate()`. At each step, it selects the token with the highest probability as the next token. This method is simple and fast, thus is suitable for generating short text. However, for longer text, it can lead to repetitive and less diverse sequences.\n",
    "\n",
    "By default, greedy search generates up to 20 new tokens unless specified in `GenerationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1316bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to show you how to make a simple, yet effective, way to get your hands on a few of the most popular and most sought after brands of beer.\n",
      "\n",
      "The first thing you need to do is to find a beer that you\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386670d6",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "Sampling selects a next token randomly based on the probability distribution over the entire vocabulary of the model. This reduces repetition and can generate more creative, diverse outputs compared to the greedy search strategy.\n",
    "\n",
    "Sampling is enabled by setting the parameters: `do_sample=True` and `num_beams=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27fbac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm getting a \"preview\" picture from Microsoft with some of the first shots, some \"lookins\" and some renders. Some people are saying that this is just pre-visualization work and nothing more than that, but the fact is,\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180c8f6",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "Beam Search maintains multiple candidate sequences (beams) simultaneously. At each step, it expands each beam by selecting tokens, then retains the top $k$ beams based on cumulative (overall) probability score. This strategy is suited for input-grounded tasks such as image captioning or speech recognition.\n",
    "\n",
    "Beam search is enabled by setting `num_beams > 1`, optionally combined with `do_sample = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c64f65d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to show you how to make a very simple and simple to use, but very useful, LED strip.\n",
      "\n",
      "The first thing you need to do is to download and install the Arduino IDE. If you don't have it yet, you\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    num_beams=5,\n",
    "    do_sample=True,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aac06c",
   "metadata": {},
   "source": [
    "### Top-k Sampling\n",
    "At each step of generation, the model predicts a probability distribution over the entire vocabulary for the next token, then selects only the top $k$ most probable tokens, ranked by their predicted probability. The probabilities of these top $k$ tokens are renormalised to sum to 1, and the next token is randomly sampled from this restricted set.\n",
    "\n",
    "The number $k$ is configured by the parameter `top_k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d91960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to tell you about a very simple and simple-to-use tool that you can use to make some very powerful and fun sounds in your games.\n",
      "\n",
      "If you've played a lot of indie games recently, chances are you know what\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f598d0",
   "metadata": {},
   "source": [
    "### Top-p (Nucleus) Sampling\n",
    "Instead of selecting tokens from the entire vocabulary, nucleus sampling samples from the smallest set of tokens whose cumulative probability exceeds the threshold $p$. This introduces controlled randomness, resulting in more diverse and creative text generation.\n",
    "\n",
    "The probability threshold $p$ is configured by the parameter `top_p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cbc91d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'd like to present to you the first release of the official Linux distribution of this game:\n",
      "\n",
      "Linux 4.6.3: Xubuntu 14.04 LTS with Unity 7\n",
      ".\n",
      "- Lubuntu is a lightweight distribution based on Ubuntu\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66911c",
   "metadata": {},
   "source": [
    "## 6. Advanced Generation Strategies\n",
    "### Speculative Decoding\n",
    "Speculative decoding uses a second smaller draft model to generate multiple speculative tokens which are then verified by the larget target model to speed up autoregressive decoding. For example, with GPT-2 model:\n",
    "- Draft model: smaller GPT-2 (e.g., `gpt2` or `distilgpt2`)\n",
    "- Target model: `gpt2-large`\n",
    "\n",
    "Speculative decoding can be enabled by setting a draft model to the parameter `assistant_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2a951f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e686b666674e4987a8828719a068f22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c551550d50847a8a677039801e9b6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060bcd8ec008437ea255cb1251983644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to talk about a method that was introduced just recently called the \"WidowMaker\" method. It's a new method introduced to the framework, and it's going allow us to create a full set of objects called \"wife\", \"\n"
     ]
    }
   ],
   "source": [
    "draft_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    assistant_model=draft_model,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184c755",
   "metadata": {},
   "source": [
    "Or, we can use the same parameter in Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e99133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to give you guys the quick rundown on the new update on the project in the Dev's Corner. There was a lot of content that has been built, so this should be a quick overview of what's been done.\n",
      "\n",
      "So,\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    tokenizer=tokeniser,\n",
    "    model=model,\n",
    "    assistant_model=draft_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipe_output = pipe(\n",
    "    text_inputs=input_text,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    ")\n",
    "print(pipe_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ff03f",
   "metadata": {},
   "source": [
    "### Prompt Lookup Decoding\n",
    "Prompt Lookup Decoding is a variant of speculative decoding that leverages the significant overlap between the input prompt and the generated output. Unlike traditional speculative decoding that uses a smaller draft model to propose next tokens, this method uses substring matching directly on the input prompt to generate candidate tokens more efficiently.\n",
    "\n",
    "It is sufficient to specify the number of overlapping tokens in the `prompt_lookup_num_tokens` to enable this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569a70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'd like to talk about some things you can do with the Command Line. Not as a replacement for the IDE in your workflow, but as an alternative. I often find that the old IDE does the same job as the command line, and I know\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    prompt_lookup_num_tokens=3,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eaa0a",
   "metadata": {},
   "source": [
    "### Self-Speculative Decoding\n",
    "Self-Speculative Decoding is a method to accelerate inference in LLMs by using different parts of the same model rather instead of relying on another smaller model. It uses early (shallower) layers of the same model as a draft model and deeper layers for verification. During generation, the model selectively skips some intermediate layers to generate draft tokens faster, improving efficiecy without losing output quality.\n",
    "\n",
    "General models like GPT-2, GPT-3, or GPT-J typically do not support self-speculative decoding because they lack architectural support for layer sparsity or early exits. For this section, we will use a llama-based model that supports self-speculative decoding.\n",
    "\n",
    "Passing the `assistant_early_exist` parameter to the `generate()` function will activate self-speculative decoding. This parameter controls how many early layers are used for the draft (speculative) stage during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62b0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load .env variables\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "# from huggingface_hub import whoami\n",
    "\n",
    "# user_info = whoami()\n",
    "# print(f\"Logged in as: {user_info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6659dd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc147a6bb7e4fbe9db7decea405ae28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424d46571ae74550a4c1f08ff5e79b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0613a3f97b2d457fbd8417a81111a784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/126 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68afc95bc8444196937b82e919a7ab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6442f746a2d04e169695c688252eb237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7309b637e8314de08ef01cb0d29eceff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I am sharing a card I made for the new challenge at the Paper Players. I used the sketch from the challenge and the colors are from my stash. The sentiment is from a stamp set I have had for a long time. It is called \"Thank\n"
     ]
    }
   ],
   "source": [
    "# Self-Speculative Decoding is not available for GPT-2\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/layerskip-llama3.2-1B\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.generation_config.pad_token_id = (\n",
    "    model.generation_config.eos_token_id\n",
    ")  # Handle warnings\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/layerskip-llama3.2-1B\", padding_side=\"left\"\n",
    ")\n",
    "model_inputs = tokeniser(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    assistant_early_exit=4,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5db2d",
   "metadata": {},
   "source": [
    "### Universal Assisted Decoding\n",
    "Universal Assisted Decoding (UAD), sometimes called Universal Assisted Generation, is an advanced method designed to speed up language model text generation by using two models (a large target model and a smaller assistant model) even when those models use different tokenisers or come from entirely different model families.\n",
    "\n",
    "After the assistant model generates tokens, these are converted to text and then re-encoded with the target model's tokeniser so the target model can verify them. This allows for flexible speedups of any large model using an arbitrary smaller model from a different family or tokeniser.\n",
    "\n",
    "UAD can be implemented by setting the assistant tokeniser to the parameter `assistant_tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "787cf5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc8614fe3734a8da5cf38ead719ca55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0be4022cdc4ff0aeb116ebe2135ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64bb51d1a104104a04b8019b54cc58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40fb0800c0764bdd9dcb577bcaf479d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8c5857ebe9473daf095e71b7e00458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/714 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8fb88ba5ee42799ff6b6bd2ac831f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/272M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839d818e0c4e444b8f17dae7249da317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7a5ebe9b4843d8b59df0f3f977bb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/952 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7d5466fbe6482a8047f9ff9feb8851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5436288fc0b64bee855ada7be626fe7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to talk about the new features of the game.\n",
      "\n",
      "The new feature is the ability to play the \"Doom\" mode. This mode is a new way to experience the Doom experience. It's a game mode that allows you to\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "DRAFT_MODEL_NAME = \"double7/vicuna-68m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(DRAFT_MODEL_NAME)\n",
    "draft_tokeniser = AutoTokenizer.from_pretrained(DRAFT_MODEL_NAME)\n",
    "draft_model.generation_config.pad_token_id = draft_model.generation_config.eos_token_id\n",
    "\n",
    "input_text = \"Hello! Today I\"\n",
    "\n",
    "model_inputs = tokeniser(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    assistant_model=draft_model,\n",
    "    tokenizer=tokeniser,\n",
    "    assistant_tokenizer=draft_tokeniser,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98ed92d",
   "metadata": {},
   "source": [
    "### Contrastive Search\n",
    "Contrastive Search is a decoding method designed to reduce repetition and improve diversity in language model outputs, especially for generating long sequences. It works by comparing the similarity of candidate tokens with previously generated tokens and penalising those that are similar, thereby encouraging more diverse and informative text.\n",
    "\n",
    "This method is typically controlled by two parameters:\n",
    "\n",
    "- `penalty_alpha`: A hyperparameter that balances the trade-off between selecting high-probability tokens and penalising similar (redundant) tokens.\n",
    "- `top_k`: The number of top candidate tokens considered at each decoding step for applying the contrastive scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d7b5e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to talk about the \"Battleship\" of the United States.\n",
      "\n",
      "The Battle of Lexington\n",
      " (1803)\n",
      ". . .\n",
      ", . The Battle at Lexington was a battle fought between the English and the French in\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=False,\n",
    "    penalty_alpha=0.5,\n",
    "    top_k=4,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cdcf1a",
   "metadata": {},
   "source": [
    "### Decoding by Contrasting Layers (DoLA)\n",
    "Decoding by Contrasting Layers (DoLa) is a contrastive decoding strategy that improves factual accuracy and reduces hallucinations in large language models (LLMs) without requiring external knowledge retrieval or additional fine-tuning.\n",
    "\n",
    "DoLa compares the token predictions (logits) from the final layers with those from earlier layers during decoding, aiming to highlight tokens that represent factual knowledge more effectively. This technique is not recommended for smaller models such as GPT-2.\n",
    "\n",
    "DoLa is enabled by the following two parameters:\n",
    "\n",
    "- `dola_layers`: candidate layers to be contrasted with the final layer. Can be set to 'high' (for short-answer tasks) or 'low' (for long-answer tasks).\n",
    "- `repetition_penalty`: reduces repetition; it is recommended to set this to 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce71aa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'd like to tell you a bit about our website!\n",
      "\n",
      "\n",
      "If you like, you can come over and look around for the free games or come and have a look around in our lobby!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2-large\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "model_inputs = tokeniser(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=False,\n",
    "    dola_layers=\"high\",\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f8a93",
   "metadata": {},
   "source": [
    "### Diverse Beam Search\n",
    "Diverse Beam Search is a variant of traditional beam search designed to generate more diverse output sequences while reducing computational costs by dividing the total number of beams into smaller groups (beam groups).\n",
    "\n",
    "This method is enabled by setting the following three parameters:\n",
    "\n",
    "- `num_beams`: Total number of beams across all groups.\n",
    "- `num_beam_groups`: Number of groups into which beams are divided. Must divide evenly into num_beams (i.e., num_beams % num_beam_groups == 0).\n",
    "- `diversity_penalty`: Strength of the penalty applied to tokens in other groups, encouraging diverse outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df6add1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I want to show you how to create a simple web application using the AngularJS framework. This tutorial will show you how to create a simple web application using the AngularJS framework. This tutorial will show you how to create a simple web application using the Angular\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    num_beams=6,\n",
    "    num_beam_groups=3,\n",
    "    diversity_penalty=1.0,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c79868",
   "metadata": {},
   "source": [
    "## 7. References\n",
    "1. Aritra Roy Gosthipaty, Mostafa Elhoushi, Pedro Cuenca, Vaibhav Srivastav. (2024). Hugging Face. *Faster Text Generation with Self-Speculative Decoding*.<br>\n",
    "https://huggingface.co/blog/layerskip\n",
    "\n",
    "1. Majd Farah. (2023). *Generating Text with GPT2 in Under 10 Lines of Code*.<br>\n",
    "https://medium.com/@majd.farah08/generating-text-with-gpt2-in-under-10-lines-of-code-5725a38ea685\n",
    "\n",
    "1. Hugging Face. (n.d.). *Generation strategies*.<br>\n",
    "https://huggingface.co/docs/transformers/en/generation_strategies\n",
    "\n",
    "1. Hugging Face. (n.d.). *Text generation*. <br>\n",
    "https://huggingface.co/docs/transformers/en/llm_tutorial\n",
    "\n",
    "1. Daniel Korat, Jonathan Mamou, Nadav Timor, Oren Pereg, Joao Gante, Moshe Wasserblat, Moshe Berchansky, Lewis Tunstall. (2024). *Universal Assisted Generation: Faster Decoding with Any Assistant Model*.<br>\n",
    "https://huggingface.co/blog/universal_assisted_generation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
