{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10185455",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in Japanese\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "1. [Loading Data](#2-loading-data)\n",
    "1. [Data Preprocessing](#3-data-preprocessing)\n",
    "1. [Loading Pre-Trained Model](#4-loading-pre-trained-model)\n",
    "1. [Tokenisation](#5-tokenisation)\n",
    "1. [Training Arguments](#6-training-arguments)\n",
    "1. [Evaluation Metrics](#7-evaluation-metrics)\n",
    "1. [Fine-Tuning Transformer Model](#8-fine-tuning-transformer-model)\n",
    "1. [Predictions with Fine-Tuned Model](#9-predictions-with-fine-tuned-model)\n",
    "1. [References](#10-references)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3882cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "from typing import Dict, List, Tuple, Any, Mapping, Sequence\n",
    "from numpy.typing import NDArray\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fa47e",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Text classification is a fundamental task in natural language processing (NLP) that involves assigning predefined labels or classes to given texts. One important application of text classification is sentiment analysis, a challenging task that seeks to capture the context and nuances of language in order to identify sentiments such as positive, neutral, or negative.\n",
    "\n",
    "Unlike English, some languages (e.g., Japanese and Chinese) do not use whitespace to separate words. This lack of explicit word boundaries presents a significant challenge for NLP systems, as it requires accurate word segmentation before higher-level tasks can be performed.\n",
    "\n",
    "This project aims to implement sentiment analysis for Amazon reviews written in Japanese by fine-tuning a transformer model from Hugging Face. We will utilise a BERT-based transformer model that has been pre-trained on large-scale Japanese text corpora.\n",
    "\n",
    "## 2. Loading Data\n",
    "Retrieved from [Hugging Face - Datasets: SetFit/amazon_reviews_multi_ja](https://huggingface.co/datasets/SetFit/amazon_reviews_multi_ja/viewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30772851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"SetFit/amazon_reviews_multi_ja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcecdcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d5883",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52103543",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "N_LABELS = 3\n",
    "N_EPOCHS = 3\n",
    "MY_MODEL_PATH = \"./my_finetuned_model\"\n",
    "\n",
    "ID_2_LABEL = {0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\"}\n",
    "LABEL_2_ID = {\"NEGATIVE\": 0, \"NEUTRAL\": 1, \"POSITIVE\": 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc66d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ja_0388536',\n",
       " 'text': '普段使いとバイクに乗るときのブーツ兼用として購入しました。見た目や履き心地は良いです。 しかし、２ヶ月履いたらゴム底が削れて無くなりました。また、バイクのシフトペダルとの摩擦で表皮が剥がれ、本革でないことが露呈しました。ちなみに防水とも書いていますが、雨の日は内部に水が染みます。 安くて見た目も良く、履きやすかったのですが、耐久性のなさ、本革でも防水でも無かったことが残念です。結局、本革の防水ブーツを買い直しました。',\n",
       " 'label': 0,\n",
       " 'label_text': '0'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd5312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0, count: 40000\n",
      "label: 1, count: 40000\n",
      "label: 2, count: 40000\n",
      "label: 3, count: 40000\n",
      "label: 4, count: 40000\n"
     ]
    }
   ],
   "source": [
    "labels, counts = np.unique(ds[\"train\"][\"label\"], return_counts=True)\n",
    "for label, count in zip(labels, counts):\n",
    "    print(f\"label: {label}, count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe303e1",
   "metadata": {},
   "source": [
    "Labels in the range 0 – 4 represent review ratings from 1 to 5 stars, respectively. For sentiment classification purposes, we map these labels into three categories:\n",
    "\n",
    "- Labels 0 and 1 $\\rightarrow$ **Negative**\n",
    "- Label 2 $\\rightarrow$ **Neutral**\n",
    "- Labels 3 and 4 $\\rightarrow$ **Positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00eb9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rating_to_sentiment(\n",
    "    batch: Mapping[str, Sequence[int]],\n",
    "    id2label: Dict[int, str],\n",
    ") -> Dict[str, List[Any]]:\n",
    "    \"\"\"\n",
    "    Convert numerical review ratings to sentiment class (int) and text labels.\n",
    "\n",
    "    Args:\n",
    "        batch: A batch of dataset samples containing the key \"label\", whose values are integer ratings.\n",
    "        id2label: A dictionary mapping sentiment class IDs to their string labels,\n",
    "            e.g., `{0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}`.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing:\n",
    "            - label: List of integer sentiment classes corresponding to each sample,\n",
    "            - label_text: List of sentiment label strings corresponding to each sample.\n",
    "    \"\"\"\n",
    "    ratings = batch[\"label\"]\n",
    "    sentiments = []\n",
    "    sentiments_text = []\n",
    "    for r in ratings:\n",
    "        if r in [0, 1]:\n",
    "            sentiments.append(0)\n",
    "            sentiments_text.append(id2label[0])\n",
    "        elif r == 2:\n",
    "            sentiments.append(1)\n",
    "            sentiments_text.append(id2label[1])\n",
    "        else:\n",
    "            sentiments.append(2)\n",
    "            sentiments_text.append(id2label[2])\n",
    "    return {\"label\": sentiments, \"label_text\": sentiments_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6c967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(\n",
    "    convert_rating_to_sentiment,\n",
    "    batched=True,\n",
    "    fn_kwargs={\"id2label\": ID_2_LABEL},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9612fa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 0, count: 80000\n",
      "label: 1, count: 40000\n",
      "label: 2, count: 80000\n"
     ]
    }
   ],
   "source": [
    "labels, counts = np.unique(ds[\"train\"][\"label\"], return_counts=True)\n",
    "for label, count in zip(labels, counts):\n",
    "    print(f\"label: {label}, count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea47ec",
   "metadata": {},
   "source": [
    "## 4. Loading Pre-Trained Model\n",
    "For this task, a pre-trained BERT model ([bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3)) will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71128e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tohoku-nlp/bert-base-japanese-v3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"tohoku-nlp/bert-base-japanese-v3\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=N_LABELS,\n",
    "    id2label=ID_2_LABEL,\n",
    "    label2id=LABEL_2_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcf3fd",
   "metadata": {},
   "source": [
    "## 5. Tokenisation\n",
    "Tokenisation is a required step to convert raw text into a sequence of tokens that a model can process. \n",
    "\n",
    "The pretrained model was originally trained with a specific vocabulary and tokenisation scheme. Therefore, it is essential to use the tokeniser corresponding to the same pretrained model to ensure consistency between training and inference. This practice prevents vocabulary mismatches and guarantees that input texts are processed in a way the model expects, thereby maintaining optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1446ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenise_function at 0x142d877e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08190fd69c3845939f5408a5b213dc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f71703323e54acba19ed3b4f4db102e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea292a09b0a4e6eab052b63adc3c4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def tokenise_function(example):\n",
    "    return tokeniser(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "\n",
    "tokenised_dataset = ds.map(tokenise_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0279728f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'label', 'label_text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895c928",
   "metadata": {},
   "source": [
    "The `DataCollatorWithPadding()` dinamically applies padding to a batch of tokenised input sequences so that all sequences in the batch have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7473e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a6944",
   "metadata": {},
   "source": [
    "Format the dataset for PyTorch, including the labels as PyTorch integer tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7be396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2295e",
   "metadata": {},
   "source": [
    "## 6. Training Arguments\n",
    "The following code configures the training arguments for fine-tuning a Transformer model using Hugging Face's `Trainer` API. For this project, F1-Score will be employed as the evaluation metric to determine the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cdffb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265b20e",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "The overall performance of the fine‑tuned model will be evaluated using two metrics: accuracy and the F1‑score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5461582",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    eval_pred: Tuple[NDArray[np.float64], NDArray[np.int8]],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy and weighted F1-score for model predictions.\n",
    "\n",
    "    This function calculates the accuracy and weighted F1-score of the model's predictions\n",
    "    given the logits and true labels for a batch. Logits are expected to be floating-point\n",
    "    arrays, and labels are expected to be integer arrays. Predictions are derived via the\n",
    "    argmax function, accuracy is the proportion of correct predictions, and the F1-score\n",
    "    is weighted by class frequency.\n",
    "\n",
    "    Args:\n",
    "        eval_pred:  A tuple containing the logits (as a NumPy array of floats, shape [N, C]) and\n",
    "                    the true integer labels (NumPy array of shape [N]), where N is the number of\n",
    "                    samples and C is the number of classes.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with classification metrics:\n",
    "            - \"accuracy\": Overall accuracy as a float.\n",
    "            - \"f1\": Weighted F1-score as a float.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"], \"f1\": f1[\"f1\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c42fd2",
   "metadata": {},
   "source": [
    "## 8. Fine-Tuning Transformer Model\n",
    "Fine-tuning transformer models is computationally expensive and time-consuming. To accelerate the training process, only 20% of the complete dataset will be used for training and validation. This corresponds to 40k tokens for training and 1k tokens for validation, compared with 200k and 5k tokens respectively in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a0f6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_train_dataset = tokenised_dataset[\"train\"].train_test_split(\n",
    "    test_size=0.2, seed=RANDOM_SEED\n",
    ")[\"test\"]\n",
    "\n",
    "tokenised_val_dataset = tokenised_dataset[\"validation\"].train_test_split(\n",
    "    test_size=0.2, seed=RANDOM_SEED\n",
    ")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "451cd025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsu76i/Documents/Programming/Personal Projects/DS_Projects/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 1:50:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.590900</td>\n",
       "      <td>0.573127</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.747990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>0.585562</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.374700</td>\n",
       "      <td>0.653701</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsu76i/Documents/Programming/Personal Projects/DS_Projects/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/tsu76i/Documents/Programming/Personal Projects/DS_Projects/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.4975051188151042, metrics={'train_runtime': 6649.2331, 'train_samples_per_second': 18.047, 'train_steps_per_second': 1.128, 'total_flos': 7893402531840000.0, 'train_loss': 0.4975051188151042, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_train_dataset,\n",
    "    eval_dataset=tokenised_val_dataset,\n",
    "    processing_class=tokeniser,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe9693d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_finetuned_model/tokenizer_config.json',\n",
       " './my_finetuned_model/special_tokens_map.json',\n",
       " './my_finetuned_model/vocab.txt',\n",
       " './my_finetuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(MY_MODEL_PATH)\n",
    "tokeniser.save_pretrained(MY_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6add374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsu76i/Documents/Programming/Personal Projects/DS_Projects/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 01:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6751722097396851, 'eval_accuracy': 0.751, 'eval_f1': 0.75170711393122, 'eval_runtime': 77.3708, 'eval_samples_per_second': 64.624, 'eval_steps_per_second': 4.045, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "metrics = trainer.evaluate(tokenised_dataset[\"test\"])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1dbb2a",
   "metadata": {},
   "source": [
    "## 9. Predictions with Fine-Tuned Model\n",
    "After training, the fine-tuned model and tokeniser can be used to predict the label and confidence score of a single text sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55184979",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"この商品はとても使いやすく、期待以上の性能でした。買ってよかったです！\"\n",
    "text_2 = \"価格の割に品質が低いと感じました。リピートはしません。\"\n",
    "text_3 = \"何とも言えないです。良くも悪くもないです。\"\n",
    "text_4 = \"時々役に立つかもしれません。\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00bf36",
   "metadata": {},
   "source": [
    "The *'proper'* approach is to use PyTorch's inference mode, compute the probabilities and confidence from the logits, and then make a prediction using the argmax() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35aa9f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: この商品はとても使いやすく、期待以上の性能でした。買ってよかったです！\n",
      "Predicted: 2 (POSITIVE), Confidence: 0.9924\n"
     ]
    }
   ],
   "source": [
    "my_tokeniser = AutoTokenizer.from_pretrained(MY_MODEL_PATH)\n",
    "my_model = AutoModelForSequenceClassification.from_pretrained(MY_MODEL_PATH)\n",
    "\n",
    "inputs = my_tokeniser(text_1, return_tensors=\"pt\")\n",
    "with torch.inference_mode():\n",
    "    logits = my_model(**inputs).logits\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    confidence = probabilities.max().item()\n",
    "    predicted_class_id = probabilities.argmax().item()\n",
    "\n",
    "print(f\"Text: {text_1}\")\n",
    "print(\n",
    "    f\"Predicted: {predicted_class_id} ({my_model.config.id2label[predicted_class_id]}), Confidence: {confidence:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08099e1",
   "metadata": {},
   "source": [
    "Pipelines are another efficient and convenient way to use models for inference. By specifying the model parameter with the path to the fine‑tuned model, we can load our own model; if this parameter is omitted, the pipeline automatically selects a default pre‑trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af480946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: この商品はとても使いやすく、期待以上の性能でした。買ってよかったです！\n",
      "[{'label': 'POSITIVE', 'score': 0.9924416542053223}]\n",
      "\n",
      "Text: 価格の割に品質が低いと感じました。リピートはしません。\n",
      "[{'label': 'NEGATIVE', 'score': 0.9454251527786255}]\n",
      "\n",
      "Text: 何とも言えないです。良くも悪くもないです。\n",
      "[{'label': 'NEUTRAL', 'score': 0.8939847946166992}]\n",
      "\n",
      "Text: 時々役に立つかもしれません。\n",
      "[{'label': 'NEUTRAL', 'score': 0.8266585469245911}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=MY_MODEL_PATH,\n",
    ")\n",
    "\n",
    "print(f\"Text: {text_1}\")\n",
    "print(classifier(text_1))\n",
    "\n",
    "print(f\"\\nText: {text_2}\")\n",
    "print(classifier(text_2))\n",
    "\n",
    "print(f\"\\nText: {text_3}\")\n",
    "print(classifier(text_3))\n",
    "\n",
    "print(f\"\\nText: {text_4}\")\n",
    "print(classifier(text_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6606dd3",
   "metadata": {},
   "source": [
    "## 10. References\n",
    "\n",
    "1. Hugging Face. (n.d.). *Fine-tuning*.<br>\n",
    "https://huggingface.co/docs/transformers/training\n",
    "\n",
    "1. Hugging Face. (n.d.). *Text classification*.<br>\n",
    "https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "\n",
    "1. Hugging Face. (n.d.). *Trainer*.<br>\n",
    "https://huggingface.co/docs/transformers/trainer\n",
    "\n",
    "1. PyTorch Docs. (n.d.). *Transformers*.<br>\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "\n",
    "1. tohoku-nlp. (2023). *BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)*. Hugging Face.<br>\n",
    "https://huggingface.co/tohoku-nlp/bert-base-japanese-v3\n",
    "\n",
    "1. SetFit. (2022). *SetFit/amazon_reviews_multi_ja*. Hugging Face.<br>\n",
    "https://huggingface.co/datasets/SetFit/amazon_reviews_multi_ja/viewer?views%5B%5D=train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
