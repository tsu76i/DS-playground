{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067c7fba",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "***\n",
    "## Table of Contents\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5732b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "from typing import Dict, List, Optional\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82468ec",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cabef",
   "metadata": {},
   "source": [
    "## 2. Device Agnostic Code\n",
    "Mac GPU acceleration (`mps` backend) delivers significant speed-up over CPU for deep learning tasks, especially for large models and batch sizes. On Windows, `cuda` is used instead of `mps`.\n",
    "\n",
    "`NotImplementedError: The operator 'aten::_nested_tensor_from_mask_left_aligned' is not currently implemented for the MPS device.`\n",
    "\n",
    "As of current PyTorch version, certain Transformer operations (especially those involving masks in nn.TransformerEncoder) are not supported natively on Apple Silicon (MPS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\n",
    "#     \"mps\"  # MacOS\n",
    "#     if torch.backends.mps.is_available()\n",
    "#     else \"cuda\"  # Windows\n",
    "#     if torch.cuda.is_available()\n",
    "#     else \"cpu\"  # No GPU Available\n",
    "# )\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6b956",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n",
    "\n",
    "The dataset used in this project (retrieved from [Kaggle - IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)) includes:\n",
    "\n",
    "- **review**: Review comments in text.\n",
    "- **sentiment**: Whether the review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"_datasets/IMDB_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf36d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(f\"Shape of the dataset: {df.shape}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Count of null values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f01925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093be12e",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "1. Text Cleaning\n",
    "    - Lower all letters\n",
    "    - Removing HTML Tags\n",
    "    - Removing URLs\n",
    "    - Removing Emojis and Non-ASCII Characters\n",
    "    - Remove Punctuations\n",
    "    - Remove extra whitespace\n",
    "2. Tokenisation\n",
    "3. Building Vocabulary and Mapping Tokens to Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb803d",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef77b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "    text = re.sub(\"[{}]\".format(re.escape(string.punctuation)), \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Leave a space between words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"review\"].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f75d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_tokens\"] = df[\"clean_text\"].apply(lambda text: len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b34550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e2902",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "Split all reviews into tokens (words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [token for text in df[\"clean_text\"] for token in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cfbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df7248",
   "metadata": {},
   "source": [
    "### Building Vocabulary and Mapping Tokens to Indices\n",
    "Using `Counter()` allows us to get the frequency of each word, sort in descending order (we can specify `n` parameter to extract the top N most frequent words).\n",
    "Then we assign a unique index to each word, create mapping (word2index), and reserve indices for padding (`<PAD>`) and unknown (`<UNK>`) tokens.\n",
    "\n",
    "**Padding**:\n",
    "- Padding is the process of adding special tokens (usually represented as `<PAD>`) to sequences so that all sequences in a batch have the same length.\n",
    "- This is necessary because neural networks, especially in libraries like PyTorch, require inputs to be in tensors of consistent shape.\n",
    "\n",
    "*Example*:\n",
    "- Original:\n",
    "    - [\"i\", \"loved\", \"this\", \"movie\"]\n",
    "- After padding to length 6:\n",
    "    - [\"i\", \"loved\", \"this\", \"movie\", \"`<PAD>`\", \"`<PAD>`\"]\n",
    "\n",
    "**Unknown**:\n",
    "- `<UNK>` stands for '*unknown token*', serving as a placeholder for any token (word) in the input text that does not exist in the vocabulary.\n",
    "\n",
    "*Example*:\n",
    "- Vocabulary:\n",
    "    - { \"the\":2, \"movie\":3, \"`<PAD>`\":0, \"`<UNK>`\":1 }\n",
    "- Input:\n",
    "    - \"the plot was amazing\" -> [\"the\", \"`<UNK>`\", \"`<UNK>`\", \"`<UNK>`\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14644740",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_sorted = word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {word: i for i, (word, counts) in enumerate(all_words_sorted, start=2)}\n",
    "word2index[\"<PAD>\"] = 0\n",
    "word2index[\"<UNK>\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique words: {len(word2index) - 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to int sequences\n",
    "def text_to_int(text: str, word2index: Dict[str, int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Convert a text string into a list of integer indices based on the word2index mapping.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "        word2index: Dictionary mapping words to integer indices.\n",
    "\n",
    "    Returns:\n",
    "        List of integer indices corresponding to words in the input text.\n",
    "        Unknown words are mapped to the index of the \"<UNK>\" token.\n",
    "    \"\"\"\n",
    "    return [word2index.get(word, word2index[\"<UNK>\"]) for word in text.split()]\n",
    "\n",
    "\n",
    "def pad_or_truncate(text: List[int], max_len: int, pad_value: int = 0) -> List[int]:\n",
    "    \"\"\"\n",
    "    Pad or truncate a list of integer indices to a specified maximum length.\n",
    "\n",
    "    Args:\n",
    "        text: List of word indices representing the text.\n",
    "        max_len: The desired length of the output list.\n",
    "        pad_value: The value to use for padding. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        The input list truncated or padded to max_len.\n",
    "    \"\"\"\n",
    "    if len(text) >= max_len:  # Text length ok\n",
    "        return text[:max_len]\n",
    "    else:  # Text too short -> Add padding\n",
    "        return text + [pad_value] * (max_len - len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f99eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 128\n",
    "\n",
    "all_review_seq = [\n",
    "    pad_or_truncate(text_to_int(text, word2index), SEQUENCE_LENGTH, word2index[\"<PAD>\"])\n",
    "    for text in df[\"clean_text\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_review_seq[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6bf991",
   "metadata": {},
   "source": [
    "### Convert Labels to Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c95d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4b6b9",
   "metadata": {},
   "source": [
    "## 4. Preparing DataLoaders\n",
    "### Train Test Splitting\n",
    "80% train, 10% validation and 10% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "seq_train, seq_sub, y_train, y_sub = train_test_split(\n",
    "    all_review_seq, labels, test_size=0.2, random_state=RANDOM_SEED, stratify=labels\n",
    ")\n",
    "\n",
    "seq_val, seq_test, y_val, y_test = train_test_split(\n",
    "    seq_sub, y_sub, test_size=0.5, random_state=RANDOM_SEED, stratify=y_sub\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22c418",
   "metadata": {},
   "source": [
    "### Creating IMDB Datasets in Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586dced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for IMDB movie reviews.\n",
    "\n",
    "    Args:\n",
    "        sequences: List of tokenized and padded text sequences.\n",
    "        labels: Corresponding list of labels (floats).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequences: List[List[int]], labels: List[float]) -> None:\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sequences)\n",
    "\n",
    "     def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.sequences[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0a915",
   "metadata": {},
   "source": [
    "PyTorch's `torch.tensor()` constructor works with lists, tuples, or NumPy arrays, not pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b073a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_ds = IMDBDataset(seq_train, y_train.values)\n",
    "val_ds = IMDBDataset(seq_val, y_val.values)\n",
    "test_ds = IMDBDataset(seq_test, y_test.values)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447eade9",
   "metadata": {},
   "source": [
    "## 5. Neural Network Model Architectures\n",
    "### Long Short-Term Memory (LSTM)\n",
    "An LSTM architecture for sentiment analysis typically includes following structures:\n",
    "- **Embedding Layer**:\n",
    "    - Converts input token indices to dense vector embeddings.\n",
    "    - Inputs shape: `(batch_size, sequence_length)`\n",
    "    - Outputs shape: `(batch_size, sequence_length, embedding_size)`\n",
    "- **LSTM Layer**:\n",
    "    - Processes the embedded sequence to capture temporal depenencies.\n",
    "    - Can be either single or multi-layer LSTM.\n",
    "- **Classification Head**:\n",
    "    - Usually a fully-connected (linear) layer projecting the hidden state(s) from LSTM to the output classes.\n",
    "    - Often preceded by dropout for regularisation.\n",
    "- **Activation and Loss**:\n",
    "    - For binary sentiment, output logits go through sigmoid activation function with binary cross entropy loss.\n",
    "    - For multi-class, softmax activation with cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ba3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based classifier for sequence classification tasks.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Size of the vocabulary.\n",
    "        embedding_size: Dimension of word embeddings.\n",
    "        hidden_size: Number of features in the hidden state of the LSTM.\n",
    "        output_size: Number of output classes.\n",
    "        n_layers: Number of LSTM layers. Defaults to 1.\n",
    "        is_bidirectional: Whether to use bidirectional LSTM. Defaults to False.\n",
    "        dropout_rate: Dropout rate for regularization. Defaults to 0.5.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        n_layers: int = 1,\n",
    "        is_bidirectional: bool = False,\n",
    "        dropout_rate: float = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.is_bidirectional = is_bidirectional\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_size\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=is_bidirectional,\n",
    "            dropout=dropout_rate if n_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        directional_factor = 2 if is_bidirectional else 1\n",
    "        self.fc = nn.Linear(\n",
    "            in_features=hidden_size * directional_factor,\n",
    "            out_features=output_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, text: torch.Tensor\n",
    "    ) -> torch.Tensor:  # text shape: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        Forward propagation of the model.\n",
    "\n",
    "        Args:\n",
    "            text: Input tensor of shape (batch_size, seq_len) containing token indices.\n",
    "\n",
    "        Returns:\n",
    "            Output logits of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(text)  # (batch_size, seq_len, embedding_size)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # If bidirectional, concatenate last forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat(\n",
    "                (hidden[-2, :, :], hidden[-1, :, :]), dim=1\n",
    "            )  # (batch_size, hidden_dim*2)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]  # (batch_size, hidden_dim)\n",
    "\n",
    "        dropped = self.dropout(hidden)\n",
    "        out = self.fc(dropped)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca65a5",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "A Transformer is a neural network architecture designed primarily for handling sequential data. Unlike traditional recurrent networks (RNNs/LSTMs), transformers use a self-attention mechanism to process the entire sequence at once. This allows the model to capture long-range dependencies efficiently and in parallel, which has made transformers the foundation for many state-of-art models in NLP, such as BERT and GPT.\n",
    "\n",
    "**Features**:\n",
    "- Instead of processing tokens sequentially, a transformer applies self-attention mechanism to let each token attend to every other token in the input sequence simultaneously.\n",
    "- This is combined with positional encoding to inject information about token order, since the model doesn't inherently process inputs sequentially.\n",
    "- The original Transformer architecture consists of an encoder and a decoder; for classification tasks like sentiment analysis, usually only the encoder part is used.\n",
    "- Key components include multi-head attention layers, feed-forward neural networks, layer normalisation, and residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c6660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder-based classifier for sequence classification.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Size of the vocabulary.\n",
    "        embed_dim: Dimension of the embedding vectors.\n",
    "        nhead: Number of attention heads.\n",
    "        num_layers: Number of transformer encoder layers.\n",
    "        hidden_dim: Dimension of intermediate feed-forward layers.\n",
    "        max_len: Maximum input sequence length.\n",
    "        num_classes: Number of output classes. Defaults to 1.\n",
    "        dropout_rate: Dropout rate. Defaults to 0.2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        nhead: int,\n",
    "        num_layers: int,\n",
    "        hidden_dim: int,\n",
    "        max_len: int,\n",
    "        num_classes: int = 1,\n",
    "        dropout_rate: float = 0.2,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Embedding Layer: Maps input tokens (0 to vocab_size-1) to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        # Positional Encoding: Adds sine and cosine functions of different frequencies to embeddings (for token position in the sequence)\n",
    "        self.pos_encoding = nn.Parameter(\n",
    "            self._get_positional_encoding(max_len, embed_dim), requires_grad=False\n",
    "        )\n",
    "        # Transformer Encoder Layer: One layer of the transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=nhead,  # Number of attention heads\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # Stacks multiple encoder layers to form the full transformer encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def _get_positional_encoding(self, seq_len: int, d_model: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create fixed sinusoidal positional encodings.\n",
    "\n",
    "        Args:\n",
    "            seq_len: Sequence length.\n",
    "            d_model: Embedding dimension.\n",
    "\n",
    "        Returns:\n",
    "            Positional encoding tensor of shape (1, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        pos = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        i = torch.arange(d_model, dtype=torch.float32).unsqueeze(0)\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model)\n",
    "        angle_rads = pos * angle_rates\n",
    "        encoding = torch.zeros(seq_len, d_model)\n",
    "        encoding[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        encoding[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "        return encoding.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer classifier.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Output logits of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x) + self.pos_encoding[:, : x.size(1), :]\n",
    "        mask = x == 0  # Assuming 0 is the <PAD> index\n",
    "        out = self.transformer_encoder(embedded, src_key_padding_mask=mask)\n",
    "        pooled = out.mean(dim=1)  # mean pooling over sequence\n",
    "        return self.fc(self.dropout(pooled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(word2index)\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 128\n",
    "OUTPUT_SIZE = 1  # For binary sentiment analysis (1 or 0)\n",
    "N_LAYERS = 2\n",
    "IS_BIDIRECTIONAL = True\n",
    "DROPOUT_RATE = 0.5\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "N_HEAD = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ceeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    n_layers=N_LAYERS,\n",
    "    is_bidirectional=IS_BIDIRECTIONAL,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "transformer = TransformerClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBEDDING_SIZE,\n",
    "    nhead=N_HEAD,\n",
    "    num_layers=N_LAYERS,\n",
    "    hidden_dim=HIDDEN_SIZE,\n",
    "    max_len=SEQUENCE_LENGTH,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9951ce",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "### Accuracy\n",
    "Accuracy is the most common evaluation metric for classification problems, representing the percentage of correct predictions out of total predictions. It provides a simple measure of how often the classifier makes correct predictions across all classes.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} = \\dfrac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Samples}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f3efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy(\n",
    "    task=\"binary\",\n",
    "    num_classes=2,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b8b28",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "### Binary Cross-Entropy\n",
    "Binary Cross-Entropy loss function is used to measure the error between the true labels $y$ and the predicted probabilities $\\hat y$. The loss function is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "L = - \\dfrac{1}{n} \\sum_{i=1}^{n} y_{i} \\log(\\hat y_{i}) + (1-y_{i}) \\log(1- \\hat y_{i})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc81680",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102744d7",
   "metadata": {},
   "source": [
    "## Optimiser\n",
    "An optimiser in neural networks is used to adjust the parameters (weights and biases) of a model during training to minimise the loss. Optimisers are essential for enabling neural networks to learn from data: without them, the model would not improve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71096bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_optim = optim.Adam(lstm.parameters(), lr=LEARNING_RATE)\n",
    "transformer_optim = optim.Adam(\n",
    "    transformer.parameters(), lr=LEARNING_RATE, weight_decay=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70476cd8",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "1. Iterate through epochs\n",
    "1. For each epoch, iterate through training batches, perform training steps, calculate the train loss and evaluation metrics per batch.\n",
    "1. For each epoch, iterate through testing batches, perform testing steps, calculate the test loss and evaluation metrics per batch.\n",
    "1. Store the results.\n",
    "\n",
    "### Training Steps\n",
    "1. Zero the gradients\n",
    "    - Clear the gradients from the previous iteration to prevent accumulation across batches.\n",
    "1. Forward pass\n",
    "    - Pass inputs through the model to obtain predictions.\n",
    "1. Calculate loss and evaluation metrics per batch\n",
    "    - Measure how far the predictions deviate from the true labels using a loss function.\n",
    "    - Compute evaluation metrics (e.g., accuracy, F1 Score, etc.) for the current batch.\n",
    "1. Backward pass\n",
    "    - Compute gradients of the loss with respect to the model's parameters via backpropagation.\n",
    "    - Update the parameter $\\theta$ using the computed gradients, typically following:\n",
    "    $$\n",
    "        \\theta \\leftarrow \\theta - \\eta \\dfrac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "    $$\n",
    "    where $\\eta$ is the learning rate.\n",
    "1. Average training loss and evaluation metrics\n",
    "    - Calculate the mean loss and metric values across all batches in the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcc883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimiser: optim.Optimizer,\n",
    "    accuracy: Accuracy,\n",
    "    device: torch.device,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Perform one training epoch step.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        data_loader: DataLoader for training data.\n",
    "        criterion: Loss function.\n",
    "        optimiser: Optimizer.\n",
    "        accuracy: Torchmetrics accuracy metric.\n",
    "        device: Device to run the computations on.\n",
    "\n",
    "    Returns:\n",
    "        Average training loss and training accuracy (percentage).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    accuracy.reset()\n",
    "    train_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for texts, labels in data_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device).unsqueeze(1)\n",
    "\n",
    "        # Optimiser zero grad without intervening forward pass\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_logits = model(texts)  # Shape (batch_size, 1)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_logits, labels)\n",
    "        train_loss += loss.item() * texts.size(0)\n",
    "        total_samples += texts.size(0)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        y_probs = torch.sigmoid(y_logits)\n",
    "        y_preds = (y_probs >= 0.5).int()\n",
    "        accuracy.update(y_preds, labels.int())\n",
    "\n",
    "        # Loss backward for backpropagation (computing gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimiser step to apply gradients and update parameters\n",
    "        optimiser.step()\n",
    "    avg_train_loss = train_loss / total_samples\n",
    "    train_acc = accuracy.compute().item() * 100\n",
    "    return avg_train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0cd53",
   "metadata": {},
   "source": [
    "### Testing Steps\n",
    "1. Forward pass\n",
    "    - Set the model to evaluation mode (which disables dropout and batch normalisation and deactivates gradient tracking for safety).\n",
    "    - Pass inputs through the model to obtain predictions.\n",
    "\n",
    "1. Calculate loss and evaluation metrics per batch\n",
    "    - Measure how far the predictions deviate from the true labels using the loss function.\n",
    "    - Compute evaluation metrics (e.g., accuracy, F1-Score, etc.) for the current batch.\n",
    "\n",
    "1. Average test loss and evaluation metrics\n",
    "    - Calculate the mean loss and metric values across all batches in the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca721e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    accuracy: Accuracy,\n",
    "    device: torch.device,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Perform one validation epoch step.\n",
    "\n",
    "    Args:\n",
    "        model: The model to validate.\n",
    "        data_loader: DataLoader for validation data.\n",
    "        criterion: Loss function.\n",
    "        accuracy: Torchmetrics accuracy metric.\n",
    "        device: Device to run the computations on.\n",
    "\n",
    "    Returns:\n",
    "        Average validation loss and validation accuracy (percentage).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    accuracy.reset()\n",
    "    val_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.inference_mode():\n",
    "        for texts, labels in data_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "            # 1. Forward Pass\n",
    "            y_logits = model(texts)\n",
    "\n",
    "            # 2. Calculate loss\n",
    "            loss = criterion(y_logits, labels)\n",
    "            val_loss += loss.item() * texts.size(0)\n",
    "            total_samples += texts.size(0)\n",
    "\n",
    "            # 3. Calculate accuracy\n",
    "            y_probs = torch.sigmoid(y_logits)\n",
    "            y_preds = (y_probs >= 0.5).int()\n",
    "            accuracy.update(y_preds, labels.int())\n",
    "\n",
    "    avg_val_loss = val_loss / total_samples\n",
    "    val_acc = accuracy.compute().item() * 100\n",
    "    return avg_val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de7a59",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a57154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimiser: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    total_epochs: int,\n",
    ") -> tuple[Dict[str, List[float]], Dict[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Train and validate the model over a specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        criterion: Loss function.\n",
    "        optimiser: Optimizer.\n",
    "        device: Device to run computations on.\n",
    "        total_epochs: Number of training epochs.\n",
    "\n",
    "    Returns:\n",
    "        Dictionaries containing lists of training and validation losses and accuracies for each epoch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    epochs_range = range(1, total_epochs + 1)\n",
    "    train_results = {\"Loss\": [], \"Accuracy\": []}\n",
    "    val_results = {\"Loss\": [], \"Accuracy\": []}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in epochs_range:\n",
    "        train_loss, train_acc = train_step(\n",
    "            data_loader=train_loader,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            accuracy=accuracy,\n",
    "            device=device,\n",
    "        )\n",
    "        train_results[\"Loss\"].append(train_loss)\n",
    "        train_results[\"Accuracy\"].append(train_acc)\n",
    "\n",
    "        val_loss, val_acc = validation_step(\n",
    "            data_loader=val_loader,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            accuracy=accuracy,\n",
    "            device=device,\n",
    "        )\n",
    "        val_results[\"Loss\"].append(val_loss)\n",
    "        val_results[\"Accuracy\"].append(val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch}/{total_epochs} - \"\n",
    "            f\"Train Loss: {train_loss:.4f}  Train Accuracy: {train_acc:.2f}% | \"\n",
    "            f\"Val Loss: {val_loss:.4f}  Val Accuracy: {val_acc:.2f}%\"\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training and validation completed in {elapsed_time:.2f} seconds.\")\n",
    "    return train_results, val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190e493",
   "metadata": {},
   "source": [
    "### Performance Evaluation (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LSTM...\")\n",
    "lstm_train_results, lstm_val_results = train_and_validate(\n",
    "    lstm,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    lstm_optim,\n",
    "    device,\n",
    "    N_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7eb597",
   "metadata": {},
   "source": [
    "### Performance Evaluation (Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c8a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Transformer...\")\n",
    "transformer_train_results, transformer_val_results = train_and_validate(\n",
    "    transformer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    transformer_optim,\n",
    "    device,\n",
    "    N_EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e36ce2",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(\n",
    "    model_name: str,\n",
    "    total_epochs: int,\n",
    "    train_metrics: Dict[str, List[float]],\n",
    "    test_metrics: Dict[str, List[float]],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot training and testing metrics over epochs.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name of the model for plot titles.\n",
    "        total_epochs: Number of training epochs.\n",
    "        train_metrics: Dictionary of training metrics.\n",
    "        test_metrics: Dictionary of testing metrics.\n",
    "    \"\"\"\n",
    "    epochs_range = range(1, total_epochs + 1)\n",
    "    metric_names = list(train_metrics.keys())\n",
    "    n_metrics = len(metric_names)\n",
    "    print(metric_names)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        ax = axes[i]\n",
    "        ax.plot(\n",
    "            epochs_range, train_metrics[metric_name], label=f\"Train {metric_name}\"\n",
    "        )  # Train metric\n",
    "        ax.plot(\n",
    "            epochs_range, test_metrics[metric_name], label=f\"Test {metric_name}\"\n",
    "        )  # Test metric\n",
    "        ax.set_title(f\"{metric_name} Over Epochs ({model_name})\", fontsize=15)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(metric_name)\n",
    "\n",
    "    if n_metrics < len(axes):\n",
    "        for j in range(n_metrics, len(axes)):\n",
    "            plt.delaxes(axes[j])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(\"LSTM\", N_EPOCHS, lstm_train_results, lstm_val_results)\n",
    "plot_performance(\n",
    "    \"Transformer\", N_EPOCHS, transformer_train_results, transformer_val_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2b275",
   "metadata": {},
   "source": [
    "### Predictions on Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a13cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    accuracy: Accuracy,\n",
    "    device: torch.device,\n",
    "):\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    test_loss = 0.0\n",
    "    total_samples = 0\n",
    "    accuracy.reset()\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for texts, labels in data_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device).unsqueeze(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            y_logits = model(texts)\n",
    "            y_probs = torch.sigmoid(y_logits)\n",
    "            y_preds = (y_probs >= 0.5).int()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            accuracy.update(y_preds, labels.int())\n",
    "\n",
    "            # Update lists\n",
    "            all_predictions.append(y_preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(y_logits.view_as(labels), labels)\n",
    "            test_loss += loss.item() * texts.size(0)\n",
    "            total_samples += texts.size(0)\n",
    "    avg_test_loss = test_loss / total_samples\n",
    "    test_acc = accuracy.compute().item() * 100\n",
    "    print(f\"Test loss: {avg_test_loss:.4f} | Test acc: {test_acc:.2f}%\")\n",
    "    all_preds_tensor = torch.cat(all_predictions)\n",
    "    all_labels_tensor = torch.cat(all_labels)\n",
    "    return all_preds_tensor, all_labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8013dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing LSTM...\")\n",
    "all_preds_lstm, all_labels_lstm = make_predictions(\n",
    "    model=lstm,\n",
    "    data_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    accuracy=accuracy,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "wrong_indices_lstm = (all_preds_lstm != all_labels_lstm).nonzero(as_tuple=True)[0]\n",
    "print(\n",
    "    f\"Number of failed predictions: {len(wrong_indices_lstm)}/{len(all_labels_lstm)} ({100.0 * len(wrong_indices_lstm) / len(all_labels_lstm):.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b529734",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Transformer...\")\n",
    "all_preds_transformer, all_labels_transformer = make_predictions(\n",
    "    model=transformer,\n",
    "    data_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    accuracy=accuracy,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "wrong_indices_transformer = (all_preds_transformer != all_labels_transformer).nonzero(\n",
    "    as_tuple=True\n",
    ")[0]\n",
    "print(\n",
    "    f\"Number of failed predictions: {len(wrong_indices_transformer)}/{len(all_labels_transformer)} ({100.0 * len(wrong_indices_transformer) / len(all_labels_transformer):.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6625c",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# Convert to tensors if not already\n",
    "true_labels = all_labels_lstm\n",
    "preds_tensor_lstm = all_preds_lstm.detach().clone()\n",
    "preds_tensor_transformer = all_preds_transformer.detach().clone()\n",
    "class_names = [\"Negative\", \"Positive\"]\n",
    "model_names = [\"LSTM\", \"Transformer\"]\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = ConfusionMatrix(num_classes=2, task=\"binary\")\n",
    "conf_matrix_tensor_lstm = conf_matrix(preds_tensor_lstm, true_labels)\n",
    "conf_matrix_tensor_transformer = conf_matrix(preds_tensor_transformer, true_labels)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    conf_mat=conf_matrix_tensor_lstm.numpy(),\n",
    "    cmap=\"Blues\",\n",
    "    axis=axes[0],\n",
    ")\n",
    "plot_confusion_matrix(\n",
    "    conf_mat=conf_matrix_tensor_transformer.numpy(),\n",
    "    cmap=\"Blues\",\n",
    "    axis=axes[1],\n",
    ")\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_xticks(range(len(class_names)))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(range(len(class_names)))\n",
    "    ax.set_yticklabels(class_names)\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    ax.set_title(f\"Confusion Matrix ({model_names[i]})\", fontdict={\"fontsize\": 16})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933e087",
   "metadata": {},
   "source": [
    "## Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe13f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_text(\n",
    "    model: torch.nn.Module,\n",
    "    raw_text: str,\n",
    "    word2index: Dict[str, int],\n",
    "    max_seq_len: int,\n",
    "    device: torch.device,\n",
    ") -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Predict the label and confidence score for a single raw text input using the given model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained PyTorch model for classification.\n",
    "        raw_text: The raw input string to classify.\n",
    "        word2index: Mapping from token strings to their integer indices.\n",
    "        max_seq_len: The fixed input sequence length for the model.\n",
    "        device: The device on which to perform computation (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            pred: Predicted class label (0 or 1).\n",
    "            confidence: Confidence score between 0 and 1 indicating certainty about prediction.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cleaned = clean_text(raw_text)\n",
    "    tokens = cleaned.split()\n",
    "    indices = [word2index.get(word, word2index[\"<UNK>\"]) for word in tokens]\n",
    "    if len(indices) < max_seq_len:\n",
    "        indices += [word2index[\"<PAD>\"]] * (max_seq_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_seq_len]\n",
    "    input_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "        pred = int(prob >= 0.5)\n",
    "        confidence = abs(prob - 0.5) * 2\n",
    "    return pred, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67145f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"I really enjoyed this movie! The storyline was engaging and the characters were well-developed. The visuals were stunning, and the soundtrack perfectly matched the mood. Definitely worth watching!\"\n",
    "\n",
    "pred_lstm, confidence_lstm = predict_single_text(\n",
    "    lstm, text_1, word2index, SEQUENCE_LENGTH, device\n",
    ")\n",
    "pred_transformer, confidence_transformer = predict_single_text(\n",
    "    transformer, text_1, word2index, SEQUENCE_LENGTH, device\n",
    ")\n",
    "class_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "\n",
    "print(f\"Text: {text_1}\\n\")\n",
    "print(\n",
    "    f\"LSTM:\\t\\t Predicted class: {pred_lstm}({class_map[pred_lstm]}), Confidence: {confidence_lstm:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Transformer:\\t Predicted class: {pred_transformer}({class_map[pred_transformer]}), Confidence: {confidence_transformer:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"I was quite disappointed by this movie. The story felt predictable and dull, the characters were underdeveloped, and the pacing was painfully slow. I struggled to stay interested and wouldn't recommend it.\"\n",
    "\n",
    "pred_lstm, confidence_lstm = predict_single_text(\n",
    "    lstm, text_2, word2index, SEQUENCE_LENGTH, device\n",
    ")\n",
    "pred_transformer, confidence_transformer = predict_single_text(\n",
    "    transformer, text_2, word2index, SEQUENCE_LENGTH, device\n",
    ")\n",
    "class_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "\n",
    "print(f\"Text: {text_2}\\n\")\n",
    "print(\n",
    "    f\"LSTM:\\t\\t Predicted class: {pred_lstm}({class_map[pred_lstm]}), Confidence: {confidence_lstm:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Transformer:\\t Predicted class: {pred_transformer}({class_map[pred_transformer]}), Confidence: {confidence_transformer:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffb35f",
   "metadata": {},
   "source": [
    "## Pre-Trained Transformer Model (From Hugging Face)\n",
    "\n",
    "To use pre-trained transformer models from Hugging Face, the data preparation pipelines (from loading data to fine-tuning models) should be done using `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb92c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path=\"csv\",\n",
    "    data_files={\"all_data\": \"_datasets/IMDB_Dataset.csv\"},\n",
    ")\n",
    "\n",
    "dataset = dataset.rename_column(\"sentiment\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_int(sample: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert string sentiment labels ('positive'/'negative') to integer labels (1/0).\n",
    "\n",
    "    Args:\n",
    "        sample (Dict[str, Any]): A dictionary representing a data example with a 'labels' key.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The same dictionary with 'labels' converted to integer values.\n",
    "    \"\"\"\n",
    "\n",
    "    if sample[\"labels\"] == \"positive\":\n",
    "        sample[\"labels\"] = 1\n",
    "    else:\n",
    "        sample[\"labels\"] = 0\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Apply it on the full dataset\n",
    "dataset = dataset.map(label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "# 80% train, 10% validation and 10% test\n",
    "split = dataset[\"all_data\"].train_test_split(test_size=0.2, seed=RANDOM_SEED)\n",
    "train_dataset = split[\"train\"]\n",
    "subset = split[\"test\"]\n",
    "\n",
    "subset_split = subset.train_test_split(test_size=0.5, seed=RANDOM_SEED)\n",
    "val_dataset = subset_split[\"train\"]\n",
    "test_dataset = subset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc01054",
   "metadata": {},
   "source": [
    "### Tokenisation with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b472714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def tokenise(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tokenise a batch of text data using a tokeniser.\n",
    "\n",
    "    Args:\n",
    "        batch: A batch dictionary with key 'review' containing text string(s).\n",
    "\n",
    "    Returns:\n",
    "        Tokenised output including token ids, attention mask, etc.\n",
    "    \"\"\"\n",
    "    return tokeniser(\n",
    "        batch[\"review\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_train_dataset = train_dataset.map(tokenise, batched=True)\n",
    "tokenised_val_dataset = val_dataset.map(tokenise, batched=True)\n",
    "tokenised_test_dataset = test_dataset.map(tokenise, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea94bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokeniser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369ad0d",
   "metadata": {},
   "source": [
    "### Reformatting for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb39d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_train_dataset.set_format(\n",
    "    \"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "tokenised_val_dataset.set_format(\n",
    "    \"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "tokenised_test_dataset.set_format(\n",
    "    \"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465ae14",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1c244e",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c042ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    eval_pred: Tuple[NDArray[np.float64], NDArray[np.int64]],\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for model outputs and true labels.\n",
    "\n",
    "    Args:\n",
    "        eval_pred: Tuple containing raw model logits and target labels.\n",
    "            - logits: Array of shape (batch_size, num_classes)\n",
    "            - labels: Array of shape (batch_size,)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of computed metric scores, e.g., {'accuracy': 0.95}.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255853ff",
   "metadata": {},
   "source": [
    "### Loading Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d53453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 2  # Binary Classification\n",
    "hf_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2657b07",
   "metadata": {},
   "source": [
    "### Training Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=hf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_train_dataset,\n",
    "    eval_dataset=tokenised_val_dataset,\n",
    "    processing_class=tokeniser,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe022bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./my_finetuned_model\")\n",
    "tokeniser.save_pretrained(\"./my_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c2d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MODEL_PATH = \"./my_finetuned_model\"\n",
    "my_tokeniser = AutoTokenizer.from_pretrained(MY_MODEL_PATH)\n",
    "my_model = AutoModelForSequenceClassification.from_pretrained(MY_MODEL_PATH)\n",
    "\n",
    "inputs = my_tokeniser(text_1, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = my_model(**inputs).logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "\n",
    "print(\n",
    "    f\"Predicted: {predicted_class_id} ({my_model.config.id2label[predicted_class_id]})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=MY_MODEL_PATH)\n",
    "print(classifier(text_1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
