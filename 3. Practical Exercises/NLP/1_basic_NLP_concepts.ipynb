{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a94967",
   "metadata": {},
   "source": [
    "# Basic NLP Concepts\n",
    "***\n",
    "## Table of Contents\n",
    "1. Introduction to NLP\n",
    "    - Types of NLP\n",
    "    - NLP Tasks\n",
    "    - Popular NLP Libraries\n",
    "2. Text Preprocessing\n",
    "    - Text Cleaning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8e6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036a812",
   "metadata": {},
   "source": [
    "## 1. Introduction to NLP\n",
    "\n",
    "Natural Language Processing (NLP) is a multidisciplinary field that combines computer science, linguistics, and artificial intelligence to enable computers to interpret, process, and generate human language naturally and efficiently. NLP bridges the gap between human communication and computer understanding, allowing machines to analyse, understand, and even respond to text and speech just as humans do.\n",
    "\n",
    "\n",
    "### Types of NLP\n",
    "There are several types and approaches within NLP, each with its own focus and methodology:\n",
    "\n",
    "- **Symbolic NLP**: Relies on hand-crafted rules and linguistic knowledge to process language. This traditional approach uses grammar rules and dictionaries to interpret text.\n",
    "\n",
    "- **Statistical NLP**: Uses statistical methods and machine learning to analyze large volumes of language data, identifying patterns and making predictions based on probabilities.\n",
    "\n",
    "- **Neural NLP**: Employs deep learning and neural networks to model and understand language, enabling advanced applications like language generation and large language models.\n",
    "\n",
    "\n",
    "### NLP Tasks\n",
    "NLP can be divided into several overlapping subfields and tasks, including:\n",
    "\n",
    "- **Natural Language Understanding (NLU)**: Focuses on interpreting and extracting meaning from human language (semantics and syntax).\n",
    "- **Natural Language Generation (NLG)**: Involved in generating human-like text or speech from structured data or input.\n",
    "- **Speech Recognition**: Converts spoken language into text.\n",
    "- **Text Classification**: Assigns categories or labels to text data (e.g., spam detection, topic classification).\n",
    "- **Named Entity Recognition (NER)**: Identifies and classifies entities such as names, locations, and organisations in text.\n",
    "- **Sentiment Analysis**: Determines the emotional tone behind a body of text.\n",
    "- **Machine Translation**: Automatically translates text or speech from one language to another.\n",
    "- **Part-of-Speech Tagging**: Labels words with their grammatical roles (noun, verb, etc.).\n",
    "\n",
    "### Popular NLP Libraries\n",
    "The main Python libraries used in NLP are:\n",
    "- **NLTK (Natural Language Toolkit)**: One of the oldest and most compherensive libraries for NLP tasks such as tokenisation, stemming, tagging, parsing, and semantic reasoning. Widely used for teaching, research, and foundational NLP projects, though it may be slower for large-scale production.\n",
    "- **spaCy**: Designed for fast, efficient, and production-ready NLP applications. It offered pre-trained models for multiple languages, supports tokenisation, part-of-speech tagging, named entity recognition, dependency parsing, and integrates well with deep learning frameworks.\n",
    "- **Gensim**: Specialised in topic modelling, document similarity analysis, and word embeddings (e.g. Word2Vec, FastText, LDA). It's optimised for processing large text corpora efficiently and is popular for unsupervised NLP tasks.\n",
    "- **TextBlob**: Build on top of NLTK and Pattern. TextBlob provides a simple API for common NLP tasks like sentiment analysis, part-of-speech tagging, and noun phrase extraction. It's user-friendly and great for beginners or rapid prototyping.\n",
    "- **Pattern**: Offers tools for text processing, web mining, machine learning, and network analysis. Known for its easy use and is suitable for tasks like sentiment analysis, part-of-speech tagging, and web scraping.\n",
    "- **PyNLPl(Pineapple)**: A versatile library for both basic and advanced NLP tasks, including n-gram analysis, frequency lists, and linguistic annotation. It supports various file formats and is useful for more specialised NLP workflows.\n",
    "- **Stanza** Developed by Stanford, Stanza provides deep learning-based models for tasks such as named entity recognition and part-of-speech tagging, supporting over 70 languages and integrating well with other libraries (e.g., spaCy, Hugging Face Transformers).\n",
    "- **Polyglot**: Known for its extensive multilingual support. Polyglot offers tokenisation, sentimental analysis, named entity recognition, and word embeddings across 130+ languages.\n",
    "- **CoreNLP**: A robust Java-based library from Stanford, accessible in Python via wrappers, used for tasks such as named entity recognition and coreference resolution. Often integrated with other Python NLP libraries.\n",
    "- **Hugging Face Transformers**: While primarily for large language models, this library is widely used in modern NLP for tasks (e.g., text classification, question answering, text generation using transformer-based models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af9f2d",
   "metadata": {},
   "source": [
    "## 2. Regular Expressions\n",
    "Regular expressions (also called 'regex or 'regexp') are patterns used to match, search, and manipulate text based on specific sequences of characters. They are extremely useful for extracting information, validating input, finding specific text, and replacing or splitting strings in tasks such as data cleaning, web scraping, and natural language processing.\n",
    "\n",
    "A regular expression is essentially a sequence of characters that defines a search pattern. This pattern can be made up of literal characters or special symbols (metacharacters) that represent sets, repetitions, or positions in the text.\n",
    "\n",
    "For example:\n",
    "- `/cat/` matches the exact sequence 'cat'.\n",
    "- `/c.t/` matches 'cat', 'cot', 'cut', etc. (the dot `.` matches any single character).\n",
    "- `/\\d+/` matches one or more digits (`\\d` means any digit and `+` means 'one or more').\n",
    "\n",
    "\n",
    "### Common Regex Elements\n",
    "- **Literal Characters**: Match themselves (e.g., a, 1, @)\n",
    "- **Metacharacters**:\n",
    "    - `.` (dot): Any character except newline.\n",
    "    - `\\d`: Any digit(0-9).\n",
    "    - `\\w`: Any word character (letters, digits, underscore).\n",
    "    - `\\s`: Any whitespace character (space, tab, newline).\n",
    "    - `*`: Zero or more of the preceding elements.\n",
    "    - `+`: One or more of the preceding elements.\n",
    "    - `?`: Zero or one of the preceding element.\n",
    "    - `[]`: A set or range of characters (e.g., [a-z])\n",
    "    - `^`: Start of a string.\n",
    "    - `$`: End of a string.\n",
    "    - `|`: OR operator (e.g., `cat|dog` matches 'cat' or 'dog').\n",
    "    - `()`: Grouping for subpatterns.\n",
    "\n",
    "### Example Use Cases\n",
    "- **Remove punctuation**: `r'[^\\w\\s]` matches anything that is not a word character or whitespace.\n",
    "- **Find email addresses**: `r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b'`\n",
    "- **Validate phone numbers**: Patterns like `r'^\\d{3}-\\d{3}-\\d{4}$'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26efc9b",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "Text preprocessing is the foundation of any NLP project. It involves cleaning and transforming raw text into a structured format suitable for analysis.\n",
    "\n",
    "Typical Text Preprocessing Pipeline is:\n",
    "1. **Text Cleaning**: Removes unnecessary characters, HTML, emojis, etc.\n",
    "2. **Text Normalisation**: Lowercases, expands contractions, removes punctuation, standardises spelling.\n",
    "3. **Tokenisation**: Splits text into words, sentences, or subwords.\n",
    "4. **Stop Word Removal**: Eliminates meaningless words.\n",
    "5. **Stemming / Lemmatisation**: Reduces words to their root form or dictionary form.\n",
    "6. **Part-of-Speech (POS) Tagging**: Assigns grammatical tags (noun, verb, adjective, etc.) to each token.\n",
    "\n",
    "Dataset retrieved from [Tweets Dataset](https://www.kaggle.com/datasets/mmmarchetti/tweets-dataset?select=tweets.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3afb410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "00ca1065-0b78-4633-844d-e7b85d520e9c",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK"
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df"
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl"
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd"
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...\n",
       "1  @barackobama Thank you for your incredible gra...\n",
       "2                Life goals. https://t.co/XIn1qKMKQl\n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd\n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('_datasets/tweets.csv')\n",
    "df = df.drop(columns=['author', 'country', 'date_time', 'id', 'language',\n",
    "             'latitude', 'longitude', 'number_of_likes', 'number_of_shares'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86561ed7",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "Text cleaning is a crucial first step in preparing raw text data for NLP or machine learning. It removes noise and irrelevant elements, making the data more structured and suitable for analysis.\n",
    "\n",
    "#### Removing HTML Tags\n",
    "HTML tags can appear in text scraped from web pages. Use regex to remove anything between `<` and `>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65364a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "589bf540-9144-423f-b494-3ffa224a61b5",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK"
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df"
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl",
         "Life goals. https://t.co/XIn1qKMKQl"
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd",
         "Me right now üôèüèª https://t.co/gW55C1wrwd"
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...   \n",
       "1  @barackobama Thank you for your incredible gra...   \n",
       "2                Life goals. https://t.co/XIn1qKMKQl   \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd   \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...  \n",
       "1  @barackobama Thank you for your incredible gra...  \n",
       "2                Life goals. https://t.co/XIn1qKMKQl  \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd  \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['content'].str.replace(r'<.*?>', '', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e9bce",
   "metadata": {},
   "source": [
    "#### Removing URLs\n",
    "URLs are often irrelevant for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3e16f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7311aee3-c0ed-4a55-878d-6c454cf22860",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK",
         "Is history repeating itself...?#DONTNORMALIZEHATE "
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ "
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl",
         "Life goals. "
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd",
         "Me right now üôèüèª "
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è "
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>Life goals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>Me right now üôèüèª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...   \n",
       "1  @barackobama Thank you for your incredible gra...   \n",
       "2                Life goals. https://t.co/XIn1qKMKQl   \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd   \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...  \n",
       "1  @barackobama Thank you for your incredible gra...  \n",
       "2                                       Life goals.   \n",
       "3                                   Me right now üôèüèª   \n",
       "4       SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].str.replace(\n",
    "    r'http\\S+|www\\.\\S+', '', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9614ed",
   "metadata": {},
   "source": [
    "#### Removing Emojis and Non-ASCII Characters\n",
    "Emojis and non-ASCII symbols can be noise for many NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c127255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "82c8da45-7234-41d0-a5c8-8eeebe4c269c",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK",
         "Is history repeating itself...?#DONTNORMALIZEHATE "
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional "
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl",
         "Life goals. "
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd",
         "Me right now  "
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv",
         "SISTERS ARE DOIN' IT FOR THEMSELVES!  "
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>Life goals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>Me right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...   \n",
       "1  @barackobama Thank you for your incredible gra...   \n",
       "2                Life goals. https://t.co/XIn1qKMKQl   \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd   \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...  \n",
       "1  @barackobama Thank you for your incredible gra...  \n",
       "2                                       Life goals.   \n",
       "3                                     Me right now    \n",
       "4             SISTERS ARE DOIN' IT FOR THEMSELVES!    "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].str.replace(\n",
    "    r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439448d",
   "metadata": {},
   "source": [
    "#### Removing Mentions (For Twitter data)\n",
    "Removing mentions (usernames starting with `@`) is a common step in cleaning Twitter data, as these are often not useful for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d95690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "915fe07d-2738-42a0-b901-5e16e981cd1a",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK",
         "Is history repeating itself...?#DONTNORMALIZEHATE "
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df",
         " Thank you for your incredible grace in leadership and for being an exceptional "
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl",
         "Life goals. "
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd",
         "Me right now  "
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv",
         "SISTERS ARE DOIN' IT FOR THEMSELVES!  "
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>Thank you for your incredible grace in leader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>Life goals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>Me right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...   \n",
       "1  @barackobama Thank you for your incredible gra...   \n",
       "2                Life goals. https://t.co/XIn1qKMKQl   \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd   \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...  \n",
       "1   Thank you for your incredible grace in leader...  \n",
       "2                                       Life goals.   \n",
       "3                                     Me right now    \n",
       "4             SISTERS ARE DOIN' IT FOR THEMSELVES!    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].str.replace(r'@\\w+', '', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7808e8",
   "metadata": {},
   "source": [
    "#### Removing Hashtags (For Twitter data, Optional)\n",
    "Whether we should remove hashtags (words starting with `#`) from Twitter data depends on our analysis goals:\n",
    "\n",
    "- If hashtags are not useful for our NLP task (e.g., general sentiment analysis, topic modeling where hashtags add noise or redundancy), we can remove them just like mentions and URLs.\n",
    "\n",
    "- If hashtags carry important information (e.g., event detection, trend analysis, or if hashtags are meaningful keywords), we might want to keep them, or extract and analyse them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05be1b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "4e5ddcb4-1f9f-4493-8171-bbbab731f8c3",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK",
         "Is history repeating itself...? "
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df",
         " Thank you for your incredible grace in leadership and for being an exceptional "
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl",
         "Life goals. "
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd",
         "Me right now  "
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv",
         "SISTERS ARE DOIN' IT FOR THEMSELVES!  "
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>Is history repeating itself...?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>Thank you for your incredible grace in leader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>Life goals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>Me right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...   \n",
       "1  @barackobama Thank you for your incredible gra...   \n",
       "2                Life goals. https://t.co/XIn1qKMKQl   \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd   \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...   \n",
       "\n",
       "                                          clean_text  \n",
       "0                   Is history repeating itself...?   \n",
       "1   Thank you for your incredible grace in leader...  \n",
       "2                                       Life goals.   \n",
       "3                                     Me right now    \n",
       "4             SISTERS ARE DOIN' IT FOR THEMSELVES!    "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].str.replace(r'#\\w+', '', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f1781",
   "metadata": {},
   "source": [
    "#### Removing Extra Whitespace\n",
    "Normalise whitespace to a single space and trim leading/trailing spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e3540a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ac593ea8-862c-415c-ad18-4bb0176b8013",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK",
         "Is history repeating itself...?"
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df",
         "Thank you for your incredible grace in leadership and for being an exceptional"
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl",
         "Life goals."
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd",
         "Me right now"
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv",
         "SISTERS ARE DOIN' IT FOR THEMSELVES!"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>Is history repeating itself...?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>Thank you for your incredible grace in leaders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>Life goals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>Me right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...   \n",
       "1  @barackobama Thank you for your incredible gra...   \n",
       "2                Life goals. https://t.co/XIn1qKMKQl   \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd   \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...   \n",
       "\n",
       "                                          clean_text  \n",
       "0                    Is history repeating itself...?  \n",
       "1  Thank you for your incredible grace in leaders...  \n",
       "2                                        Life goals.  \n",
       "3                                       Me right now  \n",
       "4               SISTERS ARE DOIN' IT FOR THEMSELVES!  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].str.replace(\n",
    "    r'\\s+', ' ', regex=True).str.strip()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c006547",
   "metadata": {},
   "source": [
    "#### Removing Numbers (Optional)\n",
    "If numbers are not relevant to our analysis, we should remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eddf4edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bdb4011e-3133-43b0-85fe-54c74fb2e116",
       "rows": [
        [
         "5",
         "happy 96th gma #fourmoreyears! üéà @ LACMA Los Angeles County Museum of Art https://t.co/M9n7X8xdmA",
         "happy  th gma ! @ LACMA Los Angeles County Museum of Art"
        ],
        [
         "6",
         "Kyoto, Japan \r\n1. 5. 17. https://t.co/o28M0vw9lR",
         "Kyoto, Japan  .  .  ."
        ],
        [
         "7",
         "üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx",
         "@ Sanrio Puroland"
        ],
        [
         "8",
         "2017 resolution: to embody authenticity!",
         "resolution: to embody authenticity!"
        ],
        [
         "9",
         "sisters. https://t.co/5ZE21x2aNk",
         "sisters."
        ],
        [
         "10",
         "Happy Holidays! Sending love and light to every corner of the earth üéÅ",
         "Happy Holidays! Sending love and light to every corner of the earth"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>happy 96th gma #fourmoreyears! üéà @ LACMA Los A...</td>\n",
       "      <td>happy  th gma ! @ LACMA Los Angeles County Mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kyoto, Japan \\r\\n1. 5. 17. https://t.co/o28M0v...</td>\n",
       "      <td>Kyoto, Japan  .  .  .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx</td>\n",
       "      <td>@ Sanrio Puroland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017 resolution: to embody authenticity!</td>\n",
       "      <td>resolution: to embody authenticity!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sisters. https://t.co/5ZE21x2aNk</td>\n",
       "      <td>sisters.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Happy Holidays! Sending love and light to ever...</td>\n",
       "      <td>Happy Holidays! Sending love and light to ever...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  \\\n",
       "5   happy 96th gma #fourmoreyears! üéà @ LACMA Los A...   \n",
       "6   Kyoto, Japan \\r\\n1. 5. 17. https://t.co/o28M0v...   \n",
       "7        üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx   \n",
       "8            2017 resolution: to embody authenticity!   \n",
       "9                    sisters. https://t.co/5ZE21x2aNk   \n",
       "10  Happy Holidays! Sending love and light to ever...   \n",
       "\n",
       "                                           clean_text  \n",
       "5   happy  th gma ! @ LACMA Los Angeles County Mus...  \n",
       "6                               Kyoto, Japan  .  .  .  \n",
       "7                                   @ Sanrio Puroland  \n",
       "8                 resolution: to embody authenticity!  \n",
       "9                                            sisters.  \n",
       "10  Happy Holidays! Sending love and light to ever...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].str.replace(\n",
    "    r'\\d+', ' ', regex=True).str.strip()\n",
    "df.iloc[5:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decc2a0",
   "metadata": {},
   "source": [
    "#### Removing Line Breaks (Optional)\n",
    "If the data contains line breaks (`\\n`), replace them with spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a68056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "4bd477a8-4642-4320-9568-57125c6218ea",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK",
         "Is history repeating itself...?"
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df",
         "Thank you for your incredible grace in leadership and for being an exceptional"
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl",
         "Life goals."
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd",
         "Me right now"
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv",
         "SISTERS ARE DOIN' IT FOR THEMSELVES!"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>Is history repeating itself...?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>Thank you for your incredible grace in leaders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>Life goals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>Me right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...   \n",
       "1  @barackobama Thank you for your incredible gra...   \n",
       "2                Life goals. https://t.co/XIn1qKMKQl   \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd   \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...   \n",
       "\n",
       "                                          clean_text  \n",
       "0                    Is history repeating itself...?  \n",
       "1  Thank you for your incredible grace in leaders...  \n",
       "2                                        Life goals.  \n",
       "3                                       Me right now  \n",
       "4               SISTERS ARE DOIN' IT FOR THEMSELVES!  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].str.replace(r'[\\r\\n]+', ' ', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6851d56",
   "metadata": {},
   "source": [
    "### Text Normalisaton\n",
    "Text normalisation is the process of converting text data into a consistent, standardised, and canonical form, making it easier for computers to process and analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49dfc6",
   "metadata": {},
   "source": [
    "#### Spelling Standardisation\n",
    "Misspelled or inconsistently spelled words (e.g., 'colour' vs 'color', 'real-time' vs 'realtime', or 'hellooo' vs 'hello') can increase vocabulary size and reduce model accuracy. Spelling standardisation (also called spelling correction) helps models treat all variants as the same word, improving downstream analysis and learning. For example:\n",
    "\n",
    "- `pyspellchecker` is a context-free spell checker. It chooses the most common word within a certain edit distance, regardless of sentence meaning.\n",
    "- `TextBlob` uses a probabilistic model for spelling correction.\n",
    "\n",
    "- `Transformers` provide more advanced context-aware corrections.\n",
    "\n",
    "Popular Transformer Spell Checker Models & Toolkits:\n",
    "\n",
    "| Model/Toolkit                               | Description                                                                                 |\n",
    "|----------------------------------------------|---------------------------------------------------------------------------------------------|\n",
    "| oliverguhr/spelling-correction-english-base | Hugging Face model for English spelling and punctuation correction.                         |\n",
    "| ai-forever/T5-large-spell                   | T5-based model for standard English spelling correction.                                    |\n",
    "| NeuSpell                                    | Toolkit with several neural spell checkers, including BERT-based models.                   |\n",
    "| xfspell                                     | Transformer-based English spell checker trained on millions of parallel sentences.          |\n",
    "| Custom T5/BERT models                       | Many researchers fine-tune T5 or BERT for spelling correction as a text-to-text task.       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc6b5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction for \"grous\": group\n",
      "Candidates for \"grous\": {'group', 'grus', 'grouse', 'grows', 'gros', 'grots', 'groups', 'gross', 'grouts', 'grout'}\n",
      "Correction for \"wlak\": walk\n",
      "Candidates for \"wlak\": {'weak', 'flak', 'walk'}\n"
     ]
    }
   ],
   "source": [
    "# Using 'pyspellchecker' library\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Initialise SpellChecker class\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Example list of words\n",
    "words = ['let', 'us', 'wlak', 'on', 'the', 'grous']\n",
    "\n",
    "# Identify misspelled words\n",
    "misspelled = spell.unknown(words)\n",
    "for word in misspelled:\n",
    "    # Most probabilistic correction\n",
    "    print(f'Correction for \"{word}\": {spell.correction(word)}')\n",
    "    # All possible candidates\n",
    "    print(f'Candidates for \"{word}\": {spell.candidates(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d57feb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "# Using 'TextBlob' library\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = 'I havv goood speling!'\n",
    "corrected_text = str(TextBlob(text).correct())\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6fe7f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected: She did not go to the market.\n"
     ]
    }
   ],
   "source": [
    "# Using Transformers (oliverguhr/spelling-correction-english-base)\n",
    "from transformers import pipeline\n",
    "\n",
    "corrector = pipeline('text2text-generation',\n",
    "                     model='oliverguhr/spelling-correction-english-base')\n",
    "sentence = 'She did no go to teh marcket.'\n",
    "result = corrector(sentence)\n",
    "print('Corrected:', result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadabb2",
   "metadata": {},
   "source": [
    "Create a function to apply transformers to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e327839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    # Handle empty or missing text\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return text\n",
    "    result = corrector(text)\n",
    "    return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ec75eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b5d27979-cc79-4b7e-9d15-5f6e4e60048c",
       "rows": [
        [
         "0",
         "Is history repeating itself?"
        ],
        [
         "1",
         "Thank you for your incredible grace in leadership and for being an exceptional."
        ],
        [
         "2",
         "Life goals."
        ],
        [
         "3",
         "Me right now."
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES!"
        ],
        [
         "5",
         "Happy  the gma ! at LACMA. Los Angeles County Museum of Art."
        ],
        [
         "6",
         "Kyoto, Japan    .  ."
        ],
        [
         "7",
         "At Sanrio Puroland."
        ],
        [
         "8",
         "resolution to embody authenticity!"
        ],
        [
         "9",
         "Sisters."
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "0                         Is history repeating itself?\n",
       "1    Thank you for your incredible grace in leaders...\n",
       "2                                          Life goals.\n",
       "3                                        Me right now.\n",
       "4                 SISTERS ARE DOIN' IT FOR THEMSELVES!\n",
       "5    Happy  the gma ! at LACMA. Los Angeles County ...\n",
       "6                                 Kyoto, Japan    .  .\n",
       "7                                  At Sanrio Puroland.\n",
       "8                   resolution to embody authenticity!\n",
       "9                                             Sisters.\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! Execution time is very long ...\n",
    "# df['clean_text'] = df['clean_text'].apply(correct_spelling)\n",
    "# df.head()\n",
    "df_transformers_top10 = df['clean_text'].iloc[:10].apply(correct_spelling)\n",
    "df_transformers_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c94069",
   "metadata": {},
   "source": [
    "#### Lowercasing\n",
    "Convert all text to lowercase to ensure uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce9b85ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "54f73ab6-fb7c-42fe-96aa-b605e4565eea",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK",
         "is history repeating itself...?"
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df",
         "thank you for your incredible grace in leadership and for being an exceptional"
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl",
         "life goals."
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd",
         "me right now"
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv",
         "sisters are doin' it for themselves!"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>is history repeating itself...?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>thank you for your incredible grace in leaders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>life goals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>me right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>sisters are doin' it for themselves!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...   \n",
       "1  @barackobama Thank you for your incredible gra...   \n",
       "2                Life goals. https://t.co/XIn1qKMKQl   \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd   \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...   \n",
       "\n",
       "                                          clean_text  \n",
       "0                    is history repeating itself...?  \n",
       "1  thank you for your incredible grace in leaders...  \n",
       "2                                        life goals.  \n",
       "3                                       me right now  \n",
       "4               sisters are doin' it for themselves!  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b9f18",
   "metadata": {},
   "source": [
    "#### Expanding Contractions\n",
    "Replace contractions with their expanded forms for clarity and consistency. For the code below:\n",
    "- `contraction` is the string to be replaced (e.g., *doin'*), and `expanded` is what it should become (e.g., *doing*).\n",
    "- `re.escape(contraction)` ensures that any special characters in the contraction string (such as apostrophes) are treated as literal characters, not as regex operators.\n",
    "- `re.compile(..., re.IGNORECASE)` creates a regex pattern object that will match the contraction in a case-insensitive way (so it matches *Doin'*, *DOIN'*, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a9c124a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "8ffdf843-e477-4517-b7fa-381cfbcfbd08",
       "rows": [
        [
         "0",
         "Is history repeating itself...?#DONTNORMALIZEHATE https://t.co/ngG11quhmK",
         "is history repeating itself...?"
        ],
        [
         "1",
         "@barackobama Thank you for your incredible grace in leadership and for being an exceptional‚Ä¶ https://t.co/ZuQLZpt6df",
         "thank you for your incredible grace in leadership and for being an exceptional"
        ],
        [
         "2",
         "Life goals. https://t.co/XIn1qKMKQl",
         "life goals."
        ],
        [
         "3",
         "Me right now üôèüèª https://t.co/gW55C1wrwd",
         "me right now"
        ],
        [
         "4",
         "SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è https://t.co/0shuUYUBEv",
         "sisters are doing it for themselves!"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is history repeating itself...?#DONTNORMALIZEH...</td>\n",
       "      <td>is history repeating itself...?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@barackobama Thank you for your incredible gra...</td>\n",
       "      <td>thank you for your incredible grace in leaders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Life goals. https://t.co/XIn1qKMKQl</td>\n",
       "      <td>life goals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me right now üôèüèª https://t.co/gW55C1wrwd</td>\n",
       "      <td>me right now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...</td>\n",
       "      <td>sisters are doing it for themselves!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Is history repeating itself...?#DONTNORMALIZEH...   \n",
       "1  @barackobama Thank you for your incredible gra...   \n",
       "2                Life goals. https://t.co/XIn1qKMKQl   \n",
       "3            Me right now üôèüèª https://t.co/gW55C1wrwd   \n",
       "4  SISTERS ARE DOIN' IT FOR THEMSELVES! üôåüèªüí™üèª‚ù§Ô∏è ht...   \n",
       "\n",
       "                                          clean_text  \n",
       "0                    is history repeating itself...?  \n",
       "1  thank you for your incredible grace in leaders...  \n",
       "2                                        life goals.  \n",
       "3                                       me right now  \n",
       "4               sisters are doing it for themselves!  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def expand_contractions_only(text):\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\", \"won't\": \"will not\", \"it's\": \"it is\",\n",
    "        \"I'm\": \"I am\", \"you're\": \"you are\", \"they're\": \"they are\",\n",
    "        \"he's\": \"he is\", \"she's\": \"she is\", \"we're\": \"we are\",\n",
    "        \"that's\": \"that is\", \"what's\": \"what is\", \"where's\": \"where is\",\n",
    "        \"who's\": \"who is\", \"how's\": \"how is\", \"let's\": \"let us\",\n",
    "        \"doin'\": \"doing\", \"goin'\": \"going\", \"tryin'\": \"trying\"\n",
    "    }\n",
    "    for contraction, expanded in contractions.items():\n",
    "        # Pattern matches contraction anywhere, case-insensitive\n",
    "        pattern = re.compile(re.escape(contraction), re.IGNORECASE)\n",
    "        text = pattern.sub(expanded, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Apply only contraction expansion to the 'clean_text' column\n",
    "df['clean_text'] = df['clean_text'].apply(expand_contractions_only)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6459b7d",
   "metadata": {},
   "source": [
    "#### Removing Punctuation and Special Characters\n",
    "Strip out punctuation, symbols, and special characters to reduce noise. Using `string.punctuation` makes the task easy and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a04f757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3b065",
   "metadata": {},
   "source": [
    "`'[{}]'.format(string.punctuation)` inserts all these punctuation characters inside square brackets, resulting in a string like `'[!\"#$%&\\'()*+,-./:;<=>?@[\\$$^_{|}~]'`. Setting `regex=True` tells pandas to interpret the pattern as a regular expression. In case our data contains regex-special characters (such as `[`, `\\`, `^`), it's safer to escape them using `re.escape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "917806aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5e3c9292-e662-43dd-8b72-cb12fd35eb91",
       "rows": [
        [
         "5",
         "happy 96th gma #fourmoreyears! üéà @ LACMA Los Angeles County Museum of Art https://t.co/M9n7X8xdmA",
         "happy  th gma   lacma los angeles county museum of art"
        ],
        [
         "6",
         "Kyoto, Japan \r\n1. 5. 17. https://t.co/o28M0vw9lR",
         "kyoto japan      "
        ],
        [
         "7",
         "üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx",
         " sanrio puroland"
        ],
        [
         "8",
         "2017 resolution: to embody authenticity!",
         "resolution to embody authenticity"
        ],
        [
         "9",
         "sisters. https://t.co/5ZE21x2aNk",
         "sisters"
        ],
        [
         "10",
         "Happy Holidays! Sending love and light to every corner of the earth üéÅ",
         "happy holidays sending love and light to every corner of the earth"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>happy 96th gma #fourmoreyears! üéà @ LACMA Los A...</td>\n",
       "      <td>happy  th gma   lacma los angeles county museu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kyoto, Japan \\r\\n1. 5. 17. https://t.co/o28M0v...</td>\n",
       "      <td>kyoto japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx</td>\n",
       "      <td>sanrio puroland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017 resolution: to embody authenticity!</td>\n",
       "      <td>resolution to embody authenticity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sisters. https://t.co/5ZE21x2aNk</td>\n",
       "      <td>sisters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Happy Holidays! Sending love and light to ever...</td>\n",
       "      <td>happy holidays sending love and light to every...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  \\\n",
       "5   happy 96th gma #fourmoreyears! üéà @ LACMA Los A...   \n",
       "6   Kyoto, Japan \\r\\n1. 5. 17. https://t.co/o28M0v...   \n",
       "7        üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx   \n",
       "8            2017 resolution: to embody authenticity!   \n",
       "9                    sisters. https://t.co/5ZE21x2aNk   \n",
       "10  Happy Holidays! Sending love and light to ever...   \n",
       "\n",
       "                                           clean_text  \n",
       "5   happy  th gma   lacma los angeles county museu...  \n",
       "6                                   kyoto japan        \n",
       "7                                     sanrio puroland  \n",
       "8                   resolution to embody authenticity  \n",
       "9                                             sisters  \n",
       "10  happy holidays sending love and light to every...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "df['clean_text'] = df['clean_text'].str.replace(\n",
    "    '[{}]'.format(re.escape(string.punctuation)), '', regex=True)\n",
    "df.iloc[5:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6fe8cd",
   "metadata": {},
   "source": [
    "Alternatively, without using regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "780eebca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "c70d116e-5d08-42c6-b88f-3a17142b8e29",
       "rows": [
        [
         "5",
         "happy 96th gma #fourmoreyears! üéà @ LACMA Los Angeles County Museum of Art https://t.co/M9n7X8xdmA",
         "happy  th gma   lacma los angeles county museum of art"
        ],
        [
         "6",
         "Kyoto, Japan \r\n1. 5. 17. https://t.co/o28M0vw9lR",
         "kyoto japan      "
        ],
        [
         "7",
         "üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx",
         " sanrio puroland"
        ],
        [
         "8",
         "2017 resolution: to embody authenticity!",
         "resolution to embody authenticity"
        ],
        [
         "9",
         "sisters. https://t.co/5ZE21x2aNk",
         "sisters"
        ],
        [
         "10",
         "Happy Holidays! Sending love and light to every corner of the earth üéÅ",
         "happy holidays sending love and light to every corner of the earth"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>happy 96th gma #fourmoreyears! üéà @ LACMA Los A...</td>\n",
       "      <td>happy  th gma   lacma los angeles county museu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kyoto, Japan \\r\\n1. 5. 17. https://t.co/o28M0v...</td>\n",
       "      <td>kyoto japan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx</td>\n",
       "      <td>sanrio puroland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017 resolution: to embody authenticity!</td>\n",
       "      <td>resolution to embody authenticity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sisters. https://t.co/5ZE21x2aNk</td>\n",
       "      <td>sisters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Happy Holidays! Sending love and light to ever...</td>\n",
       "      <td>happy holidays sending love and light to every...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  \\\n",
       "5   happy 96th gma #fourmoreyears! üéà @ LACMA Los A...   \n",
       "6   Kyoto, Japan \\r\\n1. 5. 17. https://t.co/o28M0v...   \n",
       "7        üáØüáµ @ Sanrio Puroland https://t.co/eXVev5UMBx   \n",
       "8            2017 resolution: to embody authenticity!   \n",
       "9                    sisters. https://t.co/5ZE21x2aNk   \n",
       "10  Happy Holidays! Sending love and light to ever...   \n",
       "\n",
       "                                           clean_text  \n",
       "5   happy  th gma   lacma los angeles county museu...  \n",
       "6                                   kyoto japan        \n",
       "7                                     sanrio puroland  \n",
       "8                   resolution to embody authenticity  \n",
       "9                                             sisters  \n",
       "10  happy holidays sending love and light to every...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].str.translate(\n",
    "    str.maketrans('', '', string.punctuation))\n",
    "df.iloc[5:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b8269",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "Tokenisation is the process of splitting text into smaller units called tokens. In NLP, tokens are typically words, subwords, or sentences. It is a crucial first step in most NLP pipelines for further analysis such as part-of-speech tagging, lemmatisation, and sentiment analysis.\n",
    "\n",
    "Tokenisation is useful to:\n",
    "- **Prepare text for analysis**: Models and algorithms work with tokens, not raw text.\n",
    "- **Enable feature extraction**: Tokens are used to create frequency lists, embeddings, etc.\n",
    "- **Handle language structure**: Helps separate words, punctuation, and sentences for more accurate processing.\n",
    "\n",
    "#### Word Tokenisation\n",
    "Splitting text into individual words or word-like units. Most common for English and languages with clear word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fd27299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tsu76i/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a638770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With .split(): ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n",
      "With regex: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '']\n",
      "With NLTK: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'The quick brown fox jumps over the lazy dog.'\n",
    "\n",
    "# Using Python's `split()` method\n",
    "split_tokens = text.split(' ')\n",
    "print(f'With .split(): {split_tokens}')\n",
    "\n",
    "# Using regex\n",
    "regex_tokens = re.split(r'\\W+', text)  # Split on non-word characters\n",
    "print(f'With regex: {regex_tokens}')\n",
    "\n",
    "# Using NLTK library\n",
    "nltk_tokens = word_tokenize(text)\n",
    "print(f'With NLTK: {nltk_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ad166c",
   "metadata": {},
   "source": [
    "#### Sentence Tokenisation\n",
    "Splitting text into sentences, often based on punctuation and language rules. This method is useful for summarisation, translation, or sentence-level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d4a7060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The quick brown fox jumps over the lazy dog.', 'It was a sunny day.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = 'The quick brown fox jumps over the lazy dog. It was a sunny day.'\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46ced3",
   "metadata": {},
   "source": [
    "#### N-gram Tokenisation\n",
    "Splitting text into overlapping sequences of N words (bi-grams, tri-grams, etc.). Efficient way to implement feature extraction for language models or text classificaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bfc8b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "text = 'The quick brown fox jumps'\n",
    "tokens = word_tokenize(text)\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a931d2f",
   "metadata": {},
   "source": [
    "#### Character Tokenisation\n",
    "Splitting text into individual characters. Useful for languages without clear word boundaries, or for spelling correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e211167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S', 'a', 'm', 'p', 'l', 'e', ' ', 't', 'e', 'x', 't']\n"
     ]
    }
   ],
   "source": [
    "text = 'Sample text'\n",
    "char_tokens = list(text)\n",
    "print(char_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96804f",
   "metadata": {},
   "source": [
    "#### Subword Tokenisation\n",
    "Splitting words into smaller units (subwords), useful for handling rare words and out-of-vocabulary terms. Model NLP models (BERT, GPT, etc.) use subword tokenisers such as Byte-Pair-Encoding(BPE) or SentencePiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "616ca06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chat', '##bots', 'are', 'amazing', '!']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokeniser = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = 'Chatbots are amazing!'\n",
    "tokens = tokeniser.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30da103",
   "metadata": {},
   "source": [
    "### Stop Word Removal\n",
    "Stop words are common words (like 'the', 'and', 'is') that usually carry little meaningful information for NLP tasks. Removing them helps reduce noise and the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5507f199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tsu76i/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33d563b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a, about, above, after, again, against, ain, all, am, an, and, any, are, aren, aren't, as, at, be, because, been, before, being, below, between, both, but, by, can, couldn, couldn't, d, did, didn, didn't, do, does, doesn, doesn't, doing, don, don't, down, during, each, few, for, from, further, had, hadn, hadn't, has, hasn, hasn't, have, haven, haven't, having, he, he'd, he'll, her, here, hers, herself, he's, him, himself, his, how, i, i'd, if, i'll, i'm, in, into, is, isn, isn't, it, it'd, it'll, it's, its, itself, i've, just, ll, m, ma, me, mightn, mightn't, more, most, mustn, mustn't, my, myself, needn, needn't, no, nor, not, now, o, of, off, on, once, only, or, other, our, ours, ourselves, out, over, own, re, s, same, shan, shan't, she, she'd, she'll, she's, should, shouldn, shouldn't, should've, so, some, such, t, than, that, that'll, the, their, theirs, them, themselves, then, there, these, they, they'd, they'll, they're, they've, this, those, through, to, too, under, until, up, ve, very, was, wasn, wasn't, we, we'd, we'll, we're, were, weren, weren't, we've, what, when, where, which, while, who, whom, why, will, with, won, won't, wouldn, wouldn't, y, you, you'd, you'll, your, you're, yours, yourself, yourselves, you've\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all the stop words in English\n",
    "', '.join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ca56dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'demonstrating', 'stop', 'word', 'removal.']\n"
     ]
    }
   ],
   "source": [
    "text = 'This is an example sentence demonstrating stop word removal.'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in text.split() if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967cfb27",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming reduces words to their root form by chopping off suffixes. For example, 'running', 'runs' are reduced to 'run', but 'ran' remains 'ran' as stemming is rule-cased and often does not handle irregular verbs properly. Moreover, stemming may not always produce a real word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "104ff8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming: ['running', 'runner', 'runs', 'ran', 'easily', 'fairly', 'accordingly']\n",
      "After stemming: ['run', 'runner', 'run', 'ran', 'easili', 'fairli', 'accordingli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = ['running', 'runner', 'runs', 'ran', 'easily', 'fairly', 'accordingly']\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(f'Before stemming: {words}')\n",
    "print(f'After stemming: {stemmed_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b16bdc",
   "metadata": {},
   "source": [
    "### Lemmatisation\n",
    "Lemmatisation reduces words to their base or dictionary form (lemma), considering context and part of speech. Unlike stemming, lemmatisation always produces a real word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f881186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/tsu76i/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef3fc619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatisation: ['running', 'runner', 'runs', 'ran', 'easily', 'fairly', 'accordingly']\n",
      "After lemmatisation: ['run', 'runner', 'run', 'run', 'easily', 'fairly', 'accordingly']\n"
     ]
    }
   ],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "words = ['running', 'runner', 'runs', 'ran', 'easily', 'fairly', 'accordingly']\n",
    "lemmatised_words = [lemmatiser.lemmatize(word, pos='v') for word in words] # v = verbs\n",
    "print(f'Before lemmatisation: {words}')\n",
    "print(f'After lemmatisation: {lemmatised_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1d62e",
   "metadata": {},
   "source": [
    "### Part-of-Speech (POS) Tagging\n",
    "Part-of-Speech (POS) Tagging, also known as grammatical tagging, is the process of assigning each word in a text a label that indicates its grammatical role, such as noun, verb, adjective, adverb, etc. This process provides essential information about sentence structure and the relationships between words.\n",
    "\n",
    "Importance of POS:\n",
    "- **Foundation of NLP Tasks**: POS tags are used in higher-level NLP applications such as named entity recognition, sentimental analysis, and machine translation.\n",
    "- **Disambiguation**: Many words can have multiple grammatical roles. For instance, 'book' can be a noun ('*hand me that book*') or a verb ('*book a flight*'). POS tagging helps resolve such ambiguities by considering context.\n",
    "- **Improving Text Understanding**: Knowing the part of speech helps models understand sentence structure and meaning, which is crucial for accurate language processing.\n",
    "\n",
    "Techniques:\n",
    "- **Rule-Based**: Early systems relied on hand-crafted rules.\n",
    "- **Stochastic/Statistical**: Modern systems often use probabilistic models like Hidden Markov Models (HMMs) or neural networks, which learn from large annotated corpora.\n",
    "- **Hybrid**: Some systems combine rules and statistical methods.\n",
    "\n",
    "\n",
    "| POS Tag | Meaning                                   |\n",
    "|---------|-------------------------------------------|\n",
    "| CC      | Coordinating conjunction                  |\n",
    "| CD      | Cardinal number                           |\n",
    "| DT      | Determiner                                |\n",
    "| EX      | Existential there                         |\n",
    "| FW      | Foreign word                              |\n",
    "| IN      | Preposition or subordinating conjunction  |\n",
    "| JJ      | Adjective                                 |\n",
    "| JJR     | Adjective, comparative                    |\n",
    "| JJS     | Adjective, superlative                    |\n",
    "| LS      | List item marker                          |\n",
    "| MD      | Modal                                     |\n",
    "| NN      | Noun, singular or mass                    |\n",
    "| NNS     | Noun, plural                              |\n",
    "| NNP     | Proper noun, singular                     |\n",
    "| NNPS    | Proper noun, plural                       |\n",
    "| PDT     | Predeterminer                             |\n",
    "| POS     | Possessive ending                         |\n",
    "| PRP     | Personal pronoun                          |\n",
    "| PRP$    | Possessive pronoun                        |\n",
    "| RB      | Adverb                                    |\n",
    "| RBR     | Adverb, comparative                       |\n",
    "| RBS     | Adverb, superlative                       |\n",
    "| RP      | Particle                                  |\n",
    "| SYM     | Symbol                                    |\n",
    "| TO      | to                                        |\n",
    "| UH      | Interjection                              |\n",
    "| VB      | Verb, base form                           |\n",
    "| VBD     | Verb, past tense                          |\n",
    "| VBG     | Verb, gerund or present participle        |\n",
    "| VBN     | Verb, past participle                     |\n",
    "| VBP     | Verb, non-3rd person singular present     |\n",
    "| VBZ     | Verb, 3rd person singular present         |\n",
    "| WDT     | Wh-determiner                             |\n",
    "| WP      | Wh-pronoun                                |\n",
    "| WP$     | Possessive wh-pronoun                     |\n",
    "| WRB     | Wh-adverb                                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e69b55b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/tsu76i/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc006136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = 'The quick brown fox jumps over the lazy dog.'\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(f'Tokens: {tokens}')\n",
    "print(f'POS Tags: {pos_tags}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b94914",
   "metadata": {},
   "source": [
    "|  Word |  POS Tag  |           Meaning         |\n",
    "|-------|-----------|---------------------------|\n",
    "| The   |   DT\t    |  Determiner               |\n",
    "| quick |\tJJ\t    |  Adjective                |\n",
    "| brown |\tJJ\t    |  Adjective                |\n",
    "| fox   |   NN\t    |  Noun, singular           |\n",
    "| jumps |\tVBZ\t    |  Verb, 3rd person singular|\n",
    "| over  |\tIN\t    |  Preposition              |\n",
    "| the   |\tDT\t    |  Determiner               |\n",
    "| lazy  |\tJJ\t    |  Adjective                |\n",
    "| dog   |\tNN\t    |  Noun, singular           |\n",
    "| .     |\t.\t    |  Punctuation              |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
