{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdaec77d",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Frequency-Based Word Embeddings](#2-frequency-based-word-embeddings)\n",
    "    - [One-Hot Encoding](#one-hot-encoding)\n",
    "    - [Bag of Words (BoW)](#bag-of-words-bow)\n",
    "    - [Term Frequency-Inverse Document Frequency (TF-IDF)](#term-frequency-inverse-document-frequency-tf-idf)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d592bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c4468",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Vectorisation in Natural Language Processing (NLP) is the process of converting text into numerical representations (vectors) that machine learning models can process and analyse natural language data. \n",
    "\n",
    "Word embeddings are a specific type of vectorisation technique that represents words as dense vectors in a continuous, high-dimensional space. Unlike basic vectorisation methods, word embeddings are designed to reflext semantic and syntactic relationships between words. Hence, words with similar meanings or usage contexts are mapped to vectors that are close together in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8496f9f6",
   "metadata": {},
   "source": [
    "## 2. Frequency-Based Word Embeddings\n",
    "Frequency-based (or count-based) word embeddings represent words using statistics about their occurrences and co-occurrences in a cprpus. These methods do not use neural networks or prediction tasks. Instead, they rely on explicit counts and matrix manipulations to derive word vectors. Though they are typically simple, interpretable and easy to parallelise, they are unable to capture semantic similarity well.\n",
    "\n",
    "### One-Hot Encoding\n",
    "Each word in the vocabulary is represented by a sparse binary vector with a single $1$ at the position corresponding to the word and $0$ elsewhere. It does not capture semantic similarity between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27171d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "corpus = ['apple', 'banana', 'cherry', 'blueberry', 'apple']  # Sample\n",
    "\n",
    "corpus_reshaped = np.array(corpus).reshape(-1, 1)  # Reshape\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)  # Initialisation\n",
    "\n",
    "one_hot_encoded_corpus = ohe.fit_transform(corpus_reshaped)  # One Hot Encode\n",
    "\n",
    "print(one_hot_encoded_corpus)  # Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6154b0",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)\n",
    "Bag of Words is a simple technique that represents a document by the frequency of each word in the vocabulary, ignoring grammar and word order. Each document is a vector of word counts. We can implement BoW using `CountVectorizer` method from scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e9917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "] # Sample\n",
    "\n",
    "vectoriser = CountVectorizer() # Initialisation\n",
    "X = vectoriser.fit_transform(corpus) # Apply CountVectorizer()\n",
    "\n",
    "print(vectoriser.get_feature_names_out()) # Output (features)\n",
    "print(X.toarray()) # Output (BoW matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55adddf",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) extends BoW by weighting each word based on their **frequency** in a document (TF) and their **rarity** across all documents (IDF). This helps highlight important words while reducing the influence of common ones such as 'the' or 'and'. The scikit-learn library provides `TfidfVectorizer` method to implement TF-IDF.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "$t$: Speficic term (word) being evaluated\n",
    "$d$: Single document within the corpus\n",
    "$D$: Set of all documents\n",
    "\n",
    "\n",
    "- **Term Frequency (TF)**: Measures how often term $t$ appears in document $d$, normalised by the document's length.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{TF}(t, d) = \\dfrac{\\text{Number of times } t \\text{ appears in } d}{\\text{Total terms in } d}\n",
    "\\end{align*}\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**: Penalises terms common across many documents.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IDF}(t, D) = \\text{log} \\left(\\dfrac{\\text{Total documents in corpus} (N)}{\\text{Documents containing } t} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "or, **smoothed IDF** (used in scikit-learn to avoid division by zero):\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IDF}(t, D) = \\text{log} \\left(\\dfrac{N + 1}{\\text{Documents containing } t + 1} \\right) + 1\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "*Example*: If the word 'vector' appears 8 times in a 200-word document, within 50 among 10000 documents:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{TF}(8, 200) = \\dfrac{8}{200} = 0.04\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IDF}(50, 10000) = \\text{log} \\left(\\dfrac{10000}{50}\\right) = \\text{log}(200) \\approx{2.30}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{TF-IDF} = \\text{TF} \\times \\text{IDF} = 0.04 \\times 2.30 = 0.092\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bdad7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "] # Sample\n",
    "\n",
    "vectoriser = TfidfVectorizer() # Initialisation\n",
    "X = vectoriser.fit_transform(corpus) # Apply TfidfVectorizer()\n",
    "\n",
    "print(vectoriser.get_feature_names_out()) # Output (features)\n",
    "print(X.toarray()) # Output (TF-IDF matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
