{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3769f437",
   "metadata": {},
   "source": [
    "# Respiratory Disease Classification\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.typing import NDArray\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchinfo import summary\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader, Subset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility.\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9e02d",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "## 2. Device Agnostic Code\n",
    "Mac GPU acceleration (`mps` backend) delivers significant speed-up over CPU for deep learning tasks, especially for large models and batch sizes. On Windows, `cuda` is used instead of `mps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b4f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\n",
    "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# )  # For Windows\n",
    "DEVICE = torch.device(\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")  # For MacOS\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a6332",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n",
    "Retrieved from [COVID-19 Radiography Database](https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"_datasets/Radiography_Dataset\")\n",
    "\n",
    "if DATA_PATH.is_dir():\n",
    "    print(f\"{DATA_PATH} directory exists.\")\n",
    "else:\n",
    "    print(f\"{DATA_PATH} directory NOT FOUND!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_dir(dir_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Print the number of directories and image files in each subdirectory of a given directory.\n",
    "\n",
    "    Args:\n",
    "        dir_path: Path to the root directory to walk through.\n",
    "    \"\"\"\n",
    "\n",
    "    for (\n",
    "        directory_path,\n",
    "        directory_names,\n",
    "        file_names,\n",
    "    ) in os.walk(top=dir_path):\n",
    "        print(\n",
    "            f\"{len(directory_names)} directories and {len(file_names)} images found in {directory_path}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_through_dir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f88685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for image classification from a folder structure.\n",
    "\n",
    "    Attributes:\n",
    "        img_folder: Glob pattern for image files.\n",
    "        transform: Transformations to apply to images.\n",
    "        extensions: Allowed image file extensions.\n",
    "        all_paths: List of all image file paths.\n",
    "        all_labels: List of all image class labels.\n",
    "        categories: Sorted set of unique class labels.\n",
    "        label2id: Mapping from class label to integer index.\n",
    "        id2label: Mapping from integer index to class label.\n",
    "        encoded_labels: List of class label indices for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    img_folder: str\n",
    "    transform: transforms.Compose | None\n",
    "    extensions: set[str]\n",
    "    all_paths: list[Path]\n",
    "    all_labels: list[str]\n",
    "    categories = set[str]\n",
    "    label2id = dict[str, int]\n",
    "    id2label = dict[int, str]\n",
    "    all_labels_indices = list[int]\n",
    "\n",
    "    def __init__(\n",
    "        self, img_path, transform=None, extensions={\".png\", \".jpg\", \".jpeg\"}\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the dataset and maps image files to class indices.\n",
    "\n",
    "        Args:\n",
    "            img_path: Root directory containing image folders.\n",
    "            transform: Transformations to apply on image load; optional.\n",
    "            extensions: Allowed image file extensions.\n",
    "        \"\"\"\n",
    "        img_folder = \"*/images/*\"\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        self.all_paths = [\n",
    "            path\n",
    "            for path in img_path.glob(img_folder)\n",
    "            if path.suffix.lower() in extensions\n",
    "        ]\n",
    "        # Extract labels from 2 levels above the image file\n",
    "        self.all_labels = [path.parent.parent.name for path in self.all_paths]\n",
    "        self.categories = sorted(set(self.all_labels))  # Unique labels\n",
    "        self.label2id = {label: index for index, label in enumerate(self.categories)}\n",
    "        self.id2label = {index: label for label, index in self.label2id.items()}\n",
    "        self.encoded_labels = [self.label2id[label] for label in self.all_labels]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            Number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.all_paths)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        \"\"\"\n",
    "        Retrieve an image and its label index by dataset position.\n",
    "\n",
    "        Args:\n",
    "            index: Index of the image sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple (img, label_index) where img is a tensor (if transformed) and label_index is the integer class index.\n",
    "        \"\"\"\n",
    "        single_file_path = self.all_paths[index]\n",
    "        try:\n",
    "            img = Image.open(fp=single_file_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error opening image {single_file_path}: {e}\")\n",
    "        label_index = self.encoded_labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(img_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3430019",
   "metadata": {},
   "source": [
    "## 4. Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_images(dataset: Dataset) -> None:\n",
    "    \"\"\"\n",
    "    Display a grid of images from a dataset with their corresponding class names as titles.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset (image, label). Images assumed to be tensors.\n",
    "        class_names: List of class names indexed by label.\n",
    "        num_images: Number of images to display. Defaults to 9.\n",
    "\n",
    "    Prints:\n",
    "        A matplotlib plot grid of images with titles.\n",
    "    \"\"\"\n",
    "    cols, rows = 3, 3\n",
    "    figure = plt.figure(figsize=(rows * 3, cols * 3))\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_index = random.randint(a=0, b=len(dataset))\n",
    "        img, label = dataset[sample_index]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(dataset.id2label[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b329f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals, counts = np.unique(dataset.all_labels, return_counts=True)\n",
    "df_dist = pd.DataFrame({\"Class Label\": unique_vals, \"Count\": counts})\n",
    "print(df_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(dataset: Dataset) -> None:\n",
    "    \"\"\"\n",
    "    Plot the distribution of class labels in a dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: A CustomDataset or similar Dataset object with an `all_labels` attribute\n",
    "                 containing the class label for each image.\n",
    "    \"\"\"\n",
    "    unique_vals, counts = np.unique(dataset.all_labels, return_counts=True)\n",
    "    df_dist = pd.DataFrame({\"Class Label\": unique_vals, \"Count\": counts})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        data=df_dist, x=\"Class Label\", y=\"Count\", hue=\"Class Label\", palette=\"Set2\"\n",
    "    )\n",
    "    plt.xlabel(\"Class Label\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Class Labels\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_distribution(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da906baf",
   "metadata": {},
   "source": [
    "## 5. Preparing Data\n",
    "### Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad3a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset: Dataset, random_seed: int) -> tuple[Subset, Subset, Subset]:\n",
    "    \"\"\"\n",
    "    Split a dataset into training, validation, and test subsets using stratified sampling.\n",
    "\n",
    "    Args:\n",
    "        dataset: The full dataset to split, with an `encoded_labels` property.\n",
    "        random_seed: Random seed for reproducibility of splits.\n",
    "\n",
    "    Returns:\n",
    "        The train, validation, and test dataset subsets.\n",
    "    \"\"\"\n",
    "    all_indices = range(len(dataset))\n",
    "    all_labels = dataset.encoded_labels\n",
    "\n",
    "    train_indices, sub_indices = train_test_split(\n",
    "        all_indices, test_size=0.2, stratify=all_labels, random_state=random_seed\n",
    "    )  # 80% train, 20% sub\n",
    "\n",
    "    val_indices, test_indices = train_test_split(\n",
    "        sub_indices,\n",
    "        test_size=0.5,\n",
    "        stratify=[all_labels[i] for i in sub_indices],\n",
    "        random_state=random_seed,\n",
    "    )  # sub -> 50% validation, 50% test\n",
    "\n",
    "    train_data = Subset(dataset=dataset, indices=train_indices)\n",
    "    val_data = Subset(dataset=dataset, indices=val_indices)\n",
    "    test_data = Subset(dataset=dataset, indices=test_indices)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d1d57a",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "For transfer learning using pretrained models in PyTorch, it is a common and effective practice to normalise the dataset using the standard mean and standard deviation values of the ImageNet dataset, on which many pretrained models were originally trained. This ensures that the input data distribution matches the distribution expected by the pretrained model, leading to better convergence and improved performance during fine-tuning.\n",
    "\n",
    "[Reference - PyTorch Forums](https://discuss.pytorch.org/t/discussion-why-normalise-according-to-imagenet-mean-and-std-dev-for-transfer-learning/115670)\n",
    "\n",
    "In addition to normalisation, various data augmentation techniques (such as random flips and random rotations) are applied to increase data diversity and improve the model's generalisation capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "RANDOM_SEED = 42\n",
    "IMAGE_NET_MEANS = [0.485, 0.456, 0.406]\n",
    "IMAGE_NET_STDS = [0.229, 0.224, 0.225]\n",
    "CLASS_NAMES = list(dataset.label2id.keys())\n",
    "N_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    transforms=[\n",
    "        transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGE_NET_MEANS, std=IMAGE_NET_STDS),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    transforms=[\n",
    "        transforms.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGE_NET_MEANS, std=IMAGE_NET_STDS),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ca684",
   "metadata": {},
   "source": [
    "### Preparing DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset, RANDOM_SEED)\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = test_transform\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"train_dataset: {len(train_dataset)} -> train_loader: {len(train_loader)}\")\n",
    "print(f\"val_dataset: {len(val_dataset)} -> val_loader: {len(val_loader)}\")\n",
    "print(f\"test_dataset: {len(test_dataset)} -> test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf6c6a",
   "metadata": {},
   "source": [
    "## 6. Transfer Learning\n",
    "Transfer learning is a powerful technique in deep learning where a model pretrained on a large, general dataset is adapted for a related task. This practice improves performance while reducing the amount of training data and training time required.\n",
    "\n",
    "Setting `param.requires_grad = False` in PyTorch freezes the model parameters (weights and biases), preventing gradient computations and updates during training. This allows parts of the model to remain unchanged while selectively training other layers.\n",
    "\n",
    "### ResNet-50\n",
    "ResNet-50 is a convolutional neural network renowned for its use of residual connections. These residual connections work as shortcut paths that enable the training of very deep networks by mitigating the vanishing gradient problem. The \"50\" layers refer to the number of weighted layers, constructed from a combination of convolutional layers and identity shortcuts.\n",
    "\n",
    "The architecture is organised into four main sequential layers or blocks (layer1 to layer4), each consisting of multiple bottleneck residual blocks—typically more than two residual blocks per layer (for example, layer1 has 3 blocks, layer2 has 4, layer3 has 6, and layer4 has 3 blocks).\n",
    "\n",
    "In our setup, freezing all layers except the final block (layer4) and the fully connected (fc) layer enables the model to retain general low- and mid-level features learned during pretraining while fine-tuning the deeper, more task-specific features. This strikes a balance between preserving learned knowledge and allowing model adaptation to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-50 model adapted for classification with optional layer freezing.\n",
    "\n",
    "    Attributes:\n",
    "        device: The device to which the model is moved.\n",
    "        model: The ResNet-50 backbone model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        is_frozen: bool | None = True,\n",
    "        device: torch.device | str = \"cpu\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialise ResNet-50 model with transfer learning layers.\n",
    "\n",
    "        Args:\n",
    "            num_classes: Number of output classes.\n",
    "            freeze: Whether to freeze early layers. Defaults to True.\n",
    "            device: Device for the model (e.g., 'cpu', 'mps' or 'cuda'). Defaults to 'cpu'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=self.model.fc.in_features, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "        if is_frozen:\n",
    "            # Freeze all layers initially\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.model.layer4.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the ResNet-50 model.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            Output logits tensor of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3662480",
   "metadata": {},
   "source": [
    "### DenseNet-121\n",
    "DenseNet-121 is a convolutional neural network architecture characterised by dense connectivity, where each layer receives inputs from all preceding layers and passes its own feature maps to all subsequent layers. This design facilitates feature reuse, alleviates the vanishing gradient problem, and improves parameter efficiency compared to traditional convolutional networks. The network is made up of four dense blocks, each containing several densely connected convolutional layers, separated by transition layers that reduce the dimensions via convolution and pooling.\n",
    "\n",
    "In our setup, we freeze most of the model's parameters to retain the pretrained feature representations and only unfreeze the last dense block and the classification head. This allows the model to adapt higher-level features more specifically to the new image data while preserving general, pretrained low-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99437ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"\n",
    "    DenseNet-121 model adapted for classification with optional layer freezing.\n",
    "\n",
    "    Attributes:\n",
    "        device: The device to which the model is moved.\n",
    "        model: The DenseNet-121 backbone model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        is_frozen: bool | None = True,\n",
    "        device: torch.device | None = \"cpu\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialise DenseNet-121 model with transfer learning layers.\n",
    "\n",
    "        Args:\n",
    "            num_classes: Number of output classes.\n",
    "            freeze: Whether to freeze early layers. Defaults to True.\n",
    "            device: Device for the model (e.g., 'cpu', 'mps' or 'cuda'). Defaults to 'cpu'.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=n_features, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "        if is_frozen:\n",
    "            for param in self.model.parameters():  # all layers\n",
    "                param.requires_grad = False\n",
    "            # Unfreeze only the last dense block, final batch norm and classifier by default\n",
    "            for param in self.model.features[10].parameters():  # denseblock4\n",
    "                param.requires_grad = True\n",
    "            for param in self.model.features[11].parameters():  # norm5\n",
    "                param.requires_grad = True\n",
    "            for param in self.model.classifier.parameters():  # classifier\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the DenseNet-121 model.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            Output logits tensor of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a0d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = ResNet50(num_classes=N_CLASSES, device=DEVICE)\n",
    "densenet121 = DenseNet121(num_classes=N_CLASSES, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23502123",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [resnet50, densenet121]:\n",
    "    print(\n",
    "        summary(\n",
    "            model=model,\n",
    "            input_size=(\n",
    "                BATCH_SIZE,\n",
    "                3,\n",
    "                IMG_SIZE,\n",
    "                IMG_SIZE,\n",
    "            ),\n",
    "            verbose=0,\n",
    "            col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "            col_width=20,\n",
    "            row_settings=[\"var_names\"],\n",
    "        ),\n",
    "    )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9133ab",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "We will use the following evaluation metrics:\n",
    "- `torchmetrics.Accuracy`\n",
    "- `torchmetrics.F1Score`\n",
    "    - Macro F1 score is recommended for imbalanced datasets to evaluate performance equally across all classes. Each class's F1 contributes equally regardless of sample size.\n",
    "\n",
    "    - Micro F1 score aggregates all true positives, false positives, and false negatives globally, thus it favours the majority classes and is close to overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07711801",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = Accuracy(task=\"multiclass\", num_classes=N_CLASSES).to(device=DEVICE)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=N_CLASSES, average=\"macro\").to(\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "metrics = [accuracy, f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7faa7ad",
   "metadata": {},
   "source": [
    "## 8. Loss Function\n",
    "### Cross-Entropy Loss\n",
    "Cross-Entropy Loss is a loss function used for classification problems, particularly when the model outputs probabilities using a softmax activation in the final layer. It measures the difference between the true labels and the predicted probability distribution.\n",
    "\n",
    "For a single data point, the cross-entropy loss is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "    L = - \\sum^{k}_{i=1}y_{i}\\log{(\\hat y_{i})}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True label for the $i$-th class. If one-hot encoded, $y_{i} = 1$ for the corrected class, $y_{i} = 0$ otherwise.\n",
    "- $\\hat y_i$: Predicted probability for the $i$-th class.\n",
    "- $k$: Number of classes.\n",
    "\n",
    "For a batch of $m$ data point:\n",
    "\n",
    "\\begin{align*}\n",
    "    C = \\dfrac{1}{m} \\sum^{m}_{j=1} \\left (- \\sum^{k}_{i=1}y_{j, i}\\log{(\\hat y_{j, i})} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $C$: Average cross-entropy loss over the batch.\n",
    "- $m$: Number of training examples (batch size).\n",
    "- $k$: Number of classes.\n",
    "- $y_{j, i} \\in { 0, 1}$: Indicator that true class for sample $j$ corresponds to class $i$.\n",
    "- $\\hat y_{j, i} \\in { 0, 1}$: Predicted probability for sample $j$ belonging to class $i$.\n",
    "\n",
    "In PyTorch:\n",
    "- Use `nn.CrossEntropyLoss()` directly with raw logits.\n",
    "- Do not apply `Softmax()` or `LogSoftmax()` manually before the loss.\n",
    "- Internally, `nn.CrossEntropyLoss() = LogSoftmax() + NegativeLogLikelihoodLoss()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d091a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversed frequency weights\n",
    "class_weights = 1.0 / torch.tensor(counts, dtype=torch.float)\n",
    "class_weights = (class_weights / class_weights.sum()).to(DEVICE)  # Normalised\n",
    "for val, weight in zip(unique_vals, class_weights):\n",
    "    print(f\"Weight for {val}: {weight:.5f}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044a58d",
   "metadata": {},
   "source": [
    "## 9. Optimiser\n",
    "An optimiser in neural networks is used to adjust the parameters (weights and biases) of a model during training to minimise the loss. Optimisers are essential for enabling neural networks to learn from data: without them, the model would not improve over time.\n",
    "\n",
    "**AdamW** (a variant of the Adam optimiser) separates weight decay (L2 regularisation) from the gradient updates. This decoupling often improves a model's generalisation performance compared to the original Adam optimiser, reducing the risk of overfitting, especially in large-scale models.\n",
    "\n",
    "**ReduceLROnPlateau** is a learning rate scheduler that monitors a specified metric (usually validation loss) and reduces the learning rate by a given factor if the metric stops improving for a certain number of epochs (`patience`). This allows the optimiser to take smaller, more precise steps when progress plateaus, often leading to better final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a21980",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "DECAY_RATE = 1e-4\n",
    "\n",
    "# ! === ResNet-50 ===\n",
    "resnet50_optimiser = optim.AdamW(\n",
    "    params=filter(lambda p: p.requires_grad, resnet50.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=DECAY_RATE,\n",
    ")\n",
    "resnet50_scheduler = ReduceLROnPlateau(\n",
    "    optimizer=resnet50_optimiser,\n",
    "    mode=\"min\",\n",
    "    patience=3,\n",
    "    factor=0.5,\n",
    ")\n",
    "\n",
    "# ! === DenseNet-121 ===\n",
    "densenet121_optimiser = optim.AdamW(\n",
    "    params=filter(lambda p: p.requires_grad, densenet121.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=DECAY_RATE,\n",
    ")\n",
    "densenet121_scheduler = ReduceLROnPlateau(\n",
    "    optimizer=densenet121_optimiser,\n",
    "    mode=\"min\",\n",
    "    patience=3,\n",
    "    factor=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a43ec",
   "metadata": {},
   "source": [
    "## 10. Training and Evaluation\n",
    "1. Iterate through epochs\n",
    "1. For each epoch, iterate through training batches, perform training steps, calculate the train loss and evaluation metrics per batch.\n",
    "1. For each epoch, iterate through validation batches, perform validation steps, calculate the validation loss and evaluation metrics per batch.\n",
    "\n",
    "\n",
    "### Training Steps\n",
    "1. Zero the gradients\n",
    "    - Clear the gradients from the previous iteration to prevent accumulation across batches.\n",
    "1. Forward pass\n",
    "    - Pass inputs through the model to obtain predictions.\n",
    "1. Calculate loss and evaluation metrics per batch\n",
    "    - Measure how far the predictions deviate from the true labels using a loss function.\n",
    "    - Compute evaluation metrics (e.g., accuracy, F1 Score) for the current batch.\n",
    "1. Backward pass\n",
    "    - Compute gradients of the loss with respect to the model's parameters via backpropagation.\n",
    "    - Update the parameter $\\theta$ using the computed gradients, typically following:\n",
    "    \n",
    "    $$\n",
    "        \\theta \\leftarrow \\theta - \\eta \\dfrac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "    $$\n",
    "    where $\\eta$ is the learning rate.\n",
    "1. Average training loss and evaluation metrics\n",
    "    - Calculate the mean loss and metric values across all batches in the epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61517717",
   "metadata": {},
   "source": [
    "## 10. Training and Evaluation\n",
    "1. Iterate through epochs\n",
    "1. For each epoch, iterate through training batches, perform training steps, calculate the train loss and evaluation metrics per batch.\n",
    "1. For each epoch, iterate through validation batches, perform validation steps, calculate the validation loss and evaluation metrics per batch.\n",
    "\n",
    "\n",
    "### Training Steps\n",
    "1. Zero the gradients\n",
    "    - Clear the gradients from the previous iteration to prevent accumulation across batches.\n",
    "1. Forward pass\n",
    "    - Pass inputs through the model to obtain predictions.\n",
    "1. Calculate loss and evaluation metrics per batch\n",
    "    - Measure how far the predictions deviate from the true labels using a loss function.\n",
    "    - Compute evaluation metrics (e.g., accuracy, F1 Score) for the current batch.\n",
    "1. Backward pass\n",
    "    - Compute gradients of the loss with respect to the model's parameters via backpropagation.\n",
    "    - Update the parameter $\\theta$ using the computed gradients, typically following:\n",
    "    \n",
    "    $$\n",
    "        \\theta \\leftarrow \\theta - \\eta \\dfrac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "    $$\n",
    "    where $\\eta$ is the learning rate.\n",
    "1. Average training loss and evaluation metrics\n",
    "    - Calculate the mean loss and metric values across all batches in the epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimiser: optim.Optimizer,\n",
    "    metrics: list[nn.Module],\n",
    "    device: torch.device,\n",
    ") -> tuple[float, list[float]]:\n",
    "    model.train()\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "    n_total_samples = len(data_loader.dataset)\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        # Optimiser zero grad without intervening forward pass\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_logits = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(y_logits, labels)\n",
    "        train_loss += loss.item() * batch_size\n",
    "\n",
    "        # Calculate metrics\n",
    "        y_probs = torch.softmax(input=y_logits, dim=1)\n",
    "        y_preds = torch.argmax(input=y_probs, dim=1)\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric.update(y_preds, labels)\n",
    "\n",
    "        # Loss backward for backpropagation (computing gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimiser step to apply gradients and update parameters\n",
    "        optimiser.step()\n",
    "\n",
    "    avg_train_loss = train_loss / n_total_samples\n",
    "    train_metric_scores = [metric.compute().item() * 100 for metric in metrics]\n",
    "    return avg_train_loss, train_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03946ef0",
   "metadata": {},
   "source": [
    "### Validation Steps\n",
    "1. Forward pass\n",
    "    - Set the model to evaluation mode (which disables dropout and batch normalisation and desactivates gradient tracking for safety).\n",
    "    - Pass inputs through the model to obtain predictions.\n",
    "1. Calculate loss and evaluation metrics per batch\n",
    "    - Measure how far the predictions deviate from the true labels using a loss function.\n",
    "    - Compute evaluation metrics (e.g., accuracy, F1-Score) for the current batch.\n",
    "1. Average test loss and evaluation metrics\n",
    "    - Calculate the mean loss and metric values across all batches in the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    metrics: list[nn.Module],\n",
    "    device: torch.device,\n",
    ") -> tuple[float, list[float]]:\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    n_total_samples = len(data_loader.dataset)\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            y_logits = model(inputs)\n",
    "\n",
    "            # 2. Calculate loss\n",
    "            loss = criterion(y_logits, labels)\n",
    "            val_loss += loss.item() * batch_size\n",
    "\n",
    "            # 3. Calculate metrics\n",
    "            y_probs = torch.softmax(input=y_logits, dim=1)\n",
    "            y_preds = torch.argmax(input=y_probs, dim=1)\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric.update(y_preds, labels)\n",
    "\n",
    "    avg_val_loss = val_loss / n_total_samples\n",
    "    val_metric_scores = [metric.compute().item() * 100 for metric in metrics]\n",
    "    return avg_val_loss, val_metric_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434916e",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimiser: optim.Optimizer,\n",
    "    scheduler: optim.lr_scheduler,\n",
    "    metrics: list[nn.Module],\n",
    "    device: torch.device,\n",
    "    total_epochs: int,\n",
    ") -> dict[str, list[float]]:\n",
    "    model.to(device=device)\n",
    "    epochs_range = range(1, total_epochs + 1)\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "    model_name = model.__class__.__name__.lower()\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": [],\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in epochs_range:\n",
    "        train_loss, train_metrics = train_step(\n",
    "            model=model,\n",
    "            data_loader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimiser=optimiser,\n",
    "            metrics=metrics,\n",
    "            device=device,\n",
    "        )\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_metrics[0])\n",
    "        history[\"train_f1\"].append(train_metrics[1])\n",
    "\n",
    "        val_loss, val_metrics = validation_step(\n",
    "            model=model,\n",
    "            data_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            metrics=metrics,\n",
    "            device=device,\n",
    "        )\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_metrics[0])\n",
    "        history[\"val_f1\"].append(val_metrics[1])\n",
    "\n",
    "        scheduler.step(val_loss)  # Update learning rate based on validation loss\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(obj=model.state_dict(), f=f\"{model_name}_best.pth\")\n",
    "            patience_counter = 0\n",
    "            best_epoch = epoch\n",
    "        else:  # Early Stopping\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 5:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        print(f\"Epoch [{epoch}/{total_epochs}]\\n{'=' * 60}\")\n",
    "        print(\n",
    "            f\"{'Train Loss:':<12}{train_loss:>6.4f} | {'Train Accuracy:':<15}{train_metrics[0]:>6.2f}% | {'Train F1:':<10}{train_metrics[1]:>6.2f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{'Val Loss:':<12}{val_loss:>6.4f} | {'Val Accuracy:':<15}{val_metrics[0]:>6.2f}% | {'Val F1:':<10}{val_metrics[1]:>6.2f}%\"\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Training and validation completed in {elapsed_time:.2f} seconds.\\n\")\n",
    "    print(f\"The best-performing model was saved at epoch: {best_epoch}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "EPOCH_RANGE = range(1, EPOCHS + 1)\n",
    "MODEL_NAME_RESNET50 = \"ResNet-50\"\n",
    "MODEL_NAME_DENSENET121 = \"DenseNet-121\"\n",
    "\n",
    "print(f\"Training {MODEL_NAME_RESNET50}...\")\n",
    "resnet50_history = train_and_validate(\n",
    "    model=resnet50,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimiser=resnet50_optimiser,\n",
    "    scheduler=resnet50_scheduler,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    total_epochs=EPOCHS,\n",
    ")\n",
    "\n",
    "print(f\"Training {MODEL_NAME_DENSENET121}...\")\n",
    "densenet121_history = train_and_validate(\n",
    "    model=densenet121,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimiser=densenet121_optimiser,\n",
    "    scheduler=densenet121_scheduler,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    total_epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f8559",
   "metadata": {},
   "source": [
    "## 11. Results\n",
    "### Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456490f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_train_metrics = {\n",
    "    \"Loss\": resnet50_history[\"train_loss\"],\n",
    "    \"Accuracy\": resnet50_history[\"train_acc\"],\n",
    "    \"F1 Score\": resnet50_history[\"train_f1\"],\n",
    "}\n",
    "\n",
    "resnet50_val_metrics = {\n",
    "    \"Loss\": resnet50_history[\"val_loss\"],\n",
    "    \"Accuracy\": resnet50_history[\"val_acc\"],\n",
    "    \"F1 Score\": resnet50_history[\"val_f1\"],\n",
    "}\n",
    "\n",
    "densenet121_train_metrics = {\n",
    "    \"Loss\": densenet121_history[\"train_loss\"],\n",
    "    \"Accuracy\": densenet121_history[\"train_acc\"],\n",
    "    \"F1 Score\": densenet121_history[\"train_f1\"],\n",
    "}\n",
    "\n",
    "densenet121_val_metrics = {\n",
    "    \"Loss\": densenet121_history[\"val_loss\"],\n",
    "    \"Accuracy\": densenet121_history[\"val_acc\"],\n",
    "    \"F1 Score\": densenet121_history[\"val_f1\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08825393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(\n",
    "    epochs_range: range,\n",
    "    model_name: str,\n",
    "    train_metrics: dict[str, list[float]],\n",
    "    val_metrics: dict[str, list[float]],\n",
    ") -> None:\n",
    "    metric_names = list(train_metrics.keys())\n",
    "    n_metrics = len(metric_names)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=n_metrics, figsize=(16, 6))\n",
    "    axes = axes.flatten()\n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        ax = axes[i]\n",
    "        ax.plot(\n",
    "            epochs_range,\n",
    "            train_metrics[metric_name],\n",
    "            label=f\"Train {metric_name}\",\n",
    "        )  # Train metric\n",
    "        ax.plot(\n",
    "            epochs_range,\n",
    "            val_metrics[metric_name],\n",
    "            label=f\"Validation {metric_name}\",\n",
    "        )  # Validation metric\n",
    "        ax.set_title(f\"{model_name}: {metric_name} Over Epochs\", fontsize=12)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        if metric_name == \"Loss\":\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "        else:\n",
    "            ax.set_ylabel(f\"{metric_name} (%)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f6ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    epochs_range=EPOCH_RANGE,\n",
    "    model_name=MODEL_NAME_RESNET50,\n",
    "    train_metrics=resnet50_train_metrics,\n",
    "    val_metrics=resnet50_val_metrics,\n",
    ")\n",
    "plot_results(\n",
    "    epochs_range=EPOCH_RANGE,\n",
    "    model_name=MODEL_NAME_RESNET50,\n",
    "    train_metrics=densenet121_train_metrics,\n",
    "    val_metrics=densenet121_val_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db41522",
   "metadata": {},
   "source": [
    "### Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaceab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_predictions(\n",
    "    model: nn.Module,\n",
    "    model_name: str,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    metrics: list[nn.Module],\n",
    "    device: torch.device,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Make predictions over an entire dataset and calculates metrics.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model to use for predictions.\n",
    "        model_name: Model name for printing outputs.\n",
    "        data_loader: DataLoader for dataset to predict on.\n",
    "        criterion: Loss function to compute loss.\n",
    "        metrics: Metric modules to compute during prediction.\n",
    "        device: Device for computation.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing concatenated predicted labels and true labels.\n",
    "    \"\"\"\n",
    "    model_file_name = model.__class__.__name__.lower()\n",
    "    model.load_state_dict(\n",
    "        state_dict=torch.load(f=f\"{model_file_name}_best.pth\", map_location=device)\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_loss = 0.0\n",
    "    n_total_samples = len(data_loader.dataset)\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for inputs, labels in data_loader:\n",
    "            batch_size = inputs.size(0)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            y_logits = model(inputs)\n",
    "            y_probs = torch.softmax(y_logits, dim=1)\n",
    "            y_preds = torch.argmax(y_probs, dim=1)\n",
    "\n",
    "            all_preds.append(y_preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "            # 2. Calculate test loss\n",
    "            test_loss += criterion(y_logits, labels).item() * batch_size\n",
    "\n",
    "            # 3. Calculate test metrics\n",
    "            for metric in metrics:\n",
    "                metric.update(y_preds, labels)\n",
    "\n",
    "    test_loss /= n_total_samples\n",
    "    test_acc = metrics[0].compute().item() * 100\n",
    "    test_f1 = metrics[1].compute().item() * 100\n",
    "    print(f\"{model_name}\\n{'=' * 60}\")\n",
    "    print(\n",
    "        f\"{'Test Loss':<12}{test_loss:>6.4f} | {'Test Accuracy':<15}{test_acc:>6.2f}% | {'Test F1:':<10}{test_f1:>6.2f}%\\n\"\n",
    "    )\n",
    "    all_preds_tensor = torch.cat(all_preds)\n",
    "    all_labels_tensor = torch.cat(all_labels)\n",
    "    return all_preds_tensor, all_labels_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9505e63",
   "metadata": {},
   "source": [
    "Creating a new model instance before loading saved weights ensures the model to start from a clean, well-defined architecture without any unintended changes or leftover states from previous training runs (such as optimiser internal states, hooks, or temporarily stored variables). This practice helps avoid bugs and inconsistencies during evaluation.\n",
    "\n",
    "Subsequently, the `make_all_predictions()` function uses this freshly loaded, optimised model (with trained weights) to generate predictions reliably for the dataset, ensuring that the evaluation reflects the learned parameters from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e64403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = ResNet50(num_classes=N_CLASSES, is_frozen=False, device=DEVICE)\n",
    "\n",
    "all_preds_resnet50, all_labels_resnet50 = make_all_predictions(\n",
    "    model=resnet50,\n",
    "    model_name=MODEL_NAME_RESNET50,\n",
    "    data_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "densenet121 = DenseNet121(num_classes=N_CLASSES, is_frozen=False, device=DEVICE)\n",
    "\n",
    "all_preds_densenet121, all_labels_densenet121 = make_all_predictions(\n",
    "    model=densenet121,\n",
    "    model_name=MODEL_NAME_DENSENET121,\n",
    "    data_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{MODEL_NAME_RESNET50}\\n{'=' * 60}\")\n",
    "print(classification_report(y_true=all_labels_resnet50, y_pred=all_preds_resnet50))\n",
    "print(\n",
    "    classification_report(y_true=all_labels_densenet121, y_pred=all_preds_densenet121)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af023f48",
   "metadata": {},
   "source": [
    "### Missclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missclassifications(\n",
    "    all_preds: torch.Tensor,\n",
    "    all_labels: torch.Tensor,\n",
    "    model_name: str,\n",
    ") -> None:\n",
    "    missclassified = all_preds != all_labels\n",
    "    n_missclassified = int(missclassified.sum())\n",
    "    missclassified_rate = 100 * n_missclassified / len(all_labels)\n",
    "    print(f\"{model_name}\\n{'=' * 60}\")\n",
    "    print(\n",
    "        f\"Number of failed predictions: {n_missclassified}/{len(all_labels)} ({missclassified_rate:.2f}%)\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_missclassifications(\n",
    "    all_preds=all_preds_resnet50,\n",
    "    all_labels=all_labels_resnet50,\n",
    "    model_name=MODEL_NAME_RESNET50,\n",
    ")\n",
    "print_missclassifications(\n",
    "    all_preds=all_preds_densenet121,\n",
    "    all_labels=all_labels_densenet121,\n",
    "    model_name=MODEL_NAME_DENSENET121,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b373bb0",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matirx(\n",
    "    cm: NDArray[np.int64] | torch.Tensor,\n",
    "    class_names: list[str],\n",
    "    model_name: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix using seaborn heatmap.\n",
    "\n",
    "    Args:\n",
    "        cm: Confusion matrix data.\n",
    "        class_names: List of class names for axes.\n",
    "        model_name: Model name for plot title.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        data=cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(f\"{model_name}: Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81291454",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_resnet50 = confusion_matrix(y_true=all_labels_resnet50, y_pred=all_preds_resnet50)\n",
    "\n",
    "cm_densenet121 = confusion_matrix(\n",
    "    y_true=all_labels_densenet121, y_pred=all_preds_densenet121\n",
    ")\n",
    "\n",
    "plot_confusion_matirx(\n",
    "    cm=cm_resnet50,\n",
    "    class_names=CLASS_NAMES,\n",
    "    model_name=MODEL_NAME_RESNET50,\n",
    ")\n",
    "\n",
    "plot_confusion_matirx(\n",
    "    cm=cm_densenet121,\n",
    "    class_names=CLASS_NAMES,\n",
    "    model_name=MODEL_NAME_DENSENET121,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65d321",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e999d3a",
   "metadata": {},
   "source": [
    "## 12. Vision Transformer (ViT)\n",
    "The Vision Transformer (ViT) is a model architecture adapted from transformer models originally developed for natural language processing (NLP), specifically tailored for computer vision tasks. ViT treats an image as a sequence of fixed-size patches, analogous to tokens in text processing. These image patches are linearly embedded and then processed through transformer encoder layers, which utilise self-attention mechanisms to capture relationships between different parts of the image.\n",
    "\n",
    "The advantages of transformers, including global context understanding, parallelisability, and high performance with large datasets, have made the Vision Transformer particularly successful not only in text processing but also in computer vision.\n",
    "### Converting Image Data for ViT\n",
    "The Hugging Face `Trainer` excepts datasets to return dictionaries rather than tuples. For image classification, it needs to be:\n",
    "```\n",
    "{\n",
    "    'pixel_values': tensor_of_image,\n",
    "    'labels: label_integer\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"'pin_memory' argument is set as true but not supported on MPS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a22cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDatasetWrapper(Dataset):\n",
    "    \"\"\"\n",
    "    Wrapper around a dataset to output inputs as dictionary with 'pixel_values' and 'labels',\n",
    "    compatible with Hugging Face Trainer.\n",
    "\n",
    "    Attributes:\n",
    "        dataset: Original dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Initialise with original dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: The dataset to wrap.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of samples.\n",
    "\n",
    "        Returns:\n",
    "            int: Dataset length.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return a sample as a dictionary with keys 'pixel_values' and 'labels'.\n",
    "\n",
    "        Args:\n",
    "            index: Sample index.\n",
    "\n",
    "        Returns:\n",
    "            Sample dictionary.\n",
    "        \"\"\"\n",
    "        image, label = self.dataset[index]\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"labels\": label,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_vit = TransformerDatasetWrapper(dataset=train_dataset)\n",
    "val_dataset_vit = TransformerDatasetWrapper(dataset=val_dataset)\n",
    "test_dataset_vit = TransformerDatasetWrapper(dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_MODEL_PATH = \"./my_finetuned_model\"\n",
    "N_EPOCHS_VIT = 1\n",
    "LABEL_2_ID = dataset.label2id\n",
    "ID_2_LABEL = dataset.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f51cca6",
   "metadata": {},
   "source": [
    "### Loading Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06612b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "VIT_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "VIT_MODEL = ViTForImageClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=VIT_MODEL_NAME,\n",
    "    id2label=ID_2_LABEL,\n",
    "    label2id=LABEL_2_ID,\n",
    ")\n",
    "VIT_PROCESSOR = ViTImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path=VIT_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363ff43",
   "metadata": {},
   "source": [
    "### Training Arguments\n",
    "The following code configures the training arguments for fine-tuning a Transformer model using Hugger Face's `Trainer` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    report_to=\"none\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=N_EPOCHS_VIT,\n",
    "    weight_decay=0.01,\n",
    "    seed=RANDOM_SEED,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d9f93",
   "metadata": {},
   "source": [
    "### Evaluation Metrics for ViT\n",
    "The overall performance of the fine‑tuned model will be evaluated using two metrics: accuracy and the F1‑score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364bde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(\n",
    "    eval_pred: tuple[NDArray[np.float32], NDArray[np.int64]],\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy and weighted F1 score metrics suitable for Hugging Face Trainer evaluation.\n",
    "\n",
    "    Args:\n",
    "        eval_pred: Tuple with model logits and true labels.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with \"accuracy\" and \"f1\" metric scores.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"weighted\"\n",
    "    )\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befb947",
   "metadata": {},
   "source": [
    "### Fine-Tuning Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b753145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=VIT_MODEL,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_vit,\n",
    "    eval_dataset=val_dataset_vit,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=VIT_PROCESSOR,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b70f5a",
   "metadata": {},
   "source": [
    "### Predictions with Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_on_test = trainer.evaluate(eval_dataset=test_dataset_vit)\n",
    "print(eval_results_on_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8929cb",
   "metadata": {},
   "source": [
    "## 13. References\n",
    "\n",
    "1. AbubakarANuhu. (2025). Kaggle. *Chest X-Ray Disease Classification Using Resnet50*. <br>\n",
    "https://www.kaggle.com/code/abubakaranuhu/chest-x-ray-disease-classification-using-resnet50\n",
    "\n",
    "1. Ahmed Yassin. (2024). Medium. *Adam vs. AdamW: Understanding Weight Decay and Its Impact on Model Performance*.<br>\n",
    "https://yassin01.medium.com/adam-vs-adamw-understanding-weight-decay-and-its-impact-on-model-performance-b7414f0af8a1\n",
    "\n",
    "1. Bijay Kumar. (2025). PythonGuides.com. *Cross Entropy Loss in PyTorch*.<br>\n",
    "https://pythonguides.com/cross-entropy-loss-pytorch/\n",
    "\n",
    "1. BryanWBear. (2020). GitHub Issues. *\"No log\" when training RobertaForSequenceClassification using Trainer #8910*.\n",
    "https://github.com/huggingface/transformers/issues/8910\n",
    "\n",
    "1. Hugging Face. (2024). *Vision Transformer (base-sized model)*<br>\n",
    "https://huggingface.co/google/vit-base-patch16-224-in21k\n",
    "\n",
    "1. Hugging Face. (2021). *Vision Transformer (ViT)*.<br>\n",
    "https://huggingface.co/docs/transformers/en/model_doc/vit\n",
    "\n",
    "1. Kurtis Pykes. (2024). DataCamp.  *AdamW Optimizer in PyTorch Tutorial*.<br>\n",
    "https://www.datacamp.com/tutorial/adamw-optimizer-in-pytorch\n",
    "\n",
    "1. PyTorch Docs. (n.d.). *AdamW*. <br>\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
    "\n",
    "1. PyTorch Docs. (n.d.). *densenet121*.<br>\n",
    "https://docs.pytorch.org/vision/main/models/generated/torchvision.models.densenet121.html\n",
    "\n",
    "1. PyTorch Docs. (n.d.). *resnet50*.<br>\n",
    "https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "\n",
    "1. PyTorch Forums. (2021). *[Discussion] Why normalise according to ImageNet mean and std dev for transfer learning?*<br>\n",
    "https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html\n",
    "\n",
    "1. Tawsifur Rahman, Dr. Muhammad Chowdhury, Amith Khandakar. (2022). Kaggle. *COVID-19 Radiography Database*.<br>\n",
    "https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database/data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
