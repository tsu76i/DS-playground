{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94da32a6",
   "metadata": {},
   "source": [
    "# Animal Identification\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "1. [Device Agnostic-Code](#2-device-agnostic-code)\n",
    "1. [Loading Custom Dataset](#3-loading-custom-dataset)\n",
    "1. [Understanding Data](#4-understanding-data)\n",
    "1. [Data Preprocessing](#5-data-preprocessing)\n",
    "    - [Normalisation](#normalisation)\n",
    "    - [Preparing DataLoaders](#preparing-dataloaders)\n",
    "1. [Convolutional Neural Network (CNN) Architecture](#6-convolutional-neural-network-cnn-architectures)\n",
    "    - [ResNet-50](#resnet-50)\n",
    "    - [Structure](#structure)\n",
    "1. [Evaluation Metrics](#7-evaluation-metrics)\n",
    "1. [Loss Function](#8-loss-function)\n",
    "    - [Cross-Entropy Loss](#cross-entropy-loss)\n",
    "1. [Optimiser](#9-optimiser)\n",
    "1. [Training and Evaluation](#10-training-and-evaluation)\n",
    "    - [Training Steps](#training-steps)\n",
    "    - [Validation Steps](#validation-steps)\n",
    "1. [Results (Custom ResNet-50)](#11-results-custom-resnet-50)\n",
    "    - [Overall Performance](#overall-performance)\n",
    "    - [Classifications](#classifications)\n",
    "    - [Missclassifications](#missclassifications)\n",
    "    - [Confusion Matrix](#confusion-matrix)\n",
    "    - [Conclusion (Custom ResNet-50)](#conclusion-custom-resnet-50)\n",
    "1. [Transfer Learning](#12-transfer-learning)\n",
    "1. [Results (Pre-Trained ResNet-50)](#13-results-pre-trained-resnet-50)\n",
    "    - [Overall Performance](#overall-performance)\n",
    "    - [Classifications](#classifications)\n",
    "    - [Missclassifications](#missclassifications)\n",
    "    - [Confusion Matrix](#confusion-matrix)\n",
    "    - [Conclusion (Pre-Trained ResNet-50)](#conclusion-pre-trained-resnet-50)\n",
    "1. [References](#14-references)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b87a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392bd131",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b500e9bc",
   "metadata": {},
   "source": [
    "## 2. Device Agnostic-Code\n",
    "Mac GPU acceleration (`mps` backend) delivers significant speed-up over CPU for deep learning tasks, especially for large models and batch sizes. On Windows, `cuda` is used instead of `mps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33187c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # For Windows\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # For Mac\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf93d99",
   "metadata": {},
   "source": [
    "## 3. Loading Custom Dataset\n",
    "Retrieved from [Kaggle - Animal-10](https://www.kaggle.com/datasets/alessiocorrado99/animals10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility.\n",
    "random_seed = 2\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e162ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"_datasets/animal-10/raw-img\")\n",
    "\n",
    "if data_path.is_dir():\n",
    "    print(f\"{data_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"{data_path} Directory does not exist !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e14824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_it_en = {\n",
    "    \"cane\": \"dog\",\n",
    "    \"cavallo\": \"horse\",\n",
    "    \"elefante\": \"elephant\",\n",
    "    \"farfalla\": \"butterfly\",\n",
    "    \"gallina\": \"chicken\",\n",
    "    \"gatto\": \"cat\",\n",
    "    \"mucca\": \"cow\",\n",
    "    \"pecora\": \"sheep\",\n",
    "    \"ragno\": \"spider\",\n",
    "    \"scoiattolo\": \"squirrel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_int = {path.name: i for i, path in enumerate(list((data_path).iterdir()))}\n",
    "classes_to_str = {i: name for i, name in enumerate(classes_to_int)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbccdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_dir(dir_path):\n",
    "    for (\n",
    "        directory_path,\n",
    "        directory_names,\n",
    "        file_names,\n",
    "    ) in os.walk(dir_path):\n",
    "        print(\n",
    "            f\"{len(directory_names)} directories and {len(file_names)} images found in {directory_path}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1462d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_through_dir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path, transform=None) -> None:\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        self.all_paths = [path for path in img_path.glob(\"*/*\") if path.is_file()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        single_file_path = self.all_paths[index]\n",
    "        img = Image.open(single_file_path).convert(\"RGB\")\n",
    "        label = classes_to_int[single_file_path.parent.name]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a751f",
   "metadata": {},
   "source": [
    "## 4. Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_raw_samples(dataset: Dataset) -> None:\n",
    "    figure = plt.figure(figsize=(9, 9))\n",
    "    cols, rows = 3, 3\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        sample_idx = torch.randint(0, len(dataset), (1,)).item()\n",
    "        img, label = dataset[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(classes_to_str[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14079b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ds = CustomDataset(data_path)\n",
    "display_raw_samples(custom_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data size: {len(custom_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d685e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = [0, 25, 50]\n",
    "for i in sample_idx:\n",
    "    img, label = custom_ds[i]\n",
    "    img_array = np.array(img)\n",
    "    print(f\"Image of the index {i} has the shape {img_array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878030b",
   "metadata": {},
   "source": [
    "Each image has different height and width values (e.g., `(225, 300, 3)` refers to `(height, width, colour channel)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_targets = [classes_to_str[label] for _, label in custom_ds]\n",
    "class_names = np.unique(class_names_targets)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d75ed",
   "metadata": {},
   "source": [
    "The dataset contains 10 different animal categories with an imbalanced number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals, counts = np.unique(class_names_targets, return_counts=True)\n",
    "df_dist = pd.DataFrame({\"Class Label\": unique_vals, \"Count\": counts})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_dist, x=\"Class Label\", y=\"Count\", hue=\"Class Label\", palette=\"Set2\")\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Class Labels\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63208560",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "### Normalisation\n",
    "For simple models (shallow networks, logistic regression, etc.), `ToTensor()` is often sufficient as it rescales image pixel values to the range from 0 to 1. However, for state-of-the-art architectures, it is strongly recommended to re-normalise (standardise) inputs so that each colour channel has zero mean and unit variance. Many pretrained models are trained on such normalised inputs, therefore this approach tends to yield better results than basic normalisation. Furthermore, centring inputs around zero generally results in more stable training and faster convergence, particularly for architectures with activation functions such as tanh or certain weight initialisation schemes.\n",
    "\n",
    "Let:\n",
    "- $X_{n, c, h, w}$: Pixel value for image $n$, channel $c$, height $h$, and width $w$.\n",
    "- $N$: Total number of images.\n",
    "- $C$: Number of channels (RGB = 3).\n",
    "- $H, W$: Height and width of an image.\n",
    "\n",
    "For each batch of images ($\\left[B, 3, 32, 32\\right]$), \n",
    "- Mean per channel:\n",
    "$$\n",
    "\\mu_{\\text{batch, c}} = \\dfrac{1}{B \\cdot H \\cdot W}\\sum^{B}_{n=1} \\sum^{H}_{h=1} \\sum^{W}_{w=1} X_{n, c, h, w}\n",
    "$$\n",
    "\n",
    "- Squared mean per channel:\n",
    "\n",
    "$$\n",
    "s_{\\text{batch, c}} = \\dfrac{1}{B \\cdot H \\cdot W}\\sum^{B}_{n=1} \\sum^{H}_{h=1} \\sum^{W}_{w=1} X^2_{n, c, h, w}\n",
    "$$\n",
    "\n",
    "- Mean:\n",
    "$$\n",
    "\\mu = \\dfrac{\\sum_{\\text{batches}} \\mu_{\\text{batch, c}}}{n_{\\text{batches}}}\n",
    "$$\n",
    "\n",
    "Using the identity $\\text{Var}(X) = E\\left[(X - \\mu \\right)^2]$ :\n",
    "- Standard deviation:\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\sigma &= \\sqrt{E\\left[X^2\\right] - (E\\left[X\\right])^2} \\\\\n",
    " &= \\sqrt{\\dfrac{\\sum_{\\text{batches}} s_{\\text{batch, c}}}{n_{\\text{batches}}} - \\mu^2}\n",
    "\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(train_loader):\n",
    "    channel_sum, channel_squared_sum, n_batches = 0, 0, 0\n",
    "    for data, _ in train_loader:\n",
    "        # Shape: [batch_size, channel=3, height=*, width=*]\n",
    "        channel_sum += torch.mean(data, dim=(0, 2, 3))\n",
    "        channel_squared_sum += torch.mean(data**2, dim=(0, 2, 3))\n",
    "        n_batches += 1\n",
    "    mean = channel_sum / n_batches\n",
    "    std = (channel_squared_sum / n_batches - mean**2) ** 0.5\n",
    "    print(f\"Mean: {mean.tolist()}\\nStd:{std.tolist()}\")\n",
    "    return mean.tolist(), std.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959c7c4",
   "metadata": {},
   "source": [
    "### Preparing DataLoaders\n",
    "`torch.utils.data.DataLoader()` increases the computational efficiency by dividing a large dataset into smaller chunks (called **batches** or **mini-batches**). The size of these batches is controlled by the hyperparameter `batch_size`. Processing data in batches allows gradient descent to be performed once per batch rather than once per epoch, facilitating faster and more stable training process. \n",
    "\n",
    "The `prepare_dataloaders()` function follows the steps:\n",
    "\n",
    "1. Apply stratified splitting.\n",
    "    - The dataset is partitioned into training (80%), validation (10%) and test (10%) subsets.\n",
    "    - Stratified splitting ensures each class is proportionally represented in all subsets.\n",
    "1. Calculate the mean and standard deviation of the training dataset.\n",
    "    - The mean and std are computed ONLY on the training split to avoid data leakage.\n",
    "1. Normalise training, validation and testing datasets and apply data augmentation.\n",
    "    - The computed mean and standard deviation are used to normalise all data subsets.\n",
    "    - Data augmentation, such as random horizontal flipping or rotation, is applied only to the training data to increase data diversity and enhance generalisation.\n",
    "1. Create dataloaders.\n",
    "    - SubsetRandomSampler is used to randomly select samples from given index lists (equivalent to setting shuffle=True).\n",
    "    - Return dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(dataset, batch_size, img_size):\n",
    "    # ! 1. Apply stratified splitting and divide dataset into train(80%), validation (10%) and test(10%) subsets.\n",
    "    # Split into: Training+Validation / Test\n",
    "    labels = [classes_to_int[path.parent.name] for path in dataset.all_paths]\n",
    "    train_val_indices, test_indices = train_test_split(\n",
    "        list(range(len(labels))),\n",
    "        test_size=0.1,\n",
    "        stratify=labels,\n",
    "        random_state=random_seed,\n",
    "    )\n",
    "    # Split into: Training / Validation\n",
    "    train_val_labels = [labels[i] for i in train_val_indices]\n",
    "    train_indices, validation_indices = train_test_split(\n",
    "        train_val_indices,\n",
    "        test_size=0.111111,  # 0.1/0.9 for 80/10/10\n",
    "        stratify=train_val_labels,\n",
    "        random_state=random_seed,\n",
    "    )\n",
    "\n",
    "    # ! 2. Calculate mean and std of train dataset.\n",
    "    transform_for_stats = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    stats_dataset = CustomDataset(data_path, transform_for_stats)\n",
    "    stats_sampler = SubsetRandomSampler(train_indices)\n",
    "    stats_loader = DataLoader(stats_dataset, batch_size, stats_sampler)\n",
    "\n",
    "    mean, std = get_mean_and_std(stats_loader)\n",
    "\n",
    "    # ! 3. Prepare transformations for train and test subsets.\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(img_size, img_size)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "    validation_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(size=(img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = CustomDataset(data_path, train_transform)\n",
    "    validation_dataset = CustomDataset(data_path, validation_transform)\n",
    "    test_dataset = CustomDataset(data_path, validation_transform)\n",
    "\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    validation_subset = Subset(validation_dataset, validation_indices)\n",
    "    test_subset = Subset(test_dataset, test_indices)\n",
    "\n",
    "    # ! 4. Using SubsetRandomSampler, create train & test dataloaders.\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_subset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    validation_dataloader = DataLoader(\n",
    "        dataset=validation_subset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_subset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    print(\n",
    "        f\"Length of train_dataloader: {len(train_indices)}/{batch_size} = {len(train_dataloader)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Length of validation_dataloader: {len(validation_indices)}/{batch_size} = {len(validation_dataloader)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Length of test_dataloader: {len(test_indices)}/{batch_size} = {len(test_dataloader)}\"\n",
    "    )\n",
    "    return train_dataloader, validation_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d96ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "train_dataloader, validation_dataloader, test_dataloader = prepare_dataloaders(\n",
    "    custom_ds, BATCH_SIZE, IMAGE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bd8e4",
   "metadata": {},
   "source": [
    "## 6. Convolutional Neural Network (CNN) Architectures\n",
    "### ResNet-50\n",
    "ResNet-50 is a deep convolutional neural network (CNN) belonging to the Residual Networks (ResNet) family, developed to address the *vanishing gradient problem* that impairs the training of very deep networks. The core innovation of ResNet architecture is the residual block with skip (shortcut) connections, allowing the network to learn residual mappings instead of direct mappings, which stabilises training and makes deeper architectures feasible.\n",
    "\n",
    "ResNet-50 comprises 50 learnable layers, making it substantially deeper and more expressive than shallower variants such as ResNet-18. The architecture primarily consists of an initial convolutional layer, followed by a series of bottleneck residual blocks, batch normalisation, rectified linear unit (ReLU) activations, and culminates in a fully connected (FC) layer for classification. Its deeper architecture and increased capacity make ResNet-50 particularly suitable for tasks where high representational power and feature extraction capabilities are needed, such as large-scale image classification challenges.\n",
    "\n",
    "### Structure\n",
    "\n",
    "| Layer/Block                        | Details                                                                               |\n",
    "|-------------------------------------|--------------------------------------------------------------------------------------|\n",
    "| Initial Conv Layer                  | 7×7, 64 filters, stride 2, padding 3                                                 |\n",
    "| Batch Normalisation & ReLU          | Applied after initial convolutional layer                                             |\n",
    "| Max Pooling                         | 3×3, stride 2, padding 1                                                             |\n",
    "| Residual Block Stage 1 (Layer1)     | 3 bottleneck residual blocks, 256 filters each (1×1, 3×3, 1×1 convolutions)           |\n",
    "| Residual Block Stage 2 (Layer2)     | 4 bottleneck residual blocks, 512 filters each (1×1, 3×3, 1×1 convolutions)           |\n",
    "| Residual Block Stage 3 (Layer3)     | 6 bottleneck residual blocks, 1024 filters each (1×1, 3×3, 1×1 convolutions)          |\n",
    "| Residual Block Stage 4 (Layer4)     | 3 bottleneck residual blocks, 2048 filters each (1×1, 3×3, 1×1 convolutions)          |\n",
    "| Global Average Pooling              | Reduces feature maps to 1×1                                                          |\n",
    "| Fully Connected Layer               | Outputs class scores (typically 1,000 for ImageNet)                                  |\n",
    "\n",
    "**Total**: 1 (initial conv) + (3 + 4 + 6 + 3) (residual blocks per stage) × 3(convs per block) + 1 (FC) = 50 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96499613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, shortcut, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=mid_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "        )  # 1x1, 1st conv\n",
    "        self.bn_1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            in_channels=mid_channels,\n",
    "            out_channels=mid_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "        )  # 3x3, 2nd conv\n",
    "        self.bn_2 = nn.BatchNorm2d(mid_channels)\n",
    "        self.conv_3 = nn.Conv2d(\n",
    "            in_channels=mid_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "        )  # 1x1, 3rd conv\n",
    "        self.bn_3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if shortcut:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    padding=0,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)  # Shortcut path\n",
    "        out = self.relu(self.bn_1(self.conv_1(x)))  # 1x1 conv + BN + ReLU\n",
    "        out = self.relu(self.bn_2(self.conv_2(out)))  # 3x3 conv + BN + ReLU\n",
    "        out = self.bn_3(self.conv_3(out))  # 1x1 conv + BN (Without ReLU)\n",
    "        out += identity  # Add shortcut\n",
    "        return self.relu(out)  # Final ReLU Activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        # Layer 0: Conv -> MaxPool -> BN -> ReLU\n",
    "        self.layer_0 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=64,\n",
    "                kernel_size=7,\n",
    "                stride=2,\n",
    "                padding=3,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # 3 -> 4 -> 6 -> 3 BRBs.\n",
    "        # Layer 1: 3 Bottleneck Residual Blocks, 256 filters\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            BottleneckBlock(\n",
    "                in_channels=64,\n",
    "                mid_channels=64,\n",
    "                out_channels=256,\n",
    "                shortcut=True,\n",
    "                stride=1,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=256,\n",
    "                mid_channels=64,\n",
    "                out_channels=256,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=256,\n",
    "                mid_channels=64,\n",
    "                out_channels=256,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "        )\n",
    "        # Layer 2: 4 Bottleneck Residual Blocks, 512 filters\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            BottleneckBlock(\n",
    "                in_channels=256,\n",
    "                mid_channels=128,\n",
    "                out_channels=512,\n",
    "                shortcut=True,\n",
    "                stride=2,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=512,\n",
    "                mid_channels=128,\n",
    "                out_channels=512,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=512,\n",
    "                mid_channels=128,\n",
    "                out_channels=512,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=512,\n",
    "                mid_channels=128,\n",
    "                out_channels=512,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "        )\n",
    "        # Layer 3: 6 Bottleneck Residual Blocks, 1024 filters\n",
    "        self.layer_3 = nn.Sequential(\n",
    "            BottleneckBlock(\n",
    "                in_channels=512,\n",
    "                mid_channels=256,\n",
    "                out_channels=1024,\n",
    "                shortcut=True,\n",
    "                stride=2,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=1024,\n",
    "                mid_channels=256,\n",
    "                out_channels=1024,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=1024,\n",
    "                mid_channels=256,\n",
    "                out_channels=1024,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=1024,\n",
    "                mid_channels=256,\n",
    "                out_channels=1024,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=1024,\n",
    "                mid_channels=256,\n",
    "                out_channels=1024,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=1024,\n",
    "                mid_channels=256,\n",
    "                out_channels=1024,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "        )\n",
    "        # Layer 4: 3 Bottleneck Residual Blocks, 2048 filters\n",
    "        self.layer_4 = nn.Sequential(\n",
    "            BottleneckBlock(\n",
    "                in_channels=1024,\n",
    "                mid_channels=512,\n",
    "                out_channels=2048,\n",
    "                shortcut=True,\n",
    "                stride=2,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=2048,\n",
    "                mid_channels=512,\n",
    "                out_channels=2048,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "            BottleneckBlock(\n",
    "                in_channels=2048,\n",
    "                mid_channels=512,\n",
    "                out_channels=2048,\n",
    "                shortcut=False,\n",
    "                stride=1,\n",
    "            ),\n",
    "        )\n",
    "        # Classifier (AdaptiveAvgPool -> Flatten -> Dropout -> Fully connected layer)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(2048, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_0(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27daba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_50 = ResNet50(num_classes=len(class_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cec3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    resnet_50,\n",
    "    input_size=(\n",
    "        1,\n",
    "        3,\n",
    "        IMAGE_SIZE,\n",
    "        IMAGE_SIZE,\n",
    "    ),  # (batch_size, colour channels, height, width)\n",
    "    verbose=0,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c1fbc9",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "We will use the following evaluation metrics:\n",
    "- `torchmetrics.Accuracy`\n",
    "- `torchmetrics.F1Score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy, F1Score\n",
    "\n",
    "n_classes = len(class_names)\n",
    "\n",
    "calculate_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes).to(device)\n",
    "\n",
    "calculate_f1 = F1Score(task=\"multiclass\", num_classes=n_classes, average=\"macro\").to(\n",
    "    device\n",
    ")\n",
    "\n",
    "metrics = [calculate_accuracy, calculate_f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f963e4d",
   "metadata": {},
   "source": [
    "## 8. Loss Function\n",
    "### Cross-Entropy Loss\n",
    "Cross-Entropy Loss is a loss function used for classification problems, particularly when the model outputs probabilities using a softmax activation in the final layer. It measures the difference between the true labels and the predicted probability distribution.\n",
    "\n",
    "For a single data point, the cross-entropy loss is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "    L = - \\sum^{k}_{i=1}y_{i}\\log{(\\hat y_{i})}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True label for the $i$-th class. If one-hot encoded, $y_{i} = 1$ for the corrected class, $y_{i} = 0$ otherwise.\n",
    "- $\\hat y_i$: Predicted probability for the $i$-th class.\n",
    "- $k$: Number of classes.\n",
    "\n",
    "For a batch of $m$ data point:\n",
    "\n",
    "\\begin{align*}\n",
    "    C = \\dfrac{1}{m} \\sum^{m}_{j=1} \\left (- \\sum^{k}_{i=1}y_{j, i}\\log{(\\hat y_{j, i})} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $C$: Average cross-entropy loss over the batch.\n",
    "- $m$: Number of training examples (batch size).\n",
    "- $k$: Number of classes.\n",
    "- $y_{j, i} \\in { 0, 1}$: Indicator that true class for sample $j$ corresponds to class $i$.\n",
    "- $\\hat y_{j, i} \\in { 0, 1}$: Predicted probability for sample $j$ belonging to class $i$.\n",
    "\n",
    "In PyTorch:\n",
    "- Use `nn.CrossEntropyLoss()` directly with raw logits.\n",
    "- Do not apply `Softmax()` or `LogSoftmax()` manually before the loss.\n",
    "- Internally, `nn.CrossEntropyLoss() = LogSoftmax() + NegativeLogLikelihoodLoss()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000e356",
   "metadata": {},
   "source": [
    "## 9. Optimiser\n",
    "An optimiser in neural networks is used to adjust the parameters (weights and biases) of a model during training to minimise the loss. Optimisers are essential for enabling neural networks to learn from data: without them, the model would not improve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.SGD(\n",
    "    resnet_50.parameters(), lr=0.005, weight_decay=0.0005, momentum=0.9\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(optimiser, mode=\"min\", factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88d6a9",
   "metadata": {},
   "source": [
    "## 10. Training and Evaluation\n",
    "1. Iterate through epochs\n",
    "1. For each epoch, iterate through training batches, perform training steps, calculate the train loss and evaluation metrics per batch.\n",
    "1. For each epoch, iterate through testing batches, perform testing steps, calculate the test loss and evaluation metrics per batch.\n",
    "1. Store the results.\n",
    "\n",
    "### Training Steps\n",
    "1. Forward Pass\n",
    "    - Pass inputs through the model to obtain predictions.\n",
    "1. Calculate Loss Per Batch\n",
    "    - Measure how far the predictions deviate from the true labels, using a loss function.\n",
    "1. Zero the Gradients\n",
    "    - Clear the previously stored gradients to prevent accumulation from multiple backward passes.\n",
    "1. Backward Pass\n",
    "    - Computes gradients of the loss with respect to the model's parameters via backpropagation.\n",
    "1. Optimiser Step\n",
    "    - Update the parameter $\\theta$ using the gradients just computed, typically following an equation such as:\n",
    "    $$\n",
    "        \\theta \\leftarrow \\theta - \\eta \\dfrac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "    $$\n",
    "    where $\\eta$ is the learning rate.\n",
    "1. Average Training Loss\n",
    "    - Computes the mean training loss across all batches for the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    loss_function: nn.Module,\n",
    "    optimiser: torch.optim.Optimizer,\n",
    "    metrics: List[nn.Module],\n",
    "    device: torch.device = device,\n",
    "):\n",
    "    model.to(device)\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "    for X, y in tqdm(data_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss per batch\n",
    "        loss = loss_function(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric.update(y_pred, y)\n",
    "\n",
    "        # 3. Optimiser zero grad\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimiser step\n",
    "        optimiser.step()\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average per batch per epoch)\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc = calculate_accuracy.compute().item() * 100\n",
    "    train_f1 = calculate_f1.compute().item() * 100\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "    train_metrics = [train_acc, train_f1]\n",
    "    return train_loss, train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761fdbe",
   "metadata": {},
   "source": [
    "### Validation Steps\n",
    "1. Forward pass\n",
    "    - Set the model to evaluation mode (which disables dropout and batch normalisation and desactivates gradient tracking for safety).\n",
    "    - Pass inputs through the model to obtain predictions.\n",
    "1. Calculate Loss Per Batch\n",
    "    - Measure how far the predictions deviate from the true labels, using a loss function.\n",
    "1. Update and Compute Accuracy\n",
    "    - Updates accuracy state with each batch, and compute the overall accuracy after all validation batches.\n",
    "1. Average Validation Loss\n",
    "    - Computes the mean Test loss across all batches for the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b76cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    loss_function: nn.Module,\n",
    "    metrics: List[nn.Module],\n",
    "    device: torch.device = device,\n",
    "):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            validation_pred = model(X)\n",
    "\n",
    "            # 2. Calculate loss\n",
    "            validation_loss += loss_function(validation_pred, y).item()\n",
    "\n",
    "            # 3. Calculate metrics\n",
    "            for metric in metrics:\n",
    "                metric.update(validation_pred, y)\n",
    "\n",
    "        # 4. Take the averages of test loss and compute metrics\n",
    "        validation_loss /= len(data_loader)\n",
    "    validation_acc = calculate_accuracy.compute().item() * 100\n",
    "    validation_f1 = calculate_f1.compute().item() * 100\n",
    "    print(\n",
    "        f\"Validation loss: {validation_loss:.5f} | Validation accuracy: {validation_acc:.2f}%\\n\"\n",
    "    )\n",
    "    validation_metrics = [validation_acc, validation_f1]\n",
    "    return validation_loss, validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "train_losses, train_accuracies, train_f1s = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "validation_losses, validation_accuracies, validation_f1s = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "for epoch in epochs_range:\n",
    "    print(f\"Epoch: {epoch}\\n==========\")\n",
    "    train_loss, train_metrics = train_step(\n",
    "        data_loader=train_dataloader,\n",
    "        model=resnet_50,\n",
    "        loss_function=loss_function,\n",
    "        optimiser=optimiser,\n",
    "        metrics=metrics,\n",
    "        device=device,\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_metrics[0])\n",
    "    train_f1s.append(train_metrics[1])\n",
    "\n",
    "    validation_loss, validation_metrics = validation_step(\n",
    "        data_loader=validation_dataloader,\n",
    "        model=resnet_50,\n",
    "        loss_function=loss_function,\n",
    "        metrics=metrics,\n",
    "        device=device,\n",
    "    )\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracies.append(validation_metrics[0])\n",
    "    validation_f1s.append(validation_metrics[1])\n",
    "\n",
    "    scheduler.step(validation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fa40b",
   "metadata": {},
   "source": [
    "## 11. Results (Custom ResNet-50)\n",
    "### Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2791d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = {\n",
    "    \"Loss\": train_losses,\n",
    "    \"Accuracy\": train_accuracies,\n",
    "    \"F1 Score\": train_f1s,\n",
    "}\n",
    "\n",
    "validation_metrics = {\n",
    "    \"Loss\": validation_losses,\n",
    "    \"Accuracy\": validation_accuracies,\n",
    "    \"F1 Score\": validation_f1s,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(epochs_range, train_metrics, validation_metrics) -> None:\n",
    "    metric_names = list(train_metrics.keys())\n",
    "    n_metrics = len(metric_names)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        ax = axes[i]\n",
    "        ax.plot(\n",
    "            epochs_range, train_metrics[metric_name], label=f\"Train {metric_name}\"\n",
    "        )  # Train metric\n",
    "        ax.plot(\n",
    "            epochs_range,\n",
    "            validation_metrics[metric_name],\n",
    "            label=f\"Validation {metric_name}\",\n",
    "        )  # Validation metric\n",
    "        ax.set_title(f\"{metric_name} Over Epochs\", fontsize=15)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        if metric_name == \"Loss\":\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "        else:\n",
    "            ax.set_ylabel(f\"{metric_name} (%)\")\n",
    "\n",
    "    if n_metrics < len(axes):\n",
    "        for j in range(n_metrics, len(axes)):\n",
    "            plt.delaxes(axes[j])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(epochs_range, train_metrics, validation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4bdac8",
   "metadata": {},
   "source": [
    "### Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2aa524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_predictions(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    loss_function: nn.Module,\n",
    "    calculate_accuracy: nn.Module,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "    y_preds = []\n",
    "    y_labels = []\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    calculate_accuracy.reset()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            y_prob = torch.softmax(test_pred, dim=1)\n",
    "            y_pred = y_prob.argmax(dim=1)\n",
    "            y_preds.append(y_pred.cpu())\n",
    "            y_labels.append(y.cpu())\n",
    "            # 2. Calculate test loss\n",
    "            test_loss += loss_function(test_pred, y).item()\n",
    "\n",
    "            # 3. Calculate test accuracy\n",
    "            calculate_accuracy.update(test_pred, y)\n",
    "\n",
    "        # 4. Take the averages of test loss and compute metrics\n",
    "        test_loss /= len(data_loader)\n",
    "    test_acc = calculate_accuracy.compute().item() * 100\n",
    "    print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n",
    "    y_preds_tensor = torch.cat(y_preds)\n",
    "    y_labels_tensor = torch.cat(y_labels)\n",
    "    return y_preds_tensor, y_labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels = make_all_predictions(\n",
    "    model=resnet_50,\n",
    "    data_loader=test_dataloader,\n",
    "    loss_function=loss_function,\n",
    "    calculate_accuracy=calculate_accuracy,\n",
    "    device=device,\n",
    ")\n",
    "print(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399e9fe",
   "metadata": {},
   "source": [
    "### Missclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b043dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices = (all_preds != all_labels).nonzero(as_tuple=True)[0]\n",
    "print(\n",
    "    f\"Number of failed predictions: {len(wrong_indices)}/{len(all_labels)} ({100.0 * len(wrong_indices) / len(all_labels):.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b45ce4",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# Convert to tensors if not already\n",
    "true_labels = all_labels\n",
    "pred_tensor = all_preds.detach().clone()\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = ConfusionMatrix(num_classes=len(class_names), task=\"multiclass\")\n",
    "conf_matrix_tensor = conf_matrix(pred_tensor, true_labels)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=conf_matrix_tensor.numpy(), class_names=class_names, figsize=(10, 7)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c307d",
   "metadata": {},
   "source": [
    "### Conclusion (Custom ResNet-50)\n",
    "After 10 epochs, our custom ResNet-50 model achieved an accuracy of $75.82$% on the test dataset (misclassification rate of $24.18$%). The confusion matrix illustrates the distribution of correctly and incorrectly classified images across each label.\n",
    "\n",
    "A detailed inspection reveals the following misclassification patterns:\n",
    "\n",
    "- Horses (cavallo) were sometimes misclassified as butterflies (farfalla).\n",
    "- Dogs (cane) were occasionally mistaken for other animals, such as chickens (gallina), cats (gatto), cows (mucca) and sheep (pecora).\n",
    "\n",
    "Around epoch 10, the model began to overfit, as indicated by a continued increase in training accuracy while validation accuracy fluctuated or decreased in subsequent epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ab740",
   "metadata": {},
   "source": [
    "## 12. Transfer Learning\n",
    "We can use pre-trained models to improve performance on related tasks, while simultaneously reducing both the amount of training data and the time required. The use of pre-trained models to address other, related problems is known as transfer learning.\n",
    "\n",
    "Setting `param.requires_grad = False` tells PyTorch not to compute gradients for all the parameters (weights and biases) of the model ResNet-50 so that they stay frozen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "pretrained_resnet_50 = models.resnet50(\n",
    "    weights=\"IMAGENET1K_V1\"\n",
    ")  # Pre-trained ResNet-50 Model\n",
    "for param in pretrained_resnet_50.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74195ae",
   "metadata": {},
   "source": [
    "We typically freeze layers to train only the new, custom layers, such as the final classifier layer, avoiding overfitting and speeding up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8130c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = pretrained_resnet_50.fc.in_features\n",
    "hidden_dim = 256\n",
    "pretrained_resnet_50.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(n_features, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, len(class_names)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(\n",
    "    pretrained_resnet_50,\n",
    "    input_size=(\n",
    "        1,\n",
    "        3,\n",
    "        IMAGE_SIZE,\n",
    "        IMAGE_SIZE,\n",
    "    ),  # (batch_size, colour channels, height, width)\n",
    "    verbose=0,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195b588",
   "metadata": {},
   "source": [
    "The optimiser must be re-initialised only with the unfrozen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.SGD(\n",
    "    filter(lambda p: p.requires_grad, pretrained_resnet_50.parameters()),\n",
    "    lr=0.005,\n",
    "    weight_decay=0.0005,\n",
    "    momentum=0.9,\n",
    ")\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimiser, mode=\"min\", factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bbe1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies, train_f1s = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "validation_losses, validation_accuracies, validation_f1s = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "for epoch in epochs_range:\n",
    "    print(f\"Epoch: {epoch}\\n==========\")\n",
    "    train_loss, train_metrics = train_step(\n",
    "        data_loader=train_dataloader,\n",
    "        model=pretrained_resnet_50,  # Use pre-trained ResNet-50\n",
    "        loss_function=loss_function,\n",
    "        optimiser=optimiser,\n",
    "        metrics=metrics,\n",
    "        device=device,\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_metrics[0])\n",
    "    train_f1s.append(train_metrics[1])\n",
    "\n",
    "    validation_loss, validation_metrics = validation_step(\n",
    "        data_loader=validation_dataloader,\n",
    "        model=pretrained_resnet_50,  # Use pre-trained ResNet-50\n",
    "        loss_function=loss_function,\n",
    "        metrics=metrics,\n",
    "        device=device,\n",
    "    )\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracies.append(validation_metrics[0])\n",
    "    validation_f1s.append(validation_metrics[1])\n",
    "\n",
    "    scheduler.step(validation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0131c75",
   "metadata": {},
   "source": [
    "## 13. Results (Pre-Trained ResNet-50)\n",
    "### Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c14f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = {\n",
    "    \"Loss\": train_losses,\n",
    "    \"Accuracy\": train_accuracies,\n",
    "    \"F1 Score\": train_f1s,\n",
    "}\n",
    "\n",
    "validation_metrics = {\n",
    "    \"Loss\": validation_losses,\n",
    "    \"Accuracy\": validation_accuracies,\n",
    "    \"F1 Score\": validation_f1s,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(epochs_range, train_metrics, validation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117a608",
   "metadata": {},
   "source": [
    "### Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels = make_all_predictions(\n",
    "    model=pretrained_resnet_50,  # Use pre-trained ResNet-50\n",
    "    data_loader=test_dataloader,\n",
    "    loss_function=loss_function,\n",
    "    calculate_accuracy=calculate_accuracy,\n",
    "    device=device,\n",
    ")\n",
    "print(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d8c5d",
   "metadata": {},
   "source": [
    "### Missclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices = (all_preds != all_labels).nonzero(as_tuple=True)[0]\n",
    "print(\n",
    "    f\"Number of failed predictions: {len(wrong_indices)}/{len(all_labels)} ({100.0 * len(wrong_indices) / len(all_labels):.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e055c6e",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# Convert to tensors if not already\n",
    "true_labels = all_labels\n",
    "pred_tensor = all_preds.detach().clone()\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = ConfusionMatrix(num_classes=len(class_names), task=\"multiclass\")\n",
    "conf_matrix_tensor = conf_matrix(pred_tensor, true_labels)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=conf_matrix_tensor.numpy(), class_names=class_names, figsize=(10, 7)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1a70e",
   "metadata": {},
   "source": [
    "### Conclusion (Pre-Trained ResNet-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55b57e",
   "metadata": {},
   "source": [
    "## 14. References\n",
    "\n",
    "1. Aditi Rastogi. (2022). *ResNet50*. <br>\n",
    "https://blog.devgenius.io/resnet50-6b42934db431\n",
    "\n",
    "1. Bader Dammak. (2023). *ResNet-50 training from scratch on Animal-10 Dataset*.<br>\n",
    "https://github.com/Darkmyter/Popular-models-implemented-in-Pytorch/blob/main/Computer-vision/image-classification/resnet-50-animal-10.ipynb\n",
    "\n",
    "1. Dhruv Matani. (2023). *A Practical Guide to Transfer Learning using PyTorch*.<br>\n",
    "https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html\n",
    "\n",
    "1. He, K., Zhang, X., Ren, S., Sun, J. (2015). *Deep Residual Learning for Image Recognition*. arXiv preprint arXiv:1512.03385.<br>\n",
    "https://arxiv.org/abs/1512.03385\n",
    "\n",
    "1. PyTorch Docs. (n.d.). *torchvision.models.resnet*<br>\n",
    "https://docs.pytorch.org/vision/stable/_modules/torchvision/models/resnet.html#ResNet50_Weights\n",
    "\n",
    "1. PyTorch Docs. (n.d.). *Models and pre-trained weights*.<br>\n",
    "https://docs.pytorch.org/vision/0.21/models.html#models-and-pre-trained-weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
