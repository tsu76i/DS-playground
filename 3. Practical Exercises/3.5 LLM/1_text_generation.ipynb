{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb3b7d3",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f809ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c49ec7",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "## 2. Device Agnostic Code\n",
    "GPU acceleration delivers significant speed-up over CPU for deep learning tasks, especially for large models and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cc87fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    device=\"cuda\"  # GPU\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"  # MPS (MacOS)\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"  # No GPU Available\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8ddb7",
   "metadata": {},
   "source": [
    "## 3. Loading Pre-Trained Model\n",
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e524fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-large\"\n",
    "MAX_TOKENS = 50\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddfada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855e8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Hello! Today I\"\n",
    "\n",
    "model_inputs = tokeniser(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f85467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,     0,  6288,   314]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f2dcc",
   "metadata": {},
   "source": [
    "## 4. Repetition\n",
    "Phrases being repeated in the generated text is a common phenomenon in augoregressive language models. If a token or phrase has a high probability continuation in training data, the model's predictions tend to loop, generating the same phrase repeatedly. \n",
    "To avoid or reduce repetition, several techniques can be applied:\n",
    "\n",
    "- Employ nucleus sampling or top-k sampling (explained in detail below).\n",
    "- Increase `temperature` above 1.0 to flatten probability distribution, encouraging more random choices.\n",
    "- Use `repetition_penalty` (typically between 1.1 and 1.5) to reduce the probability of tokens that have already been generated.\n",
    "- Apply `no_repeat_ngram_size` to strictly prevent the model from generating the same n-gram repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb9634",
   "metadata": {},
   "source": [
    "## 5. Basic Generation Strategies\n",
    "### Greedy Search\n",
    "Greedy Search is the default setting of decoding strategy used by `.generate()`. At each step, it selects the token with the highest probability as the next token. This method is simple and fast, thus is suitable for generating short text. However, for longer text, it can lead to repetitive and less diverse sequences.\n",
    "\n",
    "By default, greedy search generates up to 20 new tokens unless specified in `GenerationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1316bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to show you how to make a simple, yet effective, way to get your hands on a few of the most popular and most sought after brands of beer.\n",
      "\n",
      "The first thing you need to do is to find a beer that you\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386670d6",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "Sampling selects a next token randomly based on the probability distribution over the entire vocabulary of the model. This reduces repetition and can generate more creative, diverse outputs compared to the greedy search strategy.\n",
    "\n",
    "Sampling is enabled by setting the parameters: `do_sample=True` and `num_beams=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27fbac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I have the most awaited update for the first installment of my Daedric armor, The Golden Crown. You guys have waited for it since the release and I'm really happy to be able to share it with you.\n",
      "\n",
      "I've always wanted to\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180c8f6",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "Beam Search maintains multiple candidate sequences (beams) simultaneously. At each step, it expands each beam by selecting tokens, then retains the top $k$ beams based on cumulative (overall) probability score. This strategy is suited for input-grounded tasks such as image captioning or speech recognition.\n",
    "\n",
    "Beam search is enabled by setting `num_beams > 1`, optionally combined with `do_sample = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c64f65d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to show you how to make a simple, easy-to-use, and easy to use app that you can use in your web applications.\n",
      "\n",
      "In this tutorial, we'll be creating a web application that will allow you to search\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    num_beams=5,\n",
    "    do_sample=True,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aac06c",
   "metadata": {},
   "source": [
    "### Top-k Sampling\n",
    "At each step of generation, the model predicts a probability distribution over the entire vocabulary for the next token, then selects only the top $k$ most probable tokens, ranked by their predicted probability. The probabilities of these top $k$ tokens are renormalised to sum to 1, and the next token is randomly sampled from this restricted set.\n",
    "\n",
    "The number $k$ is configured by the parameter `top_k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d91960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm excited to introduce you to my favorite type of game â€“ one that I have played for years but only just recently discovered. The game in question is called \"Doom\" and has a pretty simple concept: shoot zombies with the bow.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f598d0",
   "metadata": {},
   "source": [
    "### Top-p (Nucleus) Sampling\n",
    "Instead of selecting tokens from the entire vocabulary, nucleus sampling samples from the smallest set of tokens whose cumulative probability exceeds the threshold $p$. This introduces controlled randomness, resulting in more diverse and creative text generation.\n",
    "\n",
    "The probability threshold $p$ is configured by the parameter `top_p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cbc91d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I would like to share some of the best tips and tricks from my 10 years of coaching, along with a few of my own ideas and tactics I've tried on myself.\n",
      "\n",
      "In no particular order, I'll share my most helpful tactics, techniques\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66911c",
   "metadata": {},
   "source": [
    "## 6. Advanced Generation Strategies\n",
    "### Speculative Decoding\n",
    "Speculative decoding uses a second smaller draft model to generate multiple speculative tokens which are then verified by the larget target model to speed up autoregressive decoding. For example, with GPT-2 model:\n",
    "- Draft model: smaller GPT-2 (e.g., `gpt2` or `distilgpt2`)\n",
    "- Target model: `gpt2-large`\n",
    "\n",
    "Speculative decoding can be enabled by setting a draft model to the parameter `assistant_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d2a951f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I am talking about creating an interface that uses a \"cabal\" in some way. I have been working with this interface for an article that was published in RethinkDB, and I want to share some of my thoughts with you guys on this\n"
     ]
    }
   ],
   "source": [
    "draft_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    assistant_model=draft_model,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184c755",
   "metadata": {},
   "source": [
    "Or, we can use the same parameter in Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e99133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I have a new set of skills that I want you to try out. The first one is a very easy one, which is, to roll over a random enemy for a point. Here's the roll over:\n",
      "\n",
      "Here's how the skill looks\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    tokenizer=tokeniser,\n",
    "    model=model,\n",
    "    assistant_model=draft_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipe_output = pipe(\n",
    "    text_inputs=input_text,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    ")\n",
    "print(pipe_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ff03f",
   "metadata": {},
   "source": [
    "### Prompt Lookup Decoding\n",
    "Prompt Lookup Decoding is a variant of speculative decoding that leverages the significant overlap between the input prompt and the generated output. Unlike traditional speculative decoding that uses a smaller draft model to propose next tokens, this method uses substring matching directly on the input prompt to generate candidate tokens more efficiently.\n",
    "\n",
    "It is sufficient to specify the number of overlapping tokens in the `prompt_lookup_num_tokens` to enable this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569a70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I will be sharing with you a recipe that I've been loving for a while: Slow Roasted Beef Wellington with Caramelized Onions!\n",
      "\n",
      "I'm so excited for this dish because I love the idea behind it, the caramelized onions make\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    prompt_lookup_num_tokens=3,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eaa0a",
   "metadata": {},
   "source": [
    "### Self-Speculative Decoding\n",
    "Self-Speculative Decoding is a method to accelerate inference in LLMs by using different parts of the same model rather instead of relying on another smaller model. It uses early (shallower) layers of the same model as a draft model and deeper layers for verification. During generation, the model selectively skips some intermediate layers to generate draft tokens faster, improving efficiecy without losing output quality.\n",
    "\n",
    "General models like GPT-2, GPT-3, or GPT-J typically do not support self-speculative decoding because they lack architectural support for layer sparsity or early exits. For this section, we will use a llama-based model that supports self-speculative decoding.\n",
    "\n",
    "Passing the `assistant_early_exist` parameter to the `generate()` function will activate self-speculative decoding. This parameter controls how many early layers are used for the draft (speculative) stage during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load .env variables\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "# from huggingface_hub import whoami\n",
    "\n",
    "# user_info = whoami()\n",
    "# print(f\"Logged in as: {user_info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello! Today I am sharing a card I made for the new challenge at the Paper Players. I used the sketch from the challenge and the colors are from my stash. The sentiment is from a stamp set I have had for a long time. It is called \"Thank']\n"
     ]
    }
   ],
   "source": [
    "# Self-Speculative Decoding is not available for GPT-2\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"facebook/layerskip-llama3.2-1B\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.generation_config.pad_token_id = (\n",
    "    model.generation_config.eos_token_id\n",
    ")  # Handle warnings\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/layerskip-llama3.2-1B\", padding_side=\"left\"\n",
    ")\n",
    "model_inputs = tokeniser(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    assistant_early_exit=4,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5db2d",
   "metadata": {},
   "source": [
    "### Universal Assisted Decoding\n",
    "Universal Assisted Decoding (UAD), sometimes called Universal Assisted Generation, is an advanced method designed to speed up language model text generation by using two models (a large target model and a smaller assistant model) even when those models use different tokenisers or come from entirely different model families.\n",
    "\n",
    "After the assistant model generates tokens, these are converted to text and then re-encoded with the target model's tokeniser so the target model can verify them. This allows for flexible speedups of any large model using an arbitrary smaller model from a different family or tokeniser.\n",
    "\n",
    "UAD can be implemented by setting the assistant tokeniser to the parameter `assistant_tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "787cf5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to talk about the new features of the game.\n",
      "\n",
      "The new feature is the ability to play the \"Doom\" mode. This mode is a new way to experience the Doom experience. It's a game mode that allows you to\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2\"\n",
    "DRAFT_MODEL_NAME = \"double7/vicuna-68m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(DRAFT_MODEL_NAME)\n",
    "draft_tokeniser = AutoTokenizer.from_pretrained(DRAFT_MODEL_NAME)\n",
    "draft_model.generation_config.pad_token_id = draft_model.generation_config.eos_token_id\n",
    "\n",
    "input_text = \"Hello! Today I\"\n",
    "\n",
    "model_inputs = tokeniser(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    assistant_model=draft_model,\n",
    "    tokenizer=tokeniser,\n",
    "    assistant_tokenizer=draft_tokeniser,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
