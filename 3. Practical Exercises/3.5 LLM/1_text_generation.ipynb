{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb3b7d3",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25f809ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c49ec7",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "## 2. Device Agnostic Code\n",
    "GPU acceleration delivers significant speed-up over CPU for deep learning tasks, especially for large models and batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9cc87fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    device=\"cuda\"  # GPU\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"  # MPS (MacOS)\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"  # No GPU Available\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8ddb7",
   "metadata": {},
   "source": [
    "## 3. Loading Pre-Trained Model\n",
    "### GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e524fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-large\"\n",
    "MAX_TOKENS = 50\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "tokeniser = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ddfada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "855e8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Hello! Today I\"\n",
    "\n",
    "model_inputs = tokeniser(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2f85467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[15496,     0,  6288,   314]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472f2dcc",
   "metadata": {},
   "source": [
    "## 4. Repetition\n",
    "Phrases being repeated in the generated text is a common phenomenon in augoregressive language models. If a token or phrase has a high probability continuation in training data, the model's predictions tend to loop, generating the same phrase repeatedly. \n",
    "To avoid or reduce repetition, several techniques can be applied:\n",
    "\n",
    "- Employ nucleus sampling or top-k sampling (explained in detail below).\n",
    "- Increase `temperature` above 1.0 to flatten probability distribution, encouraging more random choices.\n",
    "- Use `repetition_penalty` (typically between 1.1 and 1.5) to reduce the probability of tokens that have already been generated.\n",
    "- Apply `no_repeat_ngram_size` to strictly prevent the model from generating the same n-gram repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb9634",
   "metadata": {},
   "source": [
    "## 5. Basic Generation Strategies\n",
    "### Greedy Search\n",
    "Greedy Search is the default setting of decoding strategy used by `.generate()`. At each step, it selects the token with the highest probability as the next token. This method is simple and fast, thus is suitable for generating short text. However, for longer text, it can lead to repetitive and less diverse sequences.\n",
    "\n",
    "By default, greedy search generates up to 20 new tokens unless specified in `GenerationConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1316bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to show you how to make a simple, yet effective, way to get your hands on a few of the most popular and most sought after brands of beer.\n",
      "\n",
      "The first thing you need to do is to find a beer that you\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386670d6",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "Sampling selects a next token randomly based on the probability distribution over the entire vocabulary of the model. This reduces repetition and can generate more creative, diverse outputs compared to the greedy search strategy.\n",
    "\n",
    "Sampling is enabled by setting the parameters: `do_sample=True` and `num_beams=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27fbac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I've got two amazing pieces that aren't pictured that are incredibly simple and easy to make. First, we have a little cute book with a cute little dragon at the front. I have it up on my bookshelf right now and was thinking if\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180c8f6",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "Beam Search maintains multiple candidate sequences (beams) simultaneously. At each step, it expands each beam by selecting tokens, then retains the top $k$ beams based on cumulative (overall) probability score. This strategy is suited for input-grounded tasks such as image captioning or speech recognition.\n",
    "\n",
    "Beam search is enabled by setting `num_beams > 1`, optionally combined with `do_sample = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c64f65d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to show you how to create a simple web app using Angular 2.\n",
      "\n",
      "Angular 2 is a new framework for building web applications. It's fast, easy to learn, and supports a wide variety of technologies. In this tutorial,\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    num_beams=5,\n",
    "    do_sample=True,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aac06c",
   "metadata": {},
   "source": [
    "### Top-k Sampling\n",
    "At each step of generation, the model predicts a probability distribution over the entire vocabulary for the next token, then selects only the top $k$ most probable tokens, ranked by their predicted probability. The probabilities of these top $k$ tokens are renormalised to sum to 1, and the next token is randomly sampled from this restricted set.\n",
    "\n",
    "The number $k$ is configured by the parameter `top_k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d91960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I have a very cool little project I want to talk about. My project is to make an app that will allow you to play around and make your own games. I've already written an application for it. It was pretty simple.\n",
      "\n",
      "I made\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f598d0",
   "metadata": {},
   "source": [
    "### Top-p (Nucleus) Sampling\n",
    "Instead of selecting tokens from the entire vocabulary, nucleus sampling samples from the smallest set of tokens whose cumulative probability exceeds the threshold $p$. This introduces controlled randomness, resulting in more diverse and creative text generation.\n",
    "\n",
    "The probability threshold $p$ is configured by the parameter `top_p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cbc91d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I would like to talk to you about the most important part of your career: the next level in your development. Today we will see about all of this with your new position as a UX Designer at one of the major game studios.\n",
      "\n",
      "We all\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66911c",
   "metadata": {},
   "source": [
    "## 6. Advanced Generation Strategies\n",
    "### Speculative Decoding\n",
    "Speculative decoding uses a second smaller draft model to generate multiple speculative tokens which are then verified by the larget target model to speed up autoregressive decoding. For example, with GPT-2 model:\n",
    "- Draft model: smaller GPT-2 (e.g., `gpt2` or `distilgpt2`)\n",
    "- Target model: `gpt2-large`\n",
    "\n",
    "Speculative decoding can be enabled by setting a draft model to the parameter `assistant_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d2a951f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to cover the most common error you'll encounter in using JavaScript in a web application:\n",
      "\n",
      "You need to use the <script type=\"text/javascript\">…</script> tag as JavaScript is disabled for this page. When you use\n"
     ]
    }
   ],
   "source": [
    "draft_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    assistant_model=draft_model,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184c755",
   "metadata": {},
   "source": [
    "Or, we can use the same parameter in Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9e99133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to put together a quick guide to install and create a new MFC app to test the app with (for Windows Phone users, that is) and for Android users, it will also be helpful to get you up and running with your new\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    tokenizer=tokeniser,\n",
    "    model=model,\n",
    "    assistant_model=draft_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "pipe_output = pipe(\n",
    "    text_inputs=input_text,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    ")\n",
    "print(pipe_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ff03f",
   "metadata": {},
   "source": [
    "### Prompt Lookup Decoding\n",
    "Prompt Lookup Decoding is a variant of speculative decoding that leverages the significant overlap between the input prompt and the generated output. Unlike traditional speculative decoding that uses a smaller draft model to propose next tokens, this method uses substring matching directly on the input prompt to generate candidate tokens more efficiently.\n",
    "\n",
    "It is sufficient to specify the number of overlapping tokens in the `prompt_lookup_num_tokens` to enable this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a70dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Today I'm going to be talking to you about the subject which is my subject, the things that you must never know but that I can guarantee you!\n",
      "\n",
      "Why Do We Need To Fear All Of This?\n",
      " in order to ensure you will never ever\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    prompt_lookup_num_tokens=3,\n",
    "    no_repeat_ngram_size=2,\n",
    "    do_sample=True,\n",
    ")\n",
    "print(tokeniser.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eaa0a",
   "metadata": {},
   "source": [
    "### Self-Speculative Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc07862",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
