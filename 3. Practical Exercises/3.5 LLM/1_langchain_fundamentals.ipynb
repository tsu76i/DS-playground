{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3b32c9",
   "metadata": {},
   "source": [
    "# LangChain Fundamentals\n",
    "***\n",
    "## Table of Contents\n",
    "***\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "## 2. Environmental Variables\n",
    "Firstly, environmental variables need to be configured. These variables, especially API keys should never be hardcoded or made visible to others. The [python-dotenv](https://pypi.org/project/python-dotenv/) libray makes it straightforward to securely access variables set in a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    raise ImportError(\"Error: 'python-dotenv' not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5285339",
   "metadata": {},
   "source": [
    "`os.environ` is a dictionary-like object representing the environment variables of the current process. It allows users to assign new values using the syntax: `os.environ['some_key'] = 'some_value'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"Enter OpenAI API key: \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549a505",
   "metadata": {},
   "source": [
    "## 3. Using Language Model\n",
    "### Loading Model\n",
    "There are multiple approaches to loading language models:\n",
    "1. Use `init_chat_model` from `langchain.chat_models`, with specified model name and provider. \n",
    "2. Specify the model's integration package first, then call the appropriate method to initialise the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b79fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# 1. Simpler call\n",
    "# model = init_chat_model(model=model_name, model_provider=\"openai\", temperature=0.8)\n",
    "\n",
    "# 2. Directly use of the integration package\n",
    "model = ChatOpenAI(model=model_name, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd67751",
   "metadata": {},
   "source": [
    "### Interacting with Language Model\n",
    "For a simple call, we can pass `messages` to the `.invoke` method. The list of message objects (`messages`) can be categorised into three parts:\n",
    "- SystemMessage: Text that guides or determines AI's behaviour or actions.\n",
    "- HumanMessage: Input given by a user.\n",
    "- AIMessage: Output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd53726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following message from English into French\"),\n",
    "    HumanMessage(content=\"Today is a beautiful day\"),\n",
    "]\n",
    "\n",
    "model.invoke(input=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3a4dd",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "A prompt template provides a structured way of creating inputs for language models where parts of the prompt can be dynamically changed based on context or user input.\n",
    "\n",
    "Prompts in LangChain can be split into three components:\n",
    "- System Prompt: Gives instructions or a personality to the LLM model. This prompt determines the behaviour or characteristics of the model.\n",
    "- User Prompt: Input given by a user.\n",
    "- AI Prompt: Output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    template=\"You are an AI translater. Translate text from English to {language}\",\n",
    "    input_variables=[\"language\"],\n",
    ")\n",
    "\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    template=\"\"\"Your task is to translate a text. The text to be translated is:\n",
    "    ---\n",
    "    {text}\n",
    "    ---\n",
    "    Output only the translated text, no other explanation or text should be provided\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"],  # Define a variable\n",
    ")\n",
    "\n",
    "text_in_english = \"\"\"\n",
    "A croissant is a French Viennoiserie in a crescent shape made from a laminated yeast dough that sits between a bread and a puff pastry.\n",
    "\"\"\"\n",
    "\n",
    "target_language = \"French\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872501a",
   "metadata": {},
   "source": [
    "Let's display the formatted user prompt after inserting a value into the `text` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_prompt.format(text=text_in_english))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe6fb3",
   "metadata": {},
   "source": [
    "After defining system and user prompts, we can merge them into a full chat prompt using `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da176b9",
   "metadata": {},
   "source": [
    "ChatPromptTemplate adds a prefix indicating a role of each message (e.g., System:, Human: or AI:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template.format(text=text_in_english, language=target_language))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3289d5",
   "metadata": {},
   "source": [
    "Using **L**ang**C**hain **E**xpression **L**anguage (LCEL), we can construct a chain that links the user input, prompt templates, the model and the output in a sequence:\n",
    "\n",
    "`input | prompt template | model | output`\n",
    "\n",
    "This pipeline enables smooth data flow, where the user input is formatted by the prompt template, passed to the model for inference, and the model's response is captured as the output.\n",
    "\n",
    "Note that lambda expressions are required to access prompt template variables. These variables are stored within input dictionaries or complex objects, thus the lambdas explicity extract or map the necessary fields from these data structures to ensure the correct values are passed to each stage in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"text\": lambda x: x[\"text\"], \"language\": lambda x: x[\"language\"]}\n",
    "    | prompt_template\n",
    "    | model\n",
    "    | {\"translated_text\": lambda x: x.content}\n",
    ")\n",
    "\n",
    "chain.invoke(input={\"text\": text_in_english, \"language\": target_language})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373cc34",
   "metadata": {},
   "source": [
    "### Formatting with Pydantic\n",
    "Using Pydantic, we can enforce a specific format or data structure on the output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15fee6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'org_text': 'A croissant is a French Viennoiserie in a crescent shape made from a laminated yeast dough that sits between a bread and a puff pastry.',\n",
       " 'tl_text': \"Un croissant est une viennoiserie française en forme de croissant faite d'une pâte levée feuilletée qui se situe entre un pain et une pâte feuilletée.\",\n",
       " 'n_words': 25}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Translation(BaseModel):\n",
    "    org_text: str = Field(description=\"Original text\")\n",
    "    tl_text: str = Field(description=\"Translated text\")\n",
    "    n_words: int = Field(description=\"Number of words in the translated text\")\n",
    "\n",
    "\n",
    "structured_model = model.with_structured_output(Translation)\n",
    "\n",
    "second_system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "You are an AI translator. Translate a text from English to {language}.\n",
    "\"\"\",\n",
    "    input_variables=[\"language\"],\n",
    ")\n",
    "\n",
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "    Your task is to translate the following text:\n",
    "    ---\n",
    "    {text}\n",
    "    ---\n",
    "\n",
    "    then count the number of words in the translated text. \n",
    "    Output only the translated text and the word counts, no other explanation or text should be provided.\n",
    "\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "second_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [second_system_prompt, second_user_prompt]\n",
    ")\n",
    "\n",
    "second_chain = (\n",
    "    {\n",
    "        \"text\": lambda x: x[\"text\"],\n",
    "        \"language\": lambda x: x[\"language\"],\n",
    "    }\n",
    "    | second_prompt_template\n",
    "    | structured_model\n",
    "    | {\n",
    "        \"org_text\": lambda x: x.org_text,\n",
    "        \"tl_text\": lambda x: x.tl_text,\n",
    "        \"n_words\": lambda x: x.n_words,\n",
    "    }\n",
    ")\n",
    "second_chain.invoke(input={\"text\": text_in_english, \"language\": target_language})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912f6b1",
   "metadata": {},
   "source": [
    "## 4. LangSmith\n",
    "LangSmith is a comprehensive platform developed by the LangChain team for observability, debugging, testing, and evaluation of large language model (LLM) applications. It helps developers monitor and improve AI-powered apps by capturing detailed traces of interactions, including inputs, outputs, intermediate steps, execution times, and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47472055",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\") or getpass(\n",
    "    \"Enter LangSmith API key: \"\n",
    ")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\") or getpass(\n",
    "    \"Enter project name: \"\n",
    ")\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\") or getpass(\n",
    "    \"Enter LangSmith endpoint: \"\n",
    ")\n",
    "\n",
    "print(f\"Project name: {os.environ['LANGSMITH_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c2e90",
   "metadata": {},
   "source": [
    "### Default Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babe941",
   "metadata": {},
   "source": [
    "If the API keys and parameters were properly configured in the `.env` file, the executions above should have been traced on [LangSmith UI](https://eu.smith.langchain.com/). LangSmith automatically records logs (e.g., inputs, outputs, errors, and execution time) which greatly facilitates the debugging process.\n",
    "\n",
    "![Default Trace](_images/default_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4bf3a",
   "metadata": {},
   "source": [
    "### Non-LangChain Code Tracing\n",
    "By adding the `@traceable` decorator, LangSmith will be able to trace non-LangChain functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b7e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Odd value. No problem.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "import random\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_random_int():\n",
    "    num = random.randint(1, 10)\n",
    "    if num % 2 == 0:\n",
    "        raise ValueError(\"Error: The value has to be odd.\")\n",
    "    else:\n",
    "        return \"Odd value. No problem.\"\n",
    "\n",
    "\n",
    "generate_random_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94358a6b",
   "metadata": {},
   "source": [
    "![Traceable](_images/traceable.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
