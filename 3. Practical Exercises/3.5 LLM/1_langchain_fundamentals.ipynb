{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3b32c9",
   "metadata": {},
   "source": [
    "# LangChain Fundamentals\n",
    "***\n",
    "## Table of Contents\n",
    "***\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "## 2. Environmental Variables\n",
    "Firstly, environmental variables need to be configured. These variables, especially API keys should never be hardcoded or made visible to others. The [python-dotenv](https://pypi.org/project/python-dotenv/) libray makes it straightforward to securely access variables set in a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e8f59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    raise ImportError(\"Error: 'python-dotenv' not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5285339",
   "metadata": {},
   "source": [
    "`os.environ` is a dictionary-like object representing the environment variables of the current process. It allows users to assign new values using the syntax: `os.environ['some_key'] = 'some_value'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be9b7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"Enter OpenAI API key: \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549a505",
   "metadata": {},
   "source": [
    "## 3. Using Language Model\n",
    "### Loading Model\n",
    "There are multiple approaches to loading language models:\n",
    "1. Use `init_chat_model` from `langchain.chat_models`, with specified model name and provider. \n",
    "2. Specify the model's integration package first, then call the appropriate method to initialise the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34b79fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# 1. Simpler call\n",
    "# model = init_chat_model(model=model_name, model_provider=\"openai\", temperature=0.8)\n",
    "\n",
    "# 2. Directly use of the integration package\n",
    "model = ChatOpenAI(model=model_name, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd67751",
   "metadata": {},
   "source": [
    "### Interacting with Language Model\n",
    "For a simple call, we can pass `messages` to the `.invoke` method. The list of message objects (`messages`) can be categorised into three parts:\n",
    "- SystemMessage: Text that guides or determines AI's behaviour or actions.\n",
    "- HumanMessage: Input given by a user.\n",
    "- AIMessage: Output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bd53726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Aujourd'hui est une belle journée.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 24, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-C6wvckRkUZyPJFsdpWO59d37vBXVT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ac8ea802-78be-41c5-8a46-00a85cacb89c-0', usage_metadata={'input_tokens': 24, 'output_tokens': 7, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following message from English into French\"),\n",
    "    HumanMessage(content=\"Today is a beautiful day\"),\n",
    "]\n",
    "\n",
    "model.invoke(input=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3a4dd",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "A prompt template provides a structured way of creating inputs for language models where parts of the prompt can be dynamically changed based on context or user input.\n",
    "\n",
    "Prompts in LangChain can be split into three components:\n",
    "- System Prompt: Gives instructions or a personality to the LLM model. This prompt determines the behaviour or characteristics of the model.\n",
    "- User Prompt: Input given by a user.\n",
    "- AI Prompt: Output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c59e2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    template=\"You are an AI translater. Translate text from English to {language}\",\n",
    "    input_variables=[\"language\"],\n",
    ")\n",
    "\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    template=\"\"\"Your task is to translate a text. The text to be translated is:\n",
    "    ---\n",
    "    {text}\n",
    "    ---\n",
    "    Output only the translated text, no other explanation or text should be provided\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"],  # Define a variable\n",
    ")\n",
    "\n",
    "text_in_english = \"\"\"\n",
    "A croissant is a French Viennoiserie in a crescent shape made from a laminated yeast dough that sits between a bread and a puff pastry.\n",
    "\"\"\"\n",
    "\n",
    "target_language = \"French\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872501a",
   "metadata": {},
   "source": [
    "Let's display the formatted user prompt after inserting a value into the `text` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f083c60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your task is to translate a text. The text to be translated is:\\n    ---\\n    \\nA croissant is a French Viennoiserie in a crescent shape made from a laminated yeast dough that sits between a bread and a puff pastry.\\n\\n    ---\\n    Output only the translated text, no other explanation or text should be provided\\n    ' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt.format(text=text_in_english))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe6fb3",
   "metadata": {},
   "source": [
    "After defining system and user prompts, we can merge them into a full chat prompt using `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d843d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da176b9",
   "metadata": {},
   "source": [
    "ChatPromptTemplate adds a prefix indicating a role of each message (e.g., System:, Human: or AI:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d96e587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI translater. Translate text from English to French\n",
      "Human: Your task is to translate a text. The text to be translated is:\n",
      "    ---\n",
      "    \n",
      "A croissant is a French Viennoiserie in a crescent shape made from a laminated yeast dough that sits between a bread and a puff pastry.\n",
      "\n",
      "    ---\n",
      "    Output only the translated text, no other explanation or text should be provided\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(text=text_in_english, language=target_language))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3289d5",
   "metadata": {},
   "source": [
    "Using **L**ang**C**hain **E**xpression **L**anguage (LCEL), we can construct a chain that links the user input, prompt templates, the model and the output in a sequence:\n",
    "\n",
    "`input | prompt template | model | output`\n",
    "\n",
    "This pipeline enables smooth data flow, where the user input is formatted by the prompt template, passed to the model for inference, and the model's response is captured as the output.\n",
    "\n",
    "Note that lambda expressions are required to access prompt template variables. These variables are stored within input dictionaries or complex objects, thus the lambdas explicity extract or map the necessary fields from these data structures to ensure the correct values are passed to each stage in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d56e224e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translated_text': \"Un croissant est une viennoiserie française en forme de croissant, réalisée à partir d'une pâte à base de levure laminée qui se situe entre une pâte à pain et une pâte feuilletée.\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"text\": lambda x: x[\"text\"], \"language\": lambda x: x[\"language\"]}\n",
    "    | prompt_template\n",
    "    | model\n",
    "    | {\"translated_text\": lambda x: x.content}\n",
    ")\n",
    "\n",
    "chain.invoke({\"text\": text_in_english, \"language\": target_language})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
