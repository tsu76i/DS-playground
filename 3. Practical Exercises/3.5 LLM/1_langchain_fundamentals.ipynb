{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3b32c9",
   "metadata": {},
   "source": [
    "# LangChain Fundamentals\n",
    "***\n",
    "## Table of Contents\n",
    "***\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "## 2. Environmental Variables\n",
    "Firstly, environmental variables need to be configured. These variables, especially API keys should never be hardcoded or made visible to others. The [python-dotenv](https://pypi.org/project/python-dotenv/) libray makes it straightforward to securely access variables set in a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8f59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    raise ImportError(\"Error: 'python-dotenv' not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5285339",
   "metadata": {},
   "source": [
    "`os.environ` is a dictionary-like object representing the environment variables of the current process. It allows users to assign new values using the syntax: `os.environ['some_key'] = 'some_value'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9b7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"Enter OpenAI API key: \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549a505",
   "metadata": {},
   "source": [
    "## 3. Using Language Model\n",
    "### Loading Model\n",
    "There are multiple approaches to loading language models:\n",
    "1. Use `init_chat_model` from `langchain.chat_models`, with specified model name and provider. \n",
    "2. Specify the model's integration package first, then call the appropriate method to initialise the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b79fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# 1. Simpler call\n",
    "# model = init_chat_model(model=model_name, model_provider=\"openai\", temperature=0.8)\n",
    "\n",
    "# 2. Directly use of the integration package\n",
    "model = ChatOpenAI(model=model_name, temperature=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd67751",
   "metadata": {},
   "source": [
    "### Interacting with Language Model\n",
    "For a simple call, we can pass `messages` to the `.invoke` method. The list of message objects (`messages`) can be categorised into three parts:\n",
    "- SystemMessage: Text that guides or determines AI's behaviour or actions.\n",
    "- HumanMessage: Input given by a user.\n",
    "- AIMessage: Output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd53726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Aujourd'hui est une belle journée.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 24, 'total_tokens': 31, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C722fk4QSwtvBPFi1fP2m9ArtlY0C', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--46d2394e-d99c-4e57-a042-de2c68d9b1b9-0', usage_metadata={'input_tokens': 24, 'output_tokens': 7, 'total_tokens': 31, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following message from English into French\"),\n",
    "    HumanMessage(content=\"Today is a beautiful day\"),\n",
    "]\n",
    "\n",
    "model.invoke(input=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3a4dd",
   "metadata": {},
   "source": [
    "### Prompt Template\n",
    "A prompt template provides a structured way of creating inputs for language models where parts of the prompt can be dynamically changed based on context or user input.\n",
    "\n",
    "Prompts in LangChain can be split into three components:\n",
    "- System Prompt: Gives instructions or a personality to the LLM model. This prompt determines the behaviour or characteristics of the model.\n",
    "- User Prompt: Input given by a user.\n",
    "- AI Prompt: Output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59e2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    template=\"You are an AI translater. Translate text from English to {language}\",\n",
    "    input_variables=[\"language\"],\n",
    ")\n",
    "\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    template=\"\"\"Your task is to translate a text. The text to be translated is:\n",
    "    ---\n",
    "    {text}\n",
    "    ---\n",
    "    Output only the translated text, no other explanation or text should be provided\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"],  # Define a variable\n",
    ")\n",
    "\n",
    "text_in_english = \"\"\"\n",
    "A croissant is a French Viennoiserie in a crescent shape made from a laminated yeast dough that sits between a bread and a puff pastry.\n",
    "\"\"\"\n",
    "\n",
    "target_language = \"French\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872501a",
   "metadata": {},
   "source": [
    "Let's display the formatted user prompt after inserting a value into the `text` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f083c60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Your task is to translate a text. The text to be translated is:\\n    ---\\n    \\nA croissant is a French Viennoiserie in a crescent shape made from a laminated yeast dough that sits between a bread and a puff pastry.\\n\\n    ---\\n    Output only the translated text, no other explanation or text should be provided\\n    ' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt.format(text=text_in_english))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe6fb3",
   "metadata": {},
   "source": [
    "After defining system and user prompts, we can merge them into a full chat prompt using `ChatPromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d843d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da176b9",
   "metadata": {},
   "source": [
    "ChatPromptTemplate adds a prefix indicating a role of each message (e.g., System:, Human: or AI:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d96e587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an AI translater. Translate text from English to French\n",
      "Human: Your task is to translate a text. The text to be translated is:\n",
      "    ---\n",
      "    \n",
      "A croissant is a French Viennoiserie in a crescent shape made from a laminated yeast dough that sits between a bread and a puff pastry.\n",
      "\n",
      "    ---\n",
      "    Output only the translated text, no other explanation or text should be provided\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(text=text_in_english, language=target_language))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3289d5",
   "metadata": {},
   "source": [
    "Using **L**ang**C**hain **E**xpression **L**anguage (LCEL), we can construct a chain that links the user input, prompt templates, the model and the output in a sequence:\n",
    "\n",
    "`input | prompt template | model | output`\n",
    "\n",
    "This pipeline enables smooth data flow, where the user input is formatted by the prompt template, passed to the model for inference, and the model's response is captured as the output.\n",
    "\n",
    "Note that lambda expressions are required to access prompt template variables. These variables are stored within input dictionaries or complex objects, thus the lambdas explicity extract or map the necessary fields from these data structures to ensure the correct values are passed to each stage in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56e224e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translated_text': \"Un croissant est une viennoiserie française en forme de croissant, faite à partir d'une pâte levée feuilletée qui se situe entre une pâte à pain et une pâte feuilletée.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"text\": lambda x: x[\"text\"], \"language\": lambda x: x[\"language\"]}\n",
    "    | prompt_template\n",
    "    | model\n",
    "    | {\"translated_text\": lambda x: x.content}\n",
    ")\n",
    "\n",
    "chain.invoke(input={\"text\": text_in_english, \"language\": target_language})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373cc34",
   "metadata": {},
   "source": [
    "### Formatting with Pydantic\n",
    "Using Pydantic, we can enforce a specific format or data structure on the output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15fee6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'org_text': 'A croissant is a French Viennoiserie in a crescent shape made from a laminated yeast dough that sits between a bread and a puff pastry.',\n",
       " 'tl_text': \"Un croissant est une viennoiserie française en forme de croissant faite d'une pâte levée feuilletée qui se situe entre un pain et une pâte feuilletée.\",\n",
       " 'n_words': 27}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Translation(BaseModel):\n",
    "    org_text: str = Field(description=\"Original text\")\n",
    "    tl_text: str = Field(description=\"Translated text\")\n",
    "    n_words: int = Field(description=\"Number of words in the translated text\")\n",
    "\n",
    "\n",
    "structured_model = model.with_structured_output(Translation)\n",
    "\n",
    "second_system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "You are an AI translator. Translate a text from English to {language}.\n",
    "\"\"\",\n",
    "    input_variables=[\"language\"],\n",
    ")\n",
    "\n",
    "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    template=\"\"\"\n",
    "    Your task is to translate the following text:\n",
    "    ---\n",
    "    {text}\n",
    "    ---\n",
    "\n",
    "    then count the number of words in the translated text. \n",
    "    Output only the translated text and the word counts, no other explanation or text should be provided.\n",
    "\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "second_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [second_system_prompt, second_user_prompt]\n",
    ")\n",
    "\n",
    "second_chain = (\n",
    "    {\n",
    "        \"text\": lambda x: x[\"text\"],\n",
    "        \"language\": lambda x: x[\"language\"],\n",
    "    }\n",
    "    | second_prompt_template\n",
    "    | structured_model\n",
    "    | {\n",
    "        \"org_text\": lambda x: x.org_text,\n",
    "        \"tl_text\": lambda x: x.tl_text,\n",
    "        \"n_words\": lambda x: x.n_words,\n",
    "    }\n",
    ")\n",
    "second_chain.invoke(input={\"text\": text_in_english, \"language\": target_language})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5912f6b1",
   "metadata": {},
   "source": [
    "## 4. LangSmith\n",
    "LangSmith is a comprehensive platform developed by the LangChain team for observability, debugging, testing, and evaluation of large language model (LLM) applications. It helps developers monitor and improve AI-powered apps by capturing detailed traces of interactions, including inputs, outputs, intermediate steps, execution times, and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47472055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name: lc_fundamentals\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\") or getpass(\n",
    "    \"Enter LangSmith API key: \"\n",
    ")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\") or getpass(\n",
    "    \"Enter project name: \"\n",
    ")\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\") or getpass(\n",
    "    \"Enter LangSmith endpoint: \"\n",
    ")\n",
    "\n",
    "print(f\"Project name: {os.environ['LANGSMITH_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c2e90",
   "metadata": {},
   "source": [
    "### Default Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babe941",
   "metadata": {},
   "source": [
    "If the API keys and parameters were properly configured in the `.env` file, the executions above should have been traced on [LangSmith UI](https://eu.smith.langchain.com/). LangSmith automatically records logs (e.g., inputs, outputs, errors, and execution time) which greatly facilitates the debugging process.\n",
    "\n",
    "![Default Trace](_images/default_trace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4bf3a",
   "metadata": {},
   "source": [
    "### Non-LangChain Code Tracing\n",
    "By adding the `@traceable` decorator, LangSmith will be able to trace non-LangChain functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d56b7e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Odd value. No problem.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "import random\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_random_int():\n",
    "    num = random.randint(1, 10)\n",
    "    if num % 2 == 0:\n",
    "        raise ValueError(\"Error: The value has to be odd.\")\n",
    "    else:\n",
    "        return \"Odd value. No problem.\"\n",
    "\n",
    "\n",
    "generate_random_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94358a6b",
   "metadata": {},
   "source": [
    "![Traceable](_images/traceable.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc6fc5",
   "metadata": {},
   "source": [
    "## 5. Sequential Chain\n",
    "A Sequential Chain is a chain that executes multiple steps (or sub-chains) in a defined order, where the output of one step becomes the input to the next. This allows us to build complex workflows by composing simpler chains sequentially. In the following example, the first chain takes the input `ingredient` and generates a `dish` that uses the ingredient. Then, the second chain takes the `dish` name as input and writes a `description` of the dish, returning the description as the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b05725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'ingredient': 'tomato', 'dish': 'Caprese Salad', 'description': 'Caprese salad is a classic Italian dish that showcases the vibrant flavors of fresh ingredients. It typically consists of ripe tomatoes, creamy mozzarella cheese, and fragrant basil leaves, all drizzled with olive oil and balsamic vinegar. This refreshing salad is not only visually appealing but also embodies the essence of summer with its simple yet delicious combination of tastes.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chain_1 = LLMChain(\n",
    "    llm=model,\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"ingredient\"],\n",
    "        template=\"Generate a dish that uses {ingredient}. Output only the dish's name.\",\n",
    "    ),\n",
    "    output_key=\"dish\",\n",
    ")\n",
    "\n",
    "chain_2 = LLMChain(\n",
    "    llm=model,\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"dish\"],\n",
    "        template=\"Write a simple description of {dish} in 2 - 3 sentences.\",\n",
    "    ),\n",
    "    output_key=\"description\",\n",
    ")\n",
    "\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[chain_1, chain_2],\n",
    "    input_variables=[\"ingredient\"],\n",
    "    output_variables=[\"dish\", \"description\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "result = sequential_chain.invoke({\"ingredient\": \"tomato\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d1447",
   "metadata": {},
   "source": [
    "## 6. Semantic Search\n",
    "### Document Class\n",
    "To handle external data (e.g., websites, PDF files), the Document class stores a piece of text along with its associated metadata. The following are the three attributes of the Document class:\n",
    "\n",
    "- `page_content`: Content in string.\n",
    "- `metadata`: Dictionary including metadata.\n",
    "- `id` (optional): Identifier in string for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1f85172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "doc = Document(\n",
    "    page_content=\"Page content in string format.\",\n",
    "    metadata={\"source\": \"https://example.com\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce089ae5",
   "metadata": {},
   "source": [
    "`PyPDFLoader` facilitates loading a single Document object per PDF (the `pypdf` package is required), and the `.load()` method returns a list containing the page content and metadata from the PDF file. To access the data, it is necessary to use the index `[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f740c196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"_datasets/nke-10k-2023.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb158941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAGE CONTENT:\n",
      "********************\n",
      "Table of Contents\n",
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K\n",
      "\n",
      "\n",
      "META DATA:\n",
      "********************\n",
      "{'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': '_datasets/nke-10k-2023.pdf', 'total_pages': 107, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"PAGE CONTENT:\\n{'*' * 20}\\n{docs[0].page_content[:100]}\\n\")\n",
    "print(f\"META DATA:\\n{'*' * 20}\\n{docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8607778",
   "metadata": {},
   "source": [
    "### Splitters\n",
    "Splitters are utility classes in LangChain designed to divide large bodies of text or document objects into smaller chunks. The most common example is the `TextSplitter`, such as the `ResursiveCharacterTextSplitter` which splits documents based on character count, overlap, or other configurable criteria.\n",
    "\n",
    "Text splitters ensure that content is broken down into appropriate sized chunks that do not exceed the maximum token limit of LLMs, thereby generating more precise embeddings. This approach also facilitates parallel (batch) processing of document chunks. By setting the `chunk_overlap` parameter, splitters preserve context across adjacent chunks, reducing the loss of important information and improving retrieval accuracy in retrieval-augmented generation (RAG) systems.\n",
    "\n",
    "The following example splits the loaded document into chunks of up to 1000 characters with 200 characters of overlap between chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cb7cd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 516\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Total number of chunks: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e47f0ab",
   "metadata": {},
   "source": [
    "Here are the details of the parameters:\n",
    "\n",
    "- `chunk_size`: Determines the **maximum number** of characters each chunk can contain.\n",
    "- `chunk_overlap`: Specifies the number of characters to overlap at the end of a chunk, which will be repeated at the start of the next chunk. This helps to preserve context at the boundaries between chunks.\n",
    "- `add_start_index`: When set to `True`, each split `Document` will include metadata indicating the starting character index of that chunk within the original document.\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` prioritises splitting at natural boundaries (e.g., paragraphs, sentences, or newline characters) rather than splitting strictly at the chunk size. This segmentation avoids cutting text in the middle of words or sentences; therefore, setting the `chunk_size` parameter to 1000 can result in chunks containing fewer than 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d95ed9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the 1st chunk: 972\n",
      "\n",
      "Table of Contents\n",
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K\n",
      "(Mark One)\n",
      "☑  ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "FOR THE FISCAL YEAR ENDED MAY 31, 2023\n",
      "OR\n",
      "☐  TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "FOR THE TRANSITION PERIOD FROM                         TO                         .\n",
      "Commission File No. 1-10635\n",
      "NIKE, Inc.\n",
      "(Exact name of Registrant as specified in its charter)\n",
      "Oregon 93-0584541\n",
      "(State or other jurisdiction of incorporation) (IRS Employer Identification No.)\n",
      "One Bowerman Drive, Beaverton, Oregon 97005-6453\n",
      "(Address of principal executive offices and zip code)\n",
      "(503) 671-6453\n",
      "(Registrant's telephone number, including area code)\n",
      "SECURITIES REGISTERED PURSUANT TO SECTION 12(B) OF THE ACT:\n",
      "Class B Common Stock NKE New York Stock Exchange\n",
      "(Title of each class) (Trading symbol) (Name of each exchange on which registered)\n",
      "\n",
      "{'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': '_datasets/nke-10k-2023.pdf', 'total_pages': 107, 'page': 0, 'page_label': '1', 'start_index': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters in the 1st chunk: {len(all_splits[0].page_content)}\\n\")\n",
    "print(f\"{all_splits[0].page_content}\\n\")\n",
    "print(f\"{all_splits[0].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9d20288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the 2nd chunk: 975\n",
      "\n",
      "SECURITIES REGISTERED PURSUANT TO SECTION 12(B) OF THE ACT:\n",
      "Class B Common Stock NKE New York Stock Exchange\n",
      "(Title of each class) (Trading symbol) (Name of each exchange on which registered)\n",
      "SECURITIES REGISTERED PURSUANT TO SECTION 12(G) OF THE ACT:\n",
      "NONE\n",
      "Indicate by check mark: YES NO\n",
      "• if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act. þ ¨ \n",
      "• if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Act. ¨ þ \n",
      "• whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 during the preceding\n",
      "12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the\n",
      "past 90 days.\n",
      "þ ¨ \n",
      "• whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T\n",
      "\n",
      "{'producer': 'EDGRpdf Service w/ EO.Pdf 22.0.40.0', 'creator': 'EDGAR Filing HTML Converter', 'creationdate': '2023-07-20T16:22:00-04:00', 'title': '0000320187-23-000039', 'author': 'EDGAR Online, a division of Donnelley Financial Solutions', 'subject': 'Form 10-K filed on 2023-07-20 for the period ending 2023-05-31', 'keywords': '0000320187-23-000039; ; 10-K', 'moddate': '2023-07-20T16:22:08-04:00', 'source': '_datasets/nke-10k-2023.pdf', 'total_pages': 107, 'page': 0, 'page_label': '1', 'start_index': 781}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters in the 2nd chunk: {len(all_splits[1].page_content)}\\n\")\n",
    "print(f\"{all_splits[1].page_content}\\n\")\n",
    "print(f\"{all_splits[1].metadata}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
