{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee820391",
   "metadata": {},
   "source": [
    "# Chat Bot\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9d4f9",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "## 2. Environmental Variables\n",
    "The API keys and environmental variables should never be hardcoded or exposed publicly. The [python-dotenv](https://pypi.org/project/python-dotenv/) library facilitates secure access to variables defined in a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6113549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name: lc_fundamentals\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    raise ImportError(\"Error: 'python-dotenv not installed\")\n",
    "\n",
    "print(f\"Project name: {os.environ['LANGSMITH_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85758294",
   "metadata": {},
   "source": [
    "## 3. Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cf7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "provider = \"google_genai\"\n",
    "model = init_chat_model(model=model_name, model_provider=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da6131",
   "metadata": {},
   "source": [
    "## 4. Conversational Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2819e62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello John! Nice to meet you. I'm an AI, a large language model.\\n\\nHow can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--f21f8fcd-46b3-4a2e-adcb-593ced29cecd-0', usage_metadata={'input_tokens': 7, 'output_tokens': 562, 'total_tokens': 569, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 535}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(input=\"Hi, I am John.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d03b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have access to personal information about you, including your name. I don't retain memory of past conversations or user identities.\\n\\nIf you'd like me to know your name for our current chat, please feel free to tell me!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--9a1d7d10-69ae-493d-9a2a-5e3aca9dc9c6-0', usage_metadata={'input_tokens': 6, 'output_tokens': 446, 'total_tokens': 452, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 390}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adf25a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is John.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--efc4124f-39bc-4f93-9570-61fb41536698-0', usage_metadata={'input_tokens': 24, 'output_tokens': 42, 'total_tokens': 66, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 37}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, I am John.\"),\n",
    "        AIMessage(content=\"Hello John, how can I help you today?\"),\n",
    "        HumanMessage(content=\"What is my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc120d3",
   "metadata": {},
   "source": [
    "### Traditional Methods\n",
    "\n",
    "In previous versions of LangChain (prior to 0.3), memory classes were used to store and manage conversation history internally within each chain or memory object. The following classes are **deprecated** as of August 2025 (version $\\geq$ 0.3), but I include them here for learning purposes:\n",
    "\n",
    "- **ConversationBufferMemory**: Stores the entire conversation history. This is the simplest method for managing memory.\n",
    "- **ConversationBufferWindowMemory**: Retains only the last $k$ messages in the conversation.\n",
    "- **ConversationSummaryMemory**: Rather than storing the entire history, this class generates and retains a summary of the conversation.\n",
    "- **ConversationSummaryBufferMemory**: Summarises each exchange and retains only the most recent $k$ summaries.\n",
    "\n",
    "These classes were later used in combination with the `RunnableWithMessageHistory` class, which wraps both the chain and the memory implementation. As of LangChain version 0.3, although `RunnableWithMessageHistory` is not explicitly deprecated, it is strongly recommended to migrate to LangGraph persistence, which offers a more robust and scalable approach to managing message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c74364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/71/96lm97d97jd1rltw9bhztxc80000gn/T/ipykernel_58507/1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a079a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I am John.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John, how can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Who won the Champions League in 2024?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='In 2024, Manchester City won the UEFA Champions League.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.add_user_message(message=\"Hi, I am John.\")\n",
    "memory.chat_memory.add_ai_message(message=\"Hello John, how can I assist you today?\")\n",
    "memory.chat_memory.add_user_message(message=\"Who won the Champions League in 2024?\")\n",
    "memory.chat_memory.add_ai_message(\n",
    "    message=\"In 2024, Manchester City won the UEFA Champions League.\"\n",
    ")\n",
    "\n",
    "memory.load_memory_variables(\n",
    "    {}\n",
    ")  # Need to load variables for memory type - none in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ef163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/71/96lm97d97jd1rltw9bhztxc80000gn/T/ipykernel_58507/4191846825.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(llm=model, memory=memory, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, I am John.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello John, how can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Who won the Champions League in 2024?', additional_kwargs={}, response_metadata={}), AIMessage(content='In 2024, Manchester City won the UEFA Champions League.', additional_kwargs={}, response_metadata={})]\n",
      "Human: Remind me what we spoke about earlier.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Remind me what we spoke about earlier.',\n",
       " 'history': [HumanMessage(content='Hi, I am John.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John, how can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Who won the Champions League in 2024?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='In 2024, Manchester City won the UEFA Champions League.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Remind me what we spoke about earlier.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Certainly, John! We've had a lovely chat so far.\\n\\nJust a moment ago, you asked me about the winner of the **Champions League in 2024**. And I informed you that **Manchester City** were the champions, lifting that prestigious UEFA Champions League trophy!\\n\\nBefore that, you introduced yourself to me as **John**, and I, of course, greeted you and asked how I could assist you today.\\n\\nSo, in a nutshell, we've covered your introduction and the 2024 Champions League winner! Is there anything else you'd like to discuss or perhaps another detail from our conversation you'd like me to elaborate on?\", additional_kwargs={}, response_metadata={})],\n",
       " 'response': \"Certainly, John! We've had a lovely chat so far.\\n\\nJust a moment ago, you asked me about the winner of the **Champions League in 2024**. And I informed you that **Manchester City** were the champions, lifting that prestigious UEFA Champions League trophy!\\n\\nBefore that, you introduced yourself to me as **John**, and I, of course, greeted you and asked how I could assist you today.\\n\\nSo, in a nutshell, we've covered your introduction and the 2024 Champions League winner! Is there anything else you'd like to discuss or perhaps another detail from our conversation you'd like me to elaborate on?\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(llm=model, memory=memory, verbose=True)\n",
    "\n",
    "chain.invoke({\"input\": \"Remind me what we spoke about earlier.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f4f92",
   "metadata": {},
   "source": [
    "### Modern Methods\n",
    "\n",
    "\n",
    "- **MemorySaver**: An in-memory checkpoint class for saving the state (chat messages) during execution. It's suitable for prototypes; for production, it should be swapped with persistant backends (e.g., SQLite, Postgres).\n",
    "- **StateGraph + MessagesState**: LangGraph's state machine and schema for storing the message state. `START` is a special entry node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "534f3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.base import BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7084813c",
   "metadata": {},
   "source": [
    "StateGraph is a core graph structure in LangGraph. It has to be initialised with the schema that describes the states to keep (in this case, conversation messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90cab600",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca7611",
   "metadata": {},
   "source": [
    "The following functions accepts the state (including the message history). It passes the messages to the model, receives the response, and returns the new state including updates messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4f6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState) -> dict[str, BaseMessage]:\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee483a8c",
   "metadata": {},
   "source": [
    "- `add_edge`: Defines the flow; after `START`, the workflow progresses to the `model` node.\n",
    "- `add_node`: Registers the `model` node and links it to the `call_model()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88c1457f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x10ba21160>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(start_key=START, end_key=\"model\")\n",
    "workflow.add_node(node=\"model\", action=call_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865583cd",
   "metadata": {},
   "source": [
    "After setting edges and nodes, we add memory and compile the app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6be84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0b71d",
   "metadata": {},
   "source": [
    "The `config` dictionary can contain a `configurable` key, which holds runtime parameters for LangGraph's execution. The `thread_id` value within this key identifies each conversation. If multiple usrs are chatting, each one can be assigned a unique `thread_id` (typically their user ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7cedefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"user_id_123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adac0e",
   "metadata": {},
   "source": [
    "Finally, the `app.invoke()` runs the LangGraph workflow using the given initial state. The `config` parameter ensures the conversation updates and is checkpointed under the specified thread ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3789d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello John! Nice to meet you. I'm an AI assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi, I am John.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(input={\"messages\": input_messages}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5a73d",
   "metadata": {},
   "source": [
    "## 5. Prompt Templates\n",
    "A prompt template provides a structured way of creating inputs for language models where parts of the prompt can be dynamically changed based on context or user input.\n",
    "\n",
    "Prompts in LangChain can be split into three components:\n",
    "- **System Prompt**: Gives instructions or a personality to the LLM model. This prompt determines the behaviour or characteristics of the model.\n",
    "- **User Prompt**: Input given by a user.\n",
    "- **AI Prompt**: Output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a61fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            template=\"You are an AI receptionist at a five-star hotel. Welcome and helps guests to the best of your ability in {language}.\",\n",
    "            input_variables=[\"language\"],\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66224794",
   "metadata": {},
   "source": [
    "Earlier, `MessageState` contained only a single field (`messages`). To store multiple state variables, such as language, ID, or any custom matadata, we need to define a new `State` class explicitly with all required fields. It allows the graph to manage, validate, and pass around all relevant state components clearly while reducing bugs by enabling type checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74e97a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Bonjour et bienvenue à notre hôtel. Je suis votre réceptionniste AI.\n",
      "\n",
      "Je serais ravie de vous aider à faire une réservation. Pourriez-vous me dire, s'il vous plaît :\n",
      "\n",
      "1.  **Quelles sont les dates** d'arrivée et de départ souhaitées ?\n",
      "2.  **Combien de personnes** séjourneront (adultes et enfants, si applicable) ?\n",
      "3.  **Quel type de chambre** vous intéresserait (par exemple, chambre standard, suite, chambre avec vue sur la mer) ?\n",
      "\n",
      "Une fois que j'aurai ces informations, je pourrai vérifier la disponibilité et les options pour vous.\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):  # Custom State class\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)  # Pass it to StateGraph\n",
    "\n",
    "\n",
    "def call_model(state: State) -> dict[str, BaseMessage]:\n",
    "    prompt = prompt_template.invoke(input=state)\n",
    "    response = model.invoke(input=prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(start_key=START, end_key=\"model\")\n",
    "workflow.add_node(node=\"model\", action=call_model)\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"A11111\"}}\n",
    "query = \"I want to make a reservation\"\n",
    "language = \"French\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    input={\"messages\": input_messages, \"language\": language}, config=config\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
