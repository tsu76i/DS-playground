{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee820391",
   "metadata": {},
   "source": [
    "# Chat Bot\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "1. [Environment Variables](#2-environment-variables)\n",
    "1. [Loading Model](#3-loading-model)\n",
    "1. [Conversational Memory](#4-conversational-memory)\n",
    "    - [Traditioanl Methods](#traditional-methods)\n",
    "    - [Modern Methods](#modern-methods)\n",
    "1. [Prompt Templates](#5-prompt-templates)\n",
    "1. [Trimming](#6-trimming)\n",
    "1. [Streaming](#7-streaming)\n",
    "1. [References](#8-references)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c9d4f9",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "The objective of this exercise is to build a simple chatbot with memory using LangChain and LangGraph. This tutorial is primarily based on the official LangChain documentation.\n",
    "\n",
    "## 2. Environment Variables\n",
    "The API keys and environment variables should never be hardcoded or exposed publicly. The [python-dotenv](https://pypi.org/project/python-dotenv/) library facilitates secure access to variables defined in a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6113549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project name: lc_fundamentals\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    raise ImportError(\"Error: 'python-dotenv not installed\")\n",
    "\n",
    "print(f\"Project name: {os.environ['LANGSMITH_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85758294",
   "metadata": {},
   "source": [
    "## 3. Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cf7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "provider = \"openai\"\n",
    "model = init_chat_model(model=model_name, model_provider=provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da6131",
   "metadata": {},
   "source": [
    "## 4. Conversational Memory\n",
    "It is extremely important for chatbots to retain conversation history. By default, chat models do not remember any information from previous interactions. This means that every time the model is called using the .invoke() method, it generates a new response without considering the prior conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2819e62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi John! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 13, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8Bv6RWwBHG4ZyNxeZX5CzvMnvrHs', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4fb4cad8-fbc2-4b79-ad11-2340f21450e9-0', usage_metadata={'input_tokens': 13, 'output_tokens': 10, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(input=\"Hi, I am John.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d03b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I don't have access to your name or personal information. If you let me know your name or how you'd like to be addressed, I'd be happy to use it!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 12, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8Bv6sFw9HR3iK66TRuwT7vQfzXyj', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--69379621-d9ef-44a5-9502-0f89ddf208ba-0', usage_metadata={'input_tokens': 12, 'output_tokens': 37, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adf25a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is John. How can I assist you further?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 36, 'total_tokens': 48, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8Bv8u95HUwxwJyCbLiXAnjFMaafs', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--06d5a94f-e477-4a18-8374-fe2d5fbe2b2a-0', usage_metadata={'input_tokens': 36, 'output_tokens': 12, 'total_tokens': 48, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, I am John.\"),\n",
    "        AIMessage(content=\"Hello John, how can I help you today?\"),\n",
    "        HumanMessage(content=\"What is my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc120d3",
   "metadata": {},
   "source": [
    "### Traditional Methods\n",
    "\n",
    "In previous versions of LangChain (prior to 0.3), memory classes were used to store and manage conversation history internally within each chain or memory object. The following classes are **deprecated** as of August 2025 (version $\\geq$ 0.3), but I include them here for learning purposes:\n",
    "\n",
    "- **ConversationBufferMemory**: Stores the entire conversation history. This is the simplest method for managing memory.\n",
    "- **ConversationBufferWindowMemory**: Retains only the last $k$ messages in the conversation.\n",
    "- **ConversationSummaryMemory**: Rather than storing the entire history, this class generates and retains a summary of the conversation.\n",
    "- **ConversationSummaryBufferMemory**: Summarises each exchange and retains only the most recent $k$ summaries.\n",
    "\n",
    "These classes were later used in combination with the `RunnableWithMessageHistory` class, which wraps both the chain and the memory implementation. As of LangChain version 0.3, although `RunnableWithMessageHistory` is not explicitly deprecated, it is strongly recommended to migrate to LangGraph persistence, which offers a more robust and scalable approach to managing message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c74364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/71/96lm97d97jd1rltw9bhztxc80000gn/T/ipykernel_97020/1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a079a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, I am John.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John, how can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Who won the Champions League in 2024?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='In 2024, Manchester City won the UEFA Champions League.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.add_user_message(message=\"Hi, I am John.\")\n",
    "memory.chat_memory.add_ai_message(message=\"Hello John, how can I assist you today?\")\n",
    "memory.chat_memory.add_user_message(message=\"Who won the Champions League in 2024?\")\n",
    "memory.chat_memory.add_ai_message(\n",
    "    message=\"In 2024, Manchester City won the UEFA Champions League.\"\n",
    ")\n",
    "\n",
    "memory.load_memory_variables(\n",
    "    {}\n",
    ")  # Need to load variables for memory type - none in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ef163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/71/96lm97d97jd1rltw9bhztxc80000gn/T/ipykernel_97020/4191846825.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(llm=model, memory=memory, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, I am John.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello John, how can I assist you today?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Who won the Champions League in 2024?', additional_kwargs={}, response_metadata={}), AIMessage(content='In 2024, Manchester City won the UEFA Champions League.', additional_kwargs={}, response_metadata={})]\n",
      "Human: Remind me what we spoke about earlier.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Remind me what we spoke about earlier.',\n",
       " 'history': [HumanMessage(content='Hi, I am John.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John, how can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Who won the Champions League in 2024?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='In 2024, Manchester City won the UEFA Champions League.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Remind me what we spoke about earlier.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Earlier, we talked about your name, John, and I mentioned that Manchester City won the UEFA Champions League in 2024. How can I assist you further?', additional_kwargs={}, response_metadata={})],\n",
       " 'response': 'Earlier, we talked about your name, John, and I mentioned that Manchester City won the UEFA Champions League in 2024. How can I assist you further?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(llm=model, memory=memory, verbose=True)\n",
    "\n",
    "chain.invoke({\"input\": \"Remind me what we spoke about earlier.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f4f92",
   "metadata": {},
   "source": [
    "### Modern Methods\n",
    "The official LangChain documentation strongly recommends using LangGraph persistence instead of the deprecated memory classes mentioned earlier.\n",
    "\n",
    "- **MemorySaver**: An in-memory checkpoint class that saves the conversation state (chat messages) during execution. It is suitable for prototypes but should be replaced by persistent backends, such as SQLite or PostgreSQL, in production environments.\n",
    "\n",
    "- **StateGraph + MessagesState**: LangGraph's state machine and schema for managing the message state. `START` is the graph's special entry node.\n",
    "\n",
    "This approach ensures robust and scalable conversation history management, enabling multi-turn dialogues that maintain context effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "534f3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.base import BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7084813c",
   "metadata": {},
   "source": [
    "StateGraph is a core graph structure in LangGraph. It has to be initialised with the schema that describes the states to keep (in this case, conversation messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90cab600",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca7611",
   "metadata": {},
   "source": [
    "The following functions accepts the state (including the message history). It passes the messages to the model, receives the response, and returns the new state including updates messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4f6e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState) -> dict[str, BaseMessage]:\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee483a8c",
   "metadata": {},
   "source": [
    "- `add_edge`: Defines the flow; after `START`, the workflow progresses to the `model` node.\n",
    "- `add_node`: Registers the `model` node and links it to the `call_model()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88c1457f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12a39b8c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(start_key=START, end_key=\"model\")\n",
    "workflow.add_node(node=\"model\", action=call_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865583cd",
   "metadata": {},
   "source": [
    "After setting edges and nodes, we add memory and compile the app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6be84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0b71d",
   "metadata": {},
   "source": [
    "The `config` dictionary can contain a `configurable` key, which holds runtime parameters for LangGraph's execution. The `thread_id` value within this key identifies each conversation. If multiple usrs are chatting, each one can be assigned a unique `thread_id` (typically their user ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7cedefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"user_id_123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adac0e",
   "metadata": {},
   "source": [
    "Finally, the `app.invoke()` runs the LangGraph workflow using the given initial state. The `config` parameter ensures the conversation updates and is checkpointed under the specified thread ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3789d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi John! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi, I am John.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(input={\"messages\": input_messages}, config=config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5a73d",
   "metadata": {},
   "source": [
    "## 5. Prompt Templates\n",
    "A prompt template provides a structured way of creating inputs for language models where parts of the prompt can be dynamically changed based on context or user input.\n",
    "\n",
    "Prompts in LangChain can be split into three components:\n",
    "- **System Prompt**: Gives instructions or a personality to the LLM model. This prompt determines the behaviour or characteristics of the model.\n",
    "- **User Prompt**: Input given by a user.\n",
    "- **AI Prompt**: Output generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a61fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            template=\"You are an AI receptionist at a five-star hotel. Welcome and helps guests to the best of your ability in {language}.\",\n",
    "            input_variables=[\"language\"],\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66224794",
   "metadata": {},
   "source": [
    "Earlier, `MessageState` contained only a single field (`messages`). To store multiple state variables, such as language, ID, or any custom matadata, we need to define a new `State` class explicitly with all required fields. It allows the graph to manage, validate, and pass around all relevant state components clearly while reducing bugs by enabling type checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a74e97a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Bien sûr ! Je serais ravi de vous aider à faire une réservation. Pourriez-vous me donner les détails suivants, s'il vous plaît ?\n",
      "\n",
      "1. Vos dates d'arrivée et de départ.\n",
      "2. Le type de chambre que vous préférez (simple, double, suite, etc.).\n",
      "3. Le nombre de personnes qui séjourneront.\n",
      "4. Toute demande spéciale ou besoin particulier.\n",
      "\n",
      "Merci !\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):  # Custom State class\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)  # Pass it to StateGraph\n",
    "\n",
    "\n",
    "def call_model(state: State) -> dict[str, BaseMessage]:\n",
    "    prompt = prompt_template.invoke(input=state)\n",
    "    response = model.invoke(input=prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(start_key=START, end_key=\"model\")\n",
    "workflow.add_node(node=\"model\", action=call_model)\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"A11111\"}}\n",
    "query = \"I want to make a reservation\"\n",
    "language = \"French\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    input={\"messages\": input_messages, \"language\": language}, config=config\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c1fcc",
   "metadata": {},
   "source": [
    "## 6. Trimming\n",
    "Trimming is essential to prevent the conversation history from exceeding the LLM's context window, which can cause errors or reduce performance. LangChain has a built-in trimmer function `trim_messages()` that limits the number of tokens in the conversation history passed to a model, while preserving the most relevant messages according to a specified trimming strategy. It accepts several arguments, such as:\n",
    "- **max_tokens**: The maximum total number of tokens allowed in the conversation history sent to the model.\n",
    "- **strategy**: Determines whether the trimmer keeps the `last` (most recent) messages or the `first` (oldest) messages.\n",
    "- **token_counter**: The LLM’s tokeniser function used to accurately count tokens in each message.\n",
    "- **include_system**: Controls whether system messages are preserved or removed during trimming.\n",
    "- **allow_partial**: Specifies whether messages can be partially trimmed by cutting off content if too long, rather than discarding entire messages.\n",
    "- **start_on**: Indicates whether the conversation history should start on a `human` message or an `ai` message to preserve conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb331940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='and who is harrison chasing anyways', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=80,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=False,\n",
    "    allow_partial=True,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant, you always respond with a joke.\"),\n",
    "    HumanMessage(content=\"i wonder why it's called langchain\"),\n",
    "    AIMessage(\n",
    "        content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    HumanMessage(content=\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
    "    ),\n",
    "    HumanMessage(content=\"what do you call a speechless parrot\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61dcba",
   "metadata": {},
   "source": [
    "The trimmer should be placed before passing the messages to the prompt.\n",
    "`trim -> prompt -> model.invoke()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50c4f00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages before trimming: 7\n",
      "Messages after trimming: 4\n",
      "Remaining messages:\n",
      "HumanMessage: and who is harrison chasing anyways\n",
      "AIMessage: Hmmm let me think.\n",
      "\n",
      "Why, he's probably chasing after the last cup of coffee in the office!\n",
      "HumanMessage: what do you call a speechless parrot\n",
      "HumanMessage: What did I say at first?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Dijiste \"and who is harrison chasing anyways\". ¿Necesitas ayuda con algo más relacionado con eso? Estoy aquí para ayudarte.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State) -> dict[str, list[BaseMessage]]:\n",
    "    print(f\"Messages before trimming: {len(state['messages'])}\")\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    print(f\"Messages after trimming: {len(trimmed_messages)}\")\n",
    "    print(\"Remaining messages:\")\n",
    "    for msg in trimmed_messages:\n",
    "        print(\n",
    "            f\"{type(msg).__name__}: {msg.content}\"\n",
    "        )  # Type (System, AI, Human) : Content\n",
    "    prompt = prompt_template.invoke(\n",
    "        input={\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(input=prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(start_key=START, end_key=\"model\")\n",
    "workflow.add_node(node=\"model\", action=call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"C12345\"}}\n",
    "query = \"What did I say at first?\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    input={\"messages\": input_messages, \"language\": language}, config=config\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b370d",
   "metadata": {},
   "source": [
    "## 7. Streaming\n",
    "Streaming allows chatbots to deliver the model's output tokens immediately as they are generated, instead of waiting for the entire response to be completed before displaying anything to the user. This creates a near real-time typing or response generation effect, significantly improving perceived responsiveness and user experience.\n",
    "\n",
    "To enable streaming in LangGraph or LangChain, instead of using `app.invoke()`, `app.stream()` should be used with the parameter `stream_mode`. This will yield tokens as they are produced by the model, allowing the application to process and display them incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed3b4778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages before trimming: 1\n",
      "Messages after trimming: 1\n",
      "Remaining messages:\n",
      "HumanMessage: Tell me a joke\n",
      "|Sure|!| Here's| one| for| you|:\n",
      "\n",
      "|Why| don|’t| scientists| trust| atoms|?\n",
      "\n",
      "|Because| they| make| up| everything|!| \n",
      "\n",
      "|I| hope| that| brings| a| smile| to| your| day|!| How| can| I| assist| you| further|?||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"C54321\"}}\n",
    "query = \"Tell me a joke\"\n",
    "language = \"English\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "for chunk, metadata in app.stream(\n",
    "    input={\"messages\": input_messages, \"language\": language},\n",
    "    config=config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):\n",
    "        print(chunk.content, end=\"|\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4ce77",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "\n",
    "1. Briggs, J. (2025). *LangChain Mastery in 2025 | Full 5 Hour Course [LangChain v0.3]* [Video]. Youtube.<br>\n",
    "https://www.youtube.com/watch?v=Cyv-dgv80kE\n",
    "\n",
    "1. LangChain. (n.d.). *Build a Chatbot* [Tutorial].<br>\n",
    "https://python.langchain.com/docs/tutorials/chatbot/\n",
    "\n",
    "1. LangChain. (n.d.). *Build a simple LLM application with chat models and prompt templates* [Tutorial].<br>\n",
    "https://python.langchain.com/docs/tutorials/llm_chain/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
