{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a91ea71",
   "metadata": {},
   "source": [
    "# Random Forest Classifier from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Loading Data](#2-loading-data)\n",
    "3. [Train Test Split](#3-train-test-split)\n",
    "4. [Gini Impurity and Entropy Metrics](#4-gini-impurity-and-entropy-metrics)\n",
    "    - [Gini Impurity](#gini-impurity)\n",
    "    - [Entropy](#entropy)\n",
    "5. [Information Gain](#5-information-gain)\n",
    "6. [Bootstrapping](#6-bootstrapping)\n",
    "7. [Identifying the Best Split](#7-identifying-the-best-split)\n",
    "8. [Building the Decision Tree](#8-building-the-decision-tree)\n",
    "9. [Building Random Forest](#9-building-random-forest)\n",
    "10. [Traversing the Free for Prediction](#10-traversing-the-tree-for-prediction)\n",
    "11. [Predictions](#11-predictions)\n",
    "12. [Evaluation Metrics](#12-evaluation-metrics)\n",
    "    - [Binary Confusion Matrix](#binary-confusion-matrix)\n",
    "    - [Multi-Class Confusion Matrix](#multi-class-confusion-matrix)\n",
    "    - [Accuracy](#accuracy)\n",
    "    - [Precision](#precision)\n",
    "    - [Recall](#recall)\n",
    "    - [F1-Score](#f1-score)\n",
    "13. [Encapsulation](#13-encapsulation)\n",
    "14. [Comparison with Scikit-Learn](#14-comparison-with-scikit-learn)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "430ca858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict, Optional, Any\n",
    "from numpy.typing import NDArray\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72c7bb",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This notebook is an extension of [Decision Tree Classifier from Scratch](https://github.com/tsu76i/DS-playground/blob/main/2.%20Building%20ML%20Models%20From%20Scratch/2.3%20CART/decision_tree_classifier.ipynb).\n",
    "\n",
    "Random forests are an ensemble learning technique that combines multiple decision trees, each trained on a random subset of the data (with replacement) and a random subset of features at each split. The final prediction is made by aggregating the results of all trees(**majority vote** for classification, **average** for regression). Compared to decision trees, this approach provides better accuracy, reduced overfitting and more stable predictions, though at the cost of increased computational complexity and reduced interpretability. This method introduces two key randomisation techniques during the training process:\n",
    "\n",
    "1. **Bootstrap Sampling**: Each tree is trained on a bootstrapped dataset, which is a random sample of the original dataset created *with replacement*. This ensures diversity among the trees.\n",
    "2. **Feature Randomisation**: At each split in a tree, a random subset of features is considered rather than evaluating all features. This prevents dominant features from appearing in every tree and further promotes diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64eea1",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4ced7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (569, 30)\n",
      "Target shape: (569,)\n",
      "Features: \n",
      "['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "feature_names = data.feature_names.tolist()\n",
    "class_names = data.target_names.tolist()\n",
    "X, y = data.data, data.target\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['diagnosis'] = y\n",
    "X, y = df.drop('diagnosis', axis=1), df['diagnosis']\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "print(f'Features: \\n{feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d17c0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mean radius",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean texture",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean perimeter",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean area",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean smoothness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean compactness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean concavity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean concave points",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean symmetry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean fractal dimension",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "radius error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "texture error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "perimeter error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "area error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "smoothness error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compactness error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "concavity error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "concave points error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "symmetry error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fractal dimension error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst radius",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst texture",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst perimeter",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst area",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst smoothness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst compactness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst concavity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst concave points",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst symmetry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst fractal dimension",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diagnosis",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "f281d123-a8d5-477d-a306-83fa9d965553",
       "rows": [
        [
         "0",
         "17.99",
         "10.38",
         "122.8",
         "1001.0",
         "0.1184",
         "0.2776",
         "0.3001",
         "0.1471",
         "0.2419",
         "0.07871",
         "1.095",
         "0.9053",
         "8.589",
         "153.4",
         "0.006399",
         "0.04904",
         "0.05373",
         "0.01587",
         "0.03003",
         "0.006193",
         "25.38",
         "17.33",
         "184.6",
         "2019.0",
         "0.1622",
         "0.6656",
         "0.7119",
         "0.2654",
         "0.4601",
         "0.1189",
         "0"
        ],
        [
         "1",
         "20.57",
         "17.77",
         "132.9",
         "1326.0",
         "0.08474",
         "0.07864",
         "0.0869",
         "0.07017",
         "0.1812",
         "0.05667",
         "0.5435",
         "0.7339",
         "3.398",
         "74.08",
         "0.005225",
         "0.01308",
         "0.0186",
         "0.0134",
         "0.01389",
         "0.003532",
         "24.99",
         "23.41",
         "158.8",
         "1956.0",
         "0.1238",
         "0.1866",
         "0.2416",
         "0.186",
         "0.275",
         "0.08902",
         "0"
        ],
        [
         "2",
         "19.69",
         "21.25",
         "130.0",
         "1203.0",
         "0.1096",
         "0.1599",
         "0.1974",
         "0.1279",
         "0.2069",
         "0.05999",
         "0.7456",
         "0.7869",
         "4.585",
         "94.03",
         "0.00615",
         "0.04006",
         "0.03832",
         "0.02058",
         "0.0225",
         "0.004571",
         "23.57",
         "25.53",
         "152.5",
         "1709.0",
         "0.1444",
         "0.4245",
         "0.4504",
         "0.243",
         "0.3613",
         "0.08758",
         "0"
        ],
        [
         "3",
         "11.42",
         "20.38",
         "77.58",
         "386.1",
         "0.1425",
         "0.2839",
         "0.2414",
         "0.1052",
         "0.2597",
         "0.09744",
         "0.4956",
         "1.156",
         "3.445",
         "27.23",
         "0.00911",
         "0.07458",
         "0.05661",
         "0.01867",
         "0.05963",
         "0.009208",
         "14.91",
         "26.5",
         "98.87",
         "567.7",
         "0.2098",
         "0.8663",
         "0.6869",
         "0.2575",
         "0.6638",
         "0.173",
         "0"
        ],
        [
         "4",
         "20.29",
         "14.34",
         "135.1",
         "1297.0",
         "0.1003",
         "0.1328",
         "0.198",
         "0.1043",
         "0.1809",
         "0.05883",
         "0.7572",
         "0.7813",
         "5.438",
         "94.44",
         "0.01149",
         "0.02461",
         "0.05688",
         "0.01885",
         "0.01756",
         "0.005115",
         "22.54",
         "16.67",
         "152.2",
         "1575.0",
         "0.1374",
         "0.205",
         "0.4",
         "0.1625",
         "0.2364",
         "0.07678",
         "0"
        ]
       ],
       "shape": {
        "columns": 31,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  diagnosis  \n",
       "0          0.4601                  0.11890          0  \n",
       "1          0.2750                  0.08902          0  \n",
       "2          0.3613                  0.08758          0  \n",
       "3          0.6638                  0.17300          0  \n",
       "4          0.2364                  0.07678          0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac17214d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "diagnosis",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "50203fd8-a028-4824-a53e-b88839f0247a",
       "rows": [
        [
         "1",
         "357"
        ],
        [
         "0",
         "212"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "diagnosis\n",
       "1    357\n",
       "0    212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0131056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant', 'benign']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d7e2b",
   "metadata": {},
   "source": [
    "In this dataset, the features represent the characteristics of breast cancer (e.g., radius, texture, etc.), while the target is a boolean value indicating whether the tumour is malignant (0) or benign (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b81bb",
   "metadata": {},
   "source": [
    "## 3. Train Test Split\n",
    "Train test split is a fundamental model validation technique in machine learning. It divides a dataset into two separate portions: a **training set** used to train a model, and a **testing set** used to evaluate how well the model can perform on unseen data. \n",
    "\n",
    "The typical split ratio is 80% for training and 20% for testing, though this can vary (70/30 or 90/10 are also common). The key principle is that the test set must remain completely separated during model training process, and should never be used to make decisions about the model or tune parameters. \n",
    "\n",
    "The split is usually done randomly to ensure both sets are representative of the overall dataset, and many libraries (such as scikit-learn) provide build-in functions that handle this process automatically while maintaining proper randomisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9418d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: pd.DataFrame, y: pd.Series, test_size: float = 0.2,\n",
    "                     random_state: int = None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Split arrays or matrices into random train and test subsets.\n",
    "\n",
    "    Args:\n",
    "        X: Input features, a 2D array with rows (samples) and columns (features).\n",
    "        y: Target values/labels, a 1D array with rows (samples).\n",
    "        test_size: Proportion of the dataset to include in the test split. Must be between 0.0 and 1.0. default = 0.2\n",
    "        random_state: Seed for the random number generator to ensure reproducible results. default = None\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - X_train: Training set features.\n",
    "            - X_test: Testing set features.\n",
    "            - y_train: Training set target values.\n",
    "            - y_test: Testing set target values.\n",
    "    \"\"\"\n",
    "    # Set a random seed if it exists\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Create a list of numbers from 0 to len(X)\n",
    "    indices = np.arange(len(X))\n",
    "\n",
    "    # Shuffle the indices\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Define the size of our test data from len(X)\n",
    "    test_size = int(test_size * len(X))\n",
    "\n",
    "    # Generate indices for test and train data\n",
    "    test_indices: NDArray[np.int64] = indices[:test_size]\n",
    "    train_indices: NDArray[np.int64] = indices[test_size:]\n",
    "\n",
    "    # Return: X_train, X_test, y_train, y_test\n",
    "    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e863057",
   "metadata": {},
   "source": [
    "## 4. Gini Impurity and Entropy Metrics\n",
    "### Gini Impurity\n",
    "Gini impurity is a measure of the likelihood that a randomly chosen sample from a dataset will be incorrectly classified. It quantifies how impure a node is, with values ranging from $0$ (minimum impurity) to $0.5$ (maximum impurity) for binary classification. For multi-class problems, however, the maximum impurity occurs when all classes are equally probable, and the value depends on the number of classes. The formula for Gini impurity is:\n",
    "\n",
    "\\begin{align*}\n",
    "G = 1 - \\sum_{i=1}^{k} p_{i}^{2}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $k$: Number of classes.\n",
    "- $p_{i}$: Proportion of samples belonging to class $i$ in the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cdd67fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y: pd.Series) -> float:\n",
    "    proportions = np.bincount(y) / len(y)\n",
    "    return 1 - np.sum(proportions**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9de853",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "Entropy measures the amount of uncertainty or randomness in the data. It is based on information theory and represents the expected amount of information required to classify a sample. For binary classification ranges from $0$ (minimum entropy) to $1$ (maximum entropy). For $k$ classes, the range is from $0$ to $log_{2}(k)$. The formula for entropy is:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "H = - \\sum_{i=1}^{k} p_{i} log_{2}(p_{i})\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $k$: Number of classes.\n",
    "- $p_{i}$: Proportion of samples belonging to class $i$ in the node.\n",
    "\n",
    "Gini tends to split nodes based on the most frequent classes, while entropy provides a more nuanced measure especially in cases with many classes or highly imbalanced distributions. Both metrics provide similar results, but Gini is often preferred for computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7d21d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y: pd.Series) -> float:\n",
    "    proportions = np.bincount(y) / len(y)\n",
    "    proportions = proportions[proportions > 0]  # Avoid log(0)\n",
    "    return -np.sum(proportions * np.log2(proportions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "154de035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98878826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Impurity: 0.46753\n",
      "Entropy: 0.95264\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gini Impurity: {gini(y):.5f}\")\n",
    "print(f\"Entropy: {entropy(y):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68834ab5",
   "metadata": {},
   "source": [
    "## 5. Information Gain\n",
    "Information Gain is a metric used to measure the effectiveness of a feature in splitting a dataset into subsets that are more pure concerning the target variable. It quantifies the reduction in entropy or Gini impurity, and a higher information gain indicates a better feature for making splits.\n",
    "\n",
    "\\begin{align*}\n",
    "IG(S, A) = H(S) - \\sum_{i=1}^{n} \\dfrac{|S_i|}{|S|}H(S_{i})\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $H(S)$: Entropy (or Geni) of the original dataset $S$.\n",
    "- $S_{i}$: Subset of $S$ created by splitting on feature $A$ for the $i_{th}$ value or range of the feature.\n",
    "- $\\dfrac{|S_i|}{|S|}$: Proportion of samples in subset $S_{i}$.\n",
    "- $H(S_{i})$: Entropy (or Geni) of subset $S_{i}$.\n",
    "\n",
    "\n",
    "\n",
    "The following `information_gain` function calculates the difference between the metric for the parent node and the weighted average of the metrics for the child nodes (left and right splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4714ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y: pd.Series, y_left: pd.Series, y_right: pd.Series,\n",
    "                     metric: str = 'gini') -> float:\n",
    "    \"\"\"\n",
    "    Calculate the information gain of a split.\n",
    "\n",
    "    Args:\n",
    "        y: Labels of the parent node.\n",
    "        y_left: Labels of the left child node after the split.\n",
    "        y_right: Labels of the right child node after the split.\n",
    "        metric: Splitting criterion, either 'gini' or 'entropy'. Defaults to 'gini'.\n",
    "\n",
    "    Returns:\n",
    "        Information gain resulting from the split.\n",
    "    \"\"\"\n",
    "    if metric == 'gini':\n",
    "        parent_metric = gini(y)\n",
    "        left_metric = gini(y_left)\n",
    "        right_metric = gini(y_right)\n",
    "    else:  # metric == \"entropy\"\n",
    "        parent_metric = entropy(y)\n",
    "        left_metric = entropy(y_left)\n",
    "        right_metric = entropy(y_right)\n",
    "\n",
    "    weighted_metric = (\n",
    "        len(y_left) / len(y) * left_metric\n",
    "        + len(y_right) / len(y) * right_metric\n",
    "    )\n",
    "    return parent_metric - weighted_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b9d65",
   "metadata": {},
   "source": [
    "## 6. Bootstrapping\n",
    "Bootstrapping is a statistical resampling method that involves sampling data points with replacement. In creating a new dataset (**bootstrap sample**) from the original dataset, some data points may appear multiple times, while others may be excluded. Though individual data points may repeat, the size of bootstrap sample $n$ is typically the same as the original dataset. This method ensures variability among datasets, which helps reduce overfitting when used in ensemble learning.\n",
    "\n",
    "For a dataset with $n$ examples, each sample has a $1 - \\left( 1 - \\dfrac{1}{n} \\right)^{n}$ chance of being selected at least once in the bootstrap sample. As $n$ becomes large, this value approaches $1-\\text{e}^{-1} \\approx 0.632$. Hence, about 63.2% of the original dataset is expected to appear in any given bootstrap sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "93ad2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X: pd.DataFrame, y: pd.Series, n_samples: Optional[int] = None,\n",
    "                     random_state: Optional[int] = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Generate a bootstrap sample from the dataset.\n",
    "\n",
    "    Args:\n",
    "        X: Input features.\n",
    "        y: Target labels.\n",
    "        n_samples: Samples to draw (default: dataset size).\n",
    "        random_state: Random seed.\n",
    "\n",
    "    Returns:\n",
    "        Bootstrapped (X, y) tuple.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    if n_samples is None:\n",
    "        n_samples = len(X)\n",
    "    indices = np.random.choice(len(X), size=n_samples, replace=True)\n",
    "    return X.iloc[indices], y.iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "474d4258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mean radius",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean texture",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean perimeter",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean area",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean smoothness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean compactness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean concavity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean concave points",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean symmetry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean fractal dimension",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "radius error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "texture error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "perimeter error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "area error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "smoothness error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compactness error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "concavity error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "concave points error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "symmetry error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fractal dimension error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst radius",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst texture",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst perimeter",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst area",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst smoothness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst compactness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst concavity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst concave points",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst symmetry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst fractal dimension",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a5bf9d58-8db4-4cba-8ce8-1ab6e0f2111b",
       "rows": [
        [
         "102",
         "12.18",
         "20.52",
         "77.22",
         "458.7",
         "0.08013",
         "0.04038",
         "0.02383",
         "0.0177",
         "0.1739",
         "0.05677",
         "0.1924",
         "1.571",
         "1.183",
         "14.68",
         "0.00508",
         "0.006098",
         "0.01069",
         "0.006797",
         "0.01447",
         "0.001532",
         "13.34",
         "32.84",
         "84.58",
         "547.8",
         "0.1123",
         "0.08862",
         "0.1145",
         "0.07431",
         "0.2694",
         "0.06878"
        ],
        [
         "435",
         "13.98",
         "19.62",
         "91.12",
         "599.5",
         "0.106",
         "0.1133",
         "0.1126",
         "0.06463",
         "0.1669",
         "0.06544",
         "0.2208",
         "0.9533",
         "1.602",
         "18.85",
         "0.005314",
         "0.01791",
         "0.02185",
         "0.009567",
         "0.01223",
         "0.002846",
         "17.04",
         "30.8",
         "113.9",
         "869.3",
         "0.1613",
         "0.3568",
         "0.4069",
         "0.1827",
         "0.3179",
         "0.1055"
        ],
        [
         "270",
         "14.29",
         "16.82",
         "90.3",
         "632.6",
         "0.06429",
         "0.02675",
         "0.00725",
         "0.00625",
         "0.1508",
         "0.05376",
         "0.1302",
         "0.7198",
         "0.8439",
         "10.77",
         "0.003492",
         "0.00371",
         "0.004826",
         "0.003608",
         "0.01536",
         "0.001381",
         "14.91",
         "20.65",
         "94.44",
         "684.6",
         "0.08567",
         "0.05036",
         "0.03866",
         "0.03333",
         "0.2458",
         "0.0612"
        ],
        [
         "106",
         "11.64",
         "18.33",
         "75.17",
         "412.5",
         "0.1142",
         "0.1017",
         "0.0707",
         "0.03485",
         "0.1801",
         "0.0652",
         "0.306",
         "1.657",
         "2.155",
         "20.62",
         "0.00854",
         "0.0231",
         "0.02945",
         "0.01398",
         "0.01565",
         "0.00384",
         "13.14",
         "29.26",
         "85.51",
         "521.7",
         "0.1688",
         "0.266",
         "0.2873",
         "0.1218",
         "0.2806",
         "0.09097"
        ],
        [
         "71",
         "8.888",
         "14.64",
         "58.79",
         "244.0",
         "0.09783",
         "0.1531",
         "0.08606",
         "0.02872",
         "0.1902",
         "0.0898",
         "0.5262",
         "0.8522",
         "3.168",
         "25.44",
         "0.01721",
         "0.09368",
         "0.05671",
         "0.01766",
         "0.02541",
         "0.02193",
         "9.733",
         "15.67",
         "62.56",
         "284.4",
         "0.1207",
         "0.2436",
         "0.1434",
         "0.04786",
         "0.2254",
         "0.1084"
        ],
        [
         "20",
         "13.08",
         "15.71",
         "85.63",
         "520.0",
         "0.1075",
         "0.127",
         "0.04568",
         "0.0311",
         "0.1967",
         "0.06811",
         "0.1852",
         "0.7477",
         "1.383",
         "14.67",
         "0.004097",
         "0.01898",
         "0.01698",
         "0.00649",
         "0.01678",
         "0.002425",
         "14.5",
         "20.49",
         "96.09",
         "630.5",
         "0.1312",
         "0.2776",
         "0.189",
         "0.07283",
         "0.3184",
         "0.08183"
        ],
        [
         "121",
         "18.66",
         "17.12",
         "121.4",
         "1077.0",
         "0.1054",
         "0.11",
         "0.1457",
         "0.08665",
         "0.1966",
         "0.06213",
         "0.7128",
         "1.581",
         "4.895",
         "90.47",
         "0.008102",
         "0.02101",
         "0.03342",
         "0.01601",
         "0.02045",
         "0.00457",
         "22.25",
         "24.9",
         "145.4",
         "1549.0",
         "0.1503",
         "0.2291",
         "0.3272",
         "0.1674",
         "0.2894",
         "0.08456"
        ],
        [
         "466",
         "13.14",
         "20.74",
         "85.98",
         "536.9",
         "0.08675",
         "0.1089",
         "0.1085",
         "0.0351",
         "0.1562",
         "0.0602",
         "0.3152",
         "0.7884",
         "2.312",
         "27.4",
         "0.007295",
         "0.03179",
         "0.04615",
         "0.01254",
         "0.01561",
         "0.00323",
         "14.8",
         "25.46",
         "100.9",
         "689.1",
         "0.1351",
         "0.3549",
         "0.4504",
         "0.1181",
         "0.2563",
         "0.08174"
        ],
        [
         "214",
         "14.19",
         "23.81",
         "92.87",
         "610.7",
         "0.09463",
         "0.1306",
         "0.1115",
         "0.06462",
         "0.2235",
         "0.06433",
         "0.4207",
         "1.845",
         "3.534",
         "31.0",
         "0.01088",
         "0.0371",
         "0.03688",
         "0.01627",
         "0.04499",
         "0.004768",
         "16.86",
         "34.85",
         "115.0",
         "811.3",
         "0.1559",
         "0.4059",
         "0.3744",
         "0.1772",
         "0.4724",
         "0.1026"
        ],
        [
         "330",
         "16.03",
         "15.51",
         "105.8",
         "793.2",
         "0.09491",
         "0.1371",
         "0.1204",
         "0.07041",
         "0.1782",
         "0.05976",
         "0.3371",
         "0.7476",
         "2.629",
         "33.27",
         "0.005839",
         "0.03245",
         "0.03715",
         "0.01459",
         "0.01467",
         "0.003121",
         "18.76",
         "21.98",
         "124.3",
         "1070.0",
         "0.1435",
         "0.4478",
         "0.4956",
         "0.1981",
         "0.3019",
         "0.09124"
        ]
       ],
       "shape": {
        "columns": 30,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>12.180</td>\n",
       "      <td>20.52</td>\n",
       "      <td>77.22</td>\n",
       "      <td>458.7</td>\n",
       "      <td>0.08013</td>\n",
       "      <td>0.04038</td>\n",
       "      <td>0.02383</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.05677</td>\n",
       "      <td>...</td>\n",
       "      <td>13.340</td>\n",
       "      <td>32.84</td>\n",
       "      <td>84.58</td>\n",
       "      <td>547.8</td>\n",
       "      <td>0.11230</td>\n",
       "      <td>0.08862</td>\n",
       "      <td>0.11450</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.06878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>13.980</td>\n",
       "      <td>19.62</td>\n",
       "      <td>91.12</td>\n",
       "      <td>599.5</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.06463</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06544</td>\n",
       "      <td>...</td>\n",
       "      <td>17.040</td>\n",
       "      <td>30.80</td>\n",
       "      <td>113.90</td>\n",
       "      <td>869.3</td>\n",
       "      <td>0.16130</td>\n",
       "      <td>0.35680</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.10550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>14.290</td>\n",
       "      <td>16.82</td>\n",
       "      <td>90.30</td>\n",
       "      <td>632.6</td>\n",
       "      <td>0.06429</td>\n",
       "      <td>0.02675</td>\n",
       "      <td>0.00725</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.05376</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>20.65</td>\n",
       "      <td>94.44</td>\n",
       "      <td>684.6</td>\n",
       "      <td>0.08567</td>\n",
       "      <td>0.05036</td>\n",
       "      <td>0.03866</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>11.640</td>\n",
       "      <td>18.33</td>\n",
       "      <td>75.17</td>\n",
       "      <td>412.5</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.07070</td>\n",
       "      <td>0.03485</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.06520</td>\n",
       "      <td>...</td>\n",
       "      <td>13.140</td>\n",
       "      <td>29.26</td>\n",
       "      <td>85.51</td>\n",
       "      <td>521.7</td>\n",
       "      <td>0.16880</td>\n",
       "      <td>0.26600</td>\n",
       "      <td>0.28730</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.2806</td>\n",
       "      <td>0.09097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>8.888</td>\n",
       "      <td>14.64</td>\n",
       "      <td>58.79</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.09783</td>\n",
       "      <td>0.15310</td>\n",
       "      <td>0.08606</td>\n",
       "      <td>0.02872</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.08980</td>\n",
       "      <td>...</td>\n",
       "      <td>9.733</td>\n",
       "      <td>15.67</td>\n",
       "      <td>62.56</td>\n",
       "      <td>284.4</td>\n",
       "      <td>0.12070</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.14340</td>\n",
       "      <td>0.04786</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.10840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.080</td>\n",
       "      <td>15.71</td>\n",
       "      <td>85.63</td>\n",
       "      <td>520.0</td>\n",
       "      <td>0.10750</td>\n",
       "      <td>0.12700</td>\n",
       "      <td>0.04568</td>\n",
       "      <td>0.03110</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.06811</td>\n",
       "      <td>...</td>\n",
       "      <td>14.500</td>\n",
       "      <td>20.49</td>\n",
       "      <td>96.09</td>\n",
       "      <td>630.5</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.18900</td>\n",
       "      <td>0.07283</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.08183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>18.660</td>\n",
       "      <td>17.12</td>\n",
       "      <td>121.40</td>\n",
       "      <td>1077.0</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>0.14570</td>\n",
       "      <td>0.08665</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.06213</td>\n",
       "      <td>...</td>\n",
       "      <td>22.250</td>\n",
       "      <td>24.90</td>\n",
       "      <td>145.40</td>\n",
       "      <td>1549.0</td>\n",
       "      <td>0.15030</td>\n",
       "      <td>0.22910</td>\n",
       "      <td>0.32720</td>\n",
       "      <td>0.16740</td>\n",
       "      <td>0.2894</td>\n",
       "      <td>0.08456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>13.140</td>\n",
       "      <td>20.74</td>\n",
       "      <td>85.98</td>\n",
       "      <td>536.9</td>\n",
       "      <td>0.08675</td>\n",
       "      <td>0.10890</td>\n",
       "      <td>0.10850</td>\n",
       "      <td>0.03510</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.06020</td>\n",
       "      <td>...</td>\n",
       "      <td>14.800</td>\n",
       "      <td>25.46</td>\n",
       "      <td>100.90</td>\n",
       "      <td>689.1</td>\n",
       "      <td>0.13510</td>\n",
       "      <td>0.35490</td>\n",
       "      <td>0.45040</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.2563</td>\n",
       "      <td>0.08174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>14.190</td>\n",
       "      <td>23.81</td>\n",
       "      <td>92.87</td>\n",
       "      <td>610.7</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.13060</td>\n",
       "      <td>0.11150</td>\n",
       "      <td>0.06462</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.06433</td>\n",
       "      <td>...</td>\n",
       "      <td>16.860</td>\n",
       "      <td>34.85</td>\n",
       "      <td>115.00</td>\n",
       "      <td>811.3</td>\n",
       "      <td>0.15590</td>\n",
       "      <td>0.40590</td>\n",
       "      <td>0.37440</td>\n",
       "      <td>0.17720</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.10260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>16.030</td>\n",
       "      <td>15.51</td>\n",
       "      <td>105.80</td>\n",
       "      <td>793.2</td>\n",
       "      <td>0.09491</td>\n",
       "      <td>0.13710</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.07041</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.05976</td>\n",
       "      <td>...</td>\n",
       "      <td>18.760</td>\n",
       "      <td>21.98</td>\n",
       "      <td>124.30</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>0.14350</td>\n",
       "      <td>0.44780</td>\n",
       "      <td>0.49560</td>\n",
       "      <td>0.19810</td>\n",
       "      <td>0.3019</td>\n",
       "      <td>0.09124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "102       12.180         20.52           77.22      458.7          0.08013   \n",
       "435       13.980         19.62           91.12      599.5          0.10600   \n",
       "270       14.290         16.82           90.30      632.6          0.06429   \n",
       "106       11.640         18.33           75.17      412.5          0.11420   \n",
       "71         8.888         14.64           58.79      244.0          0.09783   \n",
       "20        13.080         15.71           85.63      520.0          0.10750   \n",
       "121       18.660         17.12          121.40     1077.0          0.10540   \n",
       "466       13.140         20.74           85.98      536.9          0.08675   \n",
       "214       14.190         23.81           92.87      610.7          0.09463   \n",
       "330       16.030         15.51          105.80      793.2          0.09491   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "102           0.04038         0.02383              0.01770         0.1739   \n",
       "435           0.11330         0.11260              0.06463         0.1669   \n",
       "270           0.02675         0.00725              0.00625         0.1508   \n",
       "106           0.10170         0.07070              0.03485         0.1801   \n",
       "71            0.15310         0.08606              0.02872         0.1902   \n",
       "20            0.12700         0.04568              0.03110         0.1967   \n",
       "121           0.11000         0.14570              0.08665         0.1966   \n",
       "466           0.10890         0.10850              0.03510         0.1562   \n",
       "214           0.13060         0.11150              0.06462         0.2235   \n",
       "330           0.13710         0.12040              0.07041         0.1782   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "102                 0.05677  ...        13.340          32.84   \n",
       "435                 0.06544  ...        17.040          30.80   \n",
       "270                 0.05376  ...        14.910          20.65   \n",
       "106                 0.06520  ...        13.140          29.26   \n",
       "71                  0.08980  ...         9.733          15.67   \n",
       "20                  0.06811  ...        14.500          20.49   \n",
       "121                 0.06213  ...        22.250          24.90   \n",
       "466                 0.06020  ...        14.800          25.46   \n",
       "214                 0.06433  ...        16.860          34.85   \n",
       "330                 0.05976  ...        18.760          21.98   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "102            84.58       547.8           0.11230            0.08862   \n",
       "435           113.90       869.3           0.16130            0.35680   \n",
       "270            94.44       684.6           0.08567            0.05036   \n",
       "106            85.51       521.7           0.16880            0.26600   \n",
       "71             62.56       284.4           0.12070            0.24360   \n",
       "20             96.09       630.5           0.13120            0.27760   \n",
       "121           145.40      1549.0           0.15030            0.22910   \n",
       "466           100.90       689.1           0.13510            0.35490   \n",
       "214           115.00       811.3           0.15590            0.40590   \n",
       "330           124.30      1070.0           0.14350            0.44780   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "102          0.11450               0.07431          0.2694   \n",
       "435          0.40690               0.18270          0.3179   \n",
       "270          0.03866               0.03333          0.2458   \n",
       "106          0.28730               0.12180          0.2806   \n",
       "71           0.14340               0.04786          0.2254   \n",
       "20           0.18900               0.07283          0.3184   \n",
       "121          0.32720               0.16740          0.2894   \n",
       "466          0.45040               0.11810          0.2563   \n",
       "214          0.37440               0.17720          0.4724   \n",
       "330          0.49560               0.19810          0.3019   \n",
       "\n",
       "     worst fractal dimension  \n",
       "102                  0.06878  \n",
       "435                  0.10550  \n",
       "270                  0.06120  \n",
       "106                  0.09097  \n",
       "71                   0.10840  \n",
       "20                   0.08183  \n",
       "121                  0.08456  \n",
       "466                  0.08174  \n",
       "214                  0.10260  \n",
       "330                  0.09124  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_sample(X, y, random_state=42)[0][:10]  # X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2ca2e45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "diagnosis",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "60bc438f-ab2e-4c0b-8576-695d616dd441",
       "rows": [
        [
         "102",
         "1"
        ],
        [
         "435",
         "0"
        ],
        [
         "270",
         "1"
        ],
        [
         "106",
         "1"
        ],
        [
         "71",
         "1"
        ],
        [
         "20",
         "1"
        ],
        [
         "121",
         "0"
        ],
        [
         "466",
         "1"
        ],
        [
         "214",
         "0"
        ],
        [
         "330",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "102    1\n",
       "435    0\n",
       "270    1\n",
       "106    1\n",
       "71     1\n",
       "20     1\n",
       "121    0\n",
       "466    1\n",
       "214    0\n",
       "330    0\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_sample(X, y, random_state=42)[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b82ab",
   "metadata": {},
   "source": [
    "## 7. Identifying the Best Split\n",
    "This function identifies the best feature and threshold to split the data using the specified metric (Gini or Entropy).\n",
    "\n",
    "Steps are:\n",
    "\n",
    "1. Select some features randomly (Recommended: `sqrt` for classification, `log2` for regression).\n",
    "\n",
    "2. For each selected feature, iterate over all unique thresholds.\n",
    "\n",
    "3. Split the data into left and right subsets based on the threshold (skip invalid ones).\n",
    "\n",
    "4. Compute the Gini/Entropy for both subsets and calculate Information Gain.\n",
    "\n",
    "5. If the newly computed `info_gain` > `best_info_gain`, then update `best_info_gain` with the new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5b4e1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X: pd.DataFrame, y: pd.Series, metric: str = 'gini', feature_names=None, max_features=None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Find the best split for a dataset.\n",
    "\n",
    "    Args:\n",
    "        X: Input features (DataFrame of shape [n_samples, total_n_features]).\n",
    "        y: Labels (Series of shape [n_samples]).\n",
    "        metric: Splitting criterion, either \"gini\" or \"entropy\". Defaults to 'gini'.\n",
    "        feature_names: List of feature names. If None, indices are used. Defaults to None.\n",
    "        max_features: Number of features to consider at each split. None(√total_n_features) or int(<=total_n_features). Defaults to None.\n",
    "    Returns:\n",
    "        Dictionary containing the best split with keys:\n",
    "              - 'feature_index' : Index of the feature used for the split.\n",
    "              - 'feature_name': Name or index of the feature.\n",
    "              - 'threshold' : Threshold value for the split.\n",
    "    \"\"\"\n",
    "    if feature_names is None and hasattr(X, 'columns'):\n",
    "        feature_names = X.columns.tolist()\n",
    "\n",
    "    # Convert X if DataFrame\n",
    "    if hasattr(X, 'to_numpy'):\n",
    "        X = X.to_numpy()\n",
    "\n",
    "    best_info_gain = float('-inf')\n",
    "    best_split = None\n",
    "    total_n_features = X.shape[1]\n",
    "\n",
    "    if isinstance(max_features, int):  # if max_features is int\n",
    "        selected_n_features = max_features if max_features <= total_n_features else total_n_features\n",
    "    else:  # Default = √total_n_features\n",
    "        selected_n_features = int(np.sqrt(total_n_features))\n",
    "\n",
    "    selected_features_idx = np.random.choice(\n",
    "        a=total_n_features, size=selected_n_features, replace=False)\n",
    "\n",
    "    # Iterate over randomly selected features.\n",
    "    for feature in selected_features_idx:\n",
    "        # Iterate over all unique thresholds for each random feature.\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            # Split the data into left and right subsets based on the threshold.\n",
    "            left_mask = X[:, feature] <= threshold\n",
    "            right_mask = X[:, feature] > threshold\n",
    "\n",
    "            # Skip invalid splits.\n",
    "            if sum(left_mask) == 0 or sum(right_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            # Compute IG.\n",
    "            info_gain = information_gain(\n",
    "                y, y[left_mask], y[right_mask], metric)\n",
    "\n",
    "            # Update `best_info_gain` if `info_gain` > `best_info_gain`.\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_split = {\n",
    "                    'feature_index': int(feature),\n",
    "                    'feature_name': feature_names[feature] if feature_names is not None else feature,\n",
    "                    'threshold': float(threshold),\n",
    "                }\n",
    "\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5f6cde0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Split: {'feature_index': 3, 'feature_name': 'mean area', 'threshold': 693.7}\n"
     ]
    }
   ],
   "source": [
    "split = best_split(X, y, metric='variance')\n",
    "print('Best Split:', split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3badef22",
   "metadata": {},
   "source": [
    "## 8. Building the Decision Tree\n",
    "This function resursively creates the tree structure as a nested dictionary with conditions (`feature` and `threshold`) and leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc14bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X: pd.DataFrame, y: pd.Series, max_depth: int = None,\n",
    "               depth: int = 0, metric: str = 'gini', feature_names: List[str | int] = None, max_features=None) -> Dict[\n",
    "        str, Any]:\n",
    "    \"\"\"\n",
    "    Build a decision tree using recursive splitting.\n",
    "\n",
    "    Args:\n",
    "        X: Input features (DataFrame of shape [n_samples, n_features]).\n",
    "        y: Labels (Series of shape [n_samples]).\n",
    "        max_depth: Maximum depth of the tree. Defaults to None (unlimited depth).\n",
    "        depth: Current depth of the tree. Used internally for recursion. Defaults to 0.\n",
    "        metric: Splitting criterion, either 'gini' or 'entropy'. Defaults to 'gini'.\n",
    "        feature_names: List of feature names. If None, indices are used. Defaults to None.\n",
    "        max_features: Number of features to consider at each split. None(√total_n_features) or int(<=total_n_features). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        - Nested dictionary representing the tree structure.\n",
    "        - Nodes contain keys: 'type', 'feature', 'threshold', 'left', 'right'.\n",
    "        - Leaf nodes contain keys: 'type', 'value'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert DataFrames to NumPy arrays\n",
    "    if hasattr(X, 'to_numpy'):\n",
    "        X = X.to_numpy()\n",
    "    if hasattr(y, 'to_numpy'):\n",
    "        y = y.to_numpy().flatten()  # Ensure 1D array\n",
    "\n",
    "    # Stop the recursion if all labels are identical or the maximum depth is reached.\n",
    "    if len(set(y)) == 1 or (max_depth is not None and depth == max_depth):\n",
    "        return {'type': 'leaf', 'value': int(np.argmax(np.bincount(y)))}\n",
    "\n",
    "    # Find the best split.\n",
    "    split = best_split(X, y, metric, feature_names, max_features)\n",
    "    if not split:\n",
    "        return {'type': 'leaf', 'value': int(np.argmax(np.bincount(y)))}\n",
    "\n",
    "    # Split the data into left and right subsets.\n",
    "    # Use feature_index for calculations.\n",
    "    left_mask = X[:, split['feature_index']] <= split['threshold']\n",
    "    right_mask = X[:, split['feature_index']] > split['threshold']\n",
    "\n",
    "    # Recursively build the left and right subtrees.\n",
    "    left_tree = build_tree(X[left_mask], y[left_mask],\n",
    "                           max_depth, depth + 1, metric, feature_names, max_features)\n",
    "    right_tree = build_tree(X[right_mask], y[right_mask],\n",
    "                            max_depth, depth + 1, metric, feature_names, max_features)\n",
    "\n",
    "    # Return the tree structure as a nested dictionary.\n",
    "    return {\n",
    "        'type': 'node',\n",
    "        'feature': split['feature_name'],\n",
    "        'threshold': split['threshold'],\n",
    "        'left': left_tree,\n",
    "        'right': right_tree,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf39f613",
   "metadata": {},
   "source": [
    "## 9. Building Random Forest\n",
    "We now create `build_random_forest` function that iterates bootstrapping samples and building trees `n_estimators` times. To reduce execution speed, parallel tree construction is implemented (with all CPU cores used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5b3f8bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_random_forest(X_train: pd.DataFrame, y_train: pd.Series, n_estimators: int,\n",
    "                        n_jobs: int = -1, max_depth: int = 15) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Optimised random forest builder using parallel processing\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        n_estimators: Number of trees\n",
    "        n_jobs: Number of CPU cores to use (-1 = all cores)\n",
    "        max_depth: Maximum tree depth\n",
    "\n",
    "    Returns:\n",
    "        List of decision trees\n",
    "    \"\"\"\n",
    "    # Build single tree\n",
    "    def _build_single_tree(i):\n",
    "        X_boot, y_boot = bootstrap_sample(X_train, y_train, random_state=i)\n",
    "        return build_tree(X_boot, y_boot, max_depth=max_depth,\n",
    "                          metric='gini', feature_names=feature_names)\n",
    "\n",
    "    # Parallel execution\n",
    "    forest = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_build_single_tree)(i)\n",
    "        for i in range(n_estimators)\n",
    "    )\n",
    "\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3d7a0",
   "metadata": {},
   "source": [
    "## 10. Traversing the Tree for Prediction\n",
    "This function traverses the tree to make predictions by following the tree from the root to a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "970c8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_tree(x: pd.DataFrame, tree: Dict[str, Any],\n",
    "                  feature_names: List[str | int] = None, max_features=None) -> int:\n",
    "    \"\"\"\n",
    "    Traverse a decision tree to make a prediction for a single sample.\n",
    "\n",
    "    Args:\n",
    "        x: Single sample.\n",
    "        tree: Decision tree structure.\n",
    "        feature_names: List of feature names. Needed for name-to-index mapping. Defaults to None.\n",
    "        max_features: Number of features to consider at each split. None(√total_n_features) or int(<=total_n_features). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Predicted label.\n",
    "    \"\"\"\n",
    "    if tree['type'] == 'leaf':\n",
    "        return tree['value']\n",
    "\n",
    "    # Resolve feature index if feature_names is provided\n",
    "    feature_index = feature_names.index(\n",
    "        tree['feature']) if feature_names is not None else tree['feature']\n",
    "\n",
    "    if x[feature_index] <= tree['threshold']:\n",
    "        return traverse_tree(x, tree['left'], feature_names, max_features)\n",
    "    else:\n",
    "        return traverse_tree(x, tree['right'], feature_names, max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd8dba",
   "metadata": {},
   "source": [
    "## 11. Predictions\n",
    "This function predicts labels for all samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5383ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: pd.DataFrame, tree: Dict[str, Any],\n",
    "            feature_names: List[str | int] = None) -> int | NDArray[np.int64]:\n",
    "    \"\"\"\n",
    "    Predict labels for the given dataset using a decision tree classifier.\n",
    "\n",
    "    Args:\n",
    "        X: Input features.\n",
    "        tree: Decision tree structure.\n",
    "        feature_names : List of feature names. Needed for name-to-index mapping. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Predicted labels (1D array for multiple samples or a single label for one sample).\n",
    "    \"\"\"\n",
    "    # Convert DataFrames to NumPy arrays\n",
    "    if hasattr(X, 'to_numpy'):\n",
    "        X = X.to_numpy()\n",
    "\n",
    "    if len(X.shape) == 1:  # If a single sample is provided\n",
    "        return traverse_tree(X, tree, feature_names)\n",
    "    return np.array([traverse_tree(x, tree, feature_names) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df212c8f",
   "metadata": {},
   "source": [
    "After all predictions have been made for the `n_estimators`, we will use a majority vote to determine the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8dafb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_majority_vote(forest: List[Dict[str, Any]], X: pd.DataFrame,\n",
    "                          feature_names: List[str] = None) -> List[int]:\n",
    "    all_preds = []\n",
    "    for tree in forest:\n",
    "        preds = predict(X, tree, feature_names)\n",
    "        all_preds.append(preds)\n",
    "    all_preds = np.array(all_preds)\n",
    "    majority_vote, _ = mode(all_preds, axis=0)\n",
    "    return majority_vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414566b",
   "metadata": {},
   "source": [
    "## 12. Evaluation Metrics\n",
    "### Binary Confusion Matrix\n",
    "In a confusion matrix, the terms True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) describe the classification performance for binary classification. \n",
    "\n",
    "|                     | Predicted Negative  | Predicted Positive  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Negative** | True Negative (TN)  | False Positive (FP) |\n",
    "| **Actual Positive** | False Negative (FN) | True Positive (TP)  |\n",
    "\n",
    "\n",
    "1. True Positive (TP): The number of instances correctly predicted as positive (e.g., a disease correctly identified).\n",
    "\n",
    "2. True Negative (TN): The number of instances correctly predicted as negative (e.g., no disease correctly identified).\n",
    "\n",
    "3. False Positive (FP): The number of instances incorrectly predicted as positive (e.g., predicting disease when there isn't any).\n",
    "\n",
    "4. False Negative (FN): The number of instances incorrectly predicted as negative (e.g., missing a disease when it exists).\n",
    "\n",
    "### Multi-Class Confusion Matrix\n",
    "For multi-class classification, the concepts can be extended by treating one class as the \"positive\" class and all others as \"negative\" classes in a one-vs-all approach. Rows represent the actual classes (true labels), and columns represent the predicted classes. For a class $C$,\n",
    "1. True Positive (TP): The count in the diagonal cell corresponding to class $C$ ($\\text{matrix} [C][C]$).\n",
    "2. False Positive (FP): The sum of the column for class $C$, excluding the diagonal ($\\sum(\\text{matrix} [:, C]) - \\text{matrix} [C][C]$).\n",
    "3. False Negative (FN): The sum of the row for class $C$, excluding the diagonal ($\\sum(\\text{matrix} [C, :]) - \\text{matrix} [C][C]$).\n",
    "4. True Negative (TN): All other cells not in the row or column for class $C$ ($\\text{total} - (FP + FN + TP)$).\n",
    "\n",
    "|                  | Predicted Class 0 | Predicted Class 1 | Predicted Class 2 |\n",
    "| ---------------- | ----------------- | ----------------- | ----------------- |\n",
    "| **True Class 0** | 5                 | 2                 | 0                 |\n",
    "| **True Class 1** | 1                 | 6                 | 1                 |\n",
    "| **True Class 2** | 0                 | 2                 | 7                 |\n",
    "\n",
    "\n",
    "For Class 0:\n",
    "- TP = 5 (diagonal element for Class 0)\n",
    "- FP = 1 (sum of column 0 minus TP: 1 + 0)\n",
    "- FN = 2 (sum of row 0 minus TP: 2 + 0)\n",
    "- TN = 6 + 1 + 2 + 7 = 16 (all other cells not in row 0 or column 0)\n",
    "\n",
    "For Class 1:\n",
    "- TP = 6 (diagonal element for Class 1)\n",
    "- FP = 4 (sum of column 1 minus TP: 2 + 2)\n",
    "- FN = 2 (sum of row 1 minus TP: 1 + 1)\n",
    "- TN = 5 + 0 + 0 + 7 = 12 (all other cells not in row 1 or column 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1c4246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true: NDArray[np.int64], y_pred: NDArray[np.int64],\n",
    "                     class_names: List[str] = None) -> Tuple[NDArray[np.int64], List[str]]:\n",
    "    \"\"\"\n",
    "    Calculate the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: \n",
    "        - Confusion matrix.\n",
    "        - List of class names.\n",
    "    \"\"\"\n",
    "    # Encode labels as integers\n",
    "    unique_classes = np.unique(y_true)\n",
    "    if class_names is None:\n",
    "        class_names = [str(cls) for cls in unique_classes]\n",
    "    class_to_index = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "    n_classes = len(unique_classes)\n",
    "    matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        true_idx = class_to_index[true]\n",
    "        pred_idx = class_to_index[pred]\n",
    "        matrix[true_idx][pred_idx] += 1\n",
    "\n",
    "    return matrix, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24b349",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy is the most common evaluation metric for classification problems, representing the percentage of correct predictions out of total predictions. It provides a simple measure of how often the classifier makes correct predictions across all classes.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} = \\dfrac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Samples}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cab1ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true: NDArray[np.int64],\n",
    "             y_pred: NDArray[np.int64]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions by comparing true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth target values. Contains the actual class labels for each sample.\n",
    "        y_pred: Estimated target as returned by a classifier. Contains the predicted class labels for each sample.\n",
    "    Returns:\n",
    "        Classification accuracy (0.0 to 1.0).\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d201d96",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions made by the classifier.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Precision} = \\dfrac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "296fa7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true: NDArray[np.int64], y_pred: NDArray[np.int64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate precision for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Precision values for each class.\n",
    "    \"\"\"\n",
    "    cm, _ = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / (np.sum(cm, axis=0) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1485c1",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall measures the proportion of true positive predications out of all actual positive cases.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Recall} = \\dfrac{\\text{True Positives (TP)} }{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cac05de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true: NDArray[np.int64], y_pred: NDArray[np.int64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate recall for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Recall values for each class.\n",
    "    \"\"\"\n",
    "    cm, _ = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / (np.sum(cm, axis=1) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717903b",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "The F1-Score is the harmonic mean of precision and recall.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{F1-Score} = 2 \\times \\dfrac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7b04ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true: NDArray[np.int64], y_pred: NDArray[np.int64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate F1-score for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        F1-scores for each class.\n",
    "    \"\"\"\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "37276062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true: NDArray[np.int64], y_pred: NDArray[np.int64], class_names: List[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Print evaluation metrics including accuracy, precision, recall, and F1-score for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "    \"\"\"\n",
    "    cm, class_names = confusion_matrix(y_true, y_pred, class_names)\n",
    "    acc = accuracy(y_true, y_pred)\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    # print(\"Class\\tPrecision\\tRecall\\tF1-Score\")\n",
    "    # for i, class_name in enumerate(class_names):\n",
    "    #     print(f\"{class_name}\\t{prec[i]:.4f}\\t\\t{rec[i]:.4f}\\t{f1[i]:.4f}\")\n",
    "    return acc, np.mean(prec), np.mean(rec), np.mean(f1), cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a3148",
   "metadata": {},
   "source": [
    "## 13. Encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44ee96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRandomForest:\n",
    "    def __init__(self, n_estimators: int = 100, max_depth: int = 15, min_samples_leaf: int = 1, metric: str = 'gini',\n",
    "                 max_features: Optional[int] = None, random_state: Optional[int] = None, n_jobs: int = -1) -> None:\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.metric = metric\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.forest = None\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _gini(self, y: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the Gini impurity for a set of labels.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        return 1 - np.sum(proportions ** 2)\n",
    "\n",
    "    def _entropy(self, y: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the entropy for a set of labels.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        proportions = proportions[proportions > 0]  # Avoid log(0)\n",
    "        return -np.sum(proportions * np.log2(proportions))\n",
    "\n",
    "    def _information_gain(self, y: pd.Series, y_left: pd.Series, y_right: pd.Series) -> float:\n",
    "        \"\"\"\n",
    "        Compute the information gain of a split.\n",
    "\n",
    "        Args:\n",
    "            y: Series of the parent node.\n",
    "            y_left: Series of the left child node.\n",
    "            y_right: Series of the right child node.\n",
    "\n",
    "        Returns:\n",
    "            Information gain from the split.\n",
    "        \"\"\"\n",
    "        if self.metric == 'gini':\n",
    "            parent_metric = self._gini(y)\n",
    "            left_metric = self._gini(y_left)\n",
    "            right_metric = self._gini(y_right)\n",
    "        else:  # metric == \"entropy\"\n",
    "            parent_metric = self._entropy(y)\n",
    "            left_metric = self._entropy(y_left)\n",
    "            right_metric = self._entropy(y_right)\n",
    "\n",
    "        weighted_metric: float = (\n",
    "            len(y_left) / len(y) * left_metric\n",
    "            + len(y_right) / len(y) * right_metric\n",
    "        )\n",
    "        return parent_metric - weighted_metric\n",
    "\n",
    "    def _bootstrap_sample(self, X: pd.DataFrame, y: pd.Series, n_samples: Optional[int] = None,\n",
    "                          random_state: Optional[int] = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Generate a bootstrap sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            X: Input features.\n",
    "            y: Target labels.\n",
    "            n_samples: Samples to draw (default: dataset size).\n",
    "            random_state: Random seed.\n",
    "\n",
    "        Returns:\n",
    "            Bootstrapped (X, y) tuple.\n",
    "        \"\"\"\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        if n_samples is None:\n",
    "            n_samples = len(X)\n",
    "        indices = np.random.choice(len(X), size=n_samples, replace=True)\n",
    "        return X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "    def _best_split(self, X: NDArray[np.float64], y: NDArray[np.int16]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Find the best split for a dataset.\n",
    "\n",
    "        Args:\n",
    "            X: Input features (DataFrame of shape [n_samples, total_n_features]).\n",
    "            y: Labels (Series of shape [n_samples]).\n",
    "            metric: Splitting criterion, either \"gini\" or \"entropy\". Defaults to 'gini'.\n",
    "            feature_names: List of feature names. If None, indices are used. Defaults to None.\n",
    "            max_features: Number of features to consider at each split. None(√total_n_features) or int(<=total_n_features). Defaults to None.\n",
    "        Returns:\n",
    "            Dictionary containing the best split with keys:\n",
    "                - 'feature_index' : Index of the feature used for the split.\n",
    "                - 'feature_name': Name or index of the feature.\n",
    "                - 'threshold' : Threshold value for the split.\n",
    "        \"\"\"\n",
    "\n",
    "        best_info_gain = float('-inf')\n",
    "        best_split = None\n",
    "        total_n_features = X.shape[1]\n",
    "\n",
    "        if isinstance(self.max_features, int):  # if max_features is int\n",
    "            selected_n_features = self.max_features if self.max_features <= total_n_features else total_n_features\n",
    "        else:  # Default = √total_n_features\n",
    "            selected_n_features = int(np.sqrt(total_n_features))\n",
    "\n",
    "        selected_features_idx = np.random.choice(\n",
    "            a=total_n_features, size=selected_n_features, replace=False)\n",
    "\n",
    "        # Iterate over randomly selected features.\n",
    "        for feature in selected_features_idx:\n",
    "            # Iterate over all unique thresholds for each random feature.\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                # Split the data into left and right subsets based on the threshold.\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = X[:, feature] > threshold\n",
    "\n",
    "                # Skip invalid splits.\n",
    "                if sum(left_mask) < self.min_samples_leaf or sum(right_mask) < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                # Compute IG.\n",
    "                info_gain = self._information_gain(\n",
    "                    y, y[left_mask], y[right_mask])\n",
    "\n",
    "                # Update `best_info_gain` if `info_gain` > `best_info_gain`.\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = info_gain\n",
    "                    best_split = {\n",
    "                        'feature_index': int(feature),\n",
    "                        'feature_name': self.feature_names[feature] if self.feature_names is not None else feature,\n",
    "                        'threshold': float(threshold),\n",
    "                    }\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def _build_tree(self, X: pd.DataFrame, y: pd.Series, depth: int = 0) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Recursively build a decision tree.\n",
    "\n",
    "        Args:\n",
    "            X: Input features.\n",
    "            y: Target labels.\n",
    "            depth: Current tree depth.\n",
    "\n",
    "        Returns:\n",
    "            Tree structure dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        X_np = X.to_numpy() if hasattr(X, 'to_numpy') else np.array(X)\n",
    "        y_np = y.to_numpy().flatten() if hasattr(\n",
    "            y, 'to_numpy') else np.array(y).flatten()\n",
    "\n",
    "        # Stopping conditions\n",
    "        if len(np.unique(y_np)) == 1 or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return {'type': 'leaf', 'value': int(np.argmax(np.bincount(y_np)))}\n",
    "\n",
    "        if len(y) < self.min_samples_leaf:\n",
    "            return {'type': 'leaf', 'value': int(np.argmax(np.bincount(y_np)))}\n",
    "\n",
    "        # Find best split\n",
    "        split = self._best_split(X_np, y_np)\n",
    "        if not split:\n",
    "            return {'type': 'leaf', 'value': int(np.argmax(np.bincount(y_np)))}\n",
    "\n",
    "        # Apply split\n",
    "        feature_idx = split['feature_index']\n",
    "        left_mask = X_np[:, feature_idx] <= split['threshold']\n",
    "        right_mask = X_np[:, feature_idx] > split['threshold']\n",
    "\n",
    "        # Recursive tree building\n",
    "        left_tree = self._build_tree(\n",
    "            X.iloc[left_mask] if hasattr(X, 'iloc') else X[left_mask],\n",
    "            y.iloc[left_mask] if hasattr(y, 'iloc') else y[left_mask],\n",
    "            depth + 1\n",
    "        )\n",
    "        right_tree = self._build_tree(\n",
    "            X.iloc[right_mask] if hasattr(X, 'iloc') else X[right_mask],\n",
    "            y.iloc[right_mask] if hasattr(y, 'iloc') else y[right_mask],\n",
    "            depth + 1\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'type': 'node',\n",
    "            'feature': split['feature_name'],\n",
    "            'threshold': split['threshold'],\n",
    "            'left': left_tree,\n",
    "            'right': right_tree\n",
    "        }\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        \"\"\"\n",
    "        Train the random forest on input data.\n",
    "\n",
    "        Args:\n",
    "            X: Training features.\n",
    "            y: Training labels.\n",
    "        \"\"\"\n",
    "        # Store feature names\n",
    "        if hasattr(X, 'columns'):\n",
    "            self.feature_names = X.columns.tolist()\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        seeds = np.random.randint(0, 10000, size=self.n_estimators)\n",
    "\n",
    "        # Build trees in parallel\n",
    "        self.forest = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(self._build_single_tree)(X, y, seed)\n",
    "            for seed in seeds\n",
    "        )\n",
    "\n",
    "    def _build_single_tree(self, X: pd.DataFrame, y: pd.Series,\n",
    "                           seed: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Build a single decision tree with bootstrap sampling.\n",
    "        \"\"\"\n",
    "        X_boot, y_boot = self._bootstrap_sample(X, y, random_state=seed)\n",
    "        return self._build_tree(X_boot, y_boot)\n",
    "\n",
    "    def _traverse_tree(self, x: np.ndarray,\n",
    "                       tree: Dict[str, Any]) -> int:\n",
    "        \"\"\"\n",
    "        Traverse a tree to make a prediction for a single sample.\n",
    "\n",
    "        Args:\n",
    "            x: Input sample (1D array).\n",
    "            tree: Decision tree structure.\n",
    "\n",
    "        Returns:\n",
    "            Predicted label.\n",
    "        \"\"\"\n",
    "        if tree['type'] == 'leaf':\n",
    "            return tree['value']\n",
    "\n",
    "        # Resolve feature index\n",
    "        if self.feature_names is not None:\n",
    "            feature_index = self.feature_names.index(tree['feature'])\n",
    "        else:\n",
    "            feature_index = tree['feature']  # Assume integer index\n",
    "\n",
    "        if x[feature_index] <= tree['threshold']:\n",
    "            return self._traverse_tree(x, tree['left'])\n",
    "        else:\n",
    "            return self._traverse_tree(x, tree['right'])\n",
    "\n",
    "    def predict(self,\n",
    "                X: pd.DataFrame | NDArray[np.float64]) -> NDArray[np.int16]:\n",
    "        \"\"\"\n",
    "        Predict labels for input data using majority voting.\n",
    "\n",
    "        Args:\n",
    "            X: Input features (DataFrame or array)\n",
    "\n",
    "        Returns:\n",
    "            Predicted labels (1D array)\n",
    "        \"\"\"\n",
    "        if self.forest is None:\n",
    "            raise RuntimeError(\"Model not trained. Call fit() first.\")\n",
    "\n",
    "        # Convert to numpy array\n",
    "        X_np = X.to_numpy() if hasattr(X, 'to_numpy') else np.array(X)\n",
    "\n",
    "        # Single sample case\n",
    "        if len(X_np.shape) == 1:\n",
    "            tree_preds = [self._traverse_tree(\n",
    "                X_np, tree) for tree in self.forest]\n",
    "            majority_vote, _ = mode(tree_preds)\n",
    "            return majority_vote[0]\n",
    "\n",
    "        # Batch predictions\n",
    "        all_preds = np.zeros((len(self.forest), len(X_np)), dtype=int)\n",
    "        for i, tree in enumerate(self.forest):\n",
    "            all_preds[i] = [self._traverse_tree(x, tree) for x in X_np]\n",
    "\n",
    "        majority_vote, _ = mode(all_preds, axis=0)\n",
    "        return majority_vote.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "be639d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Custom): 0.9558\n",
      "Precision: (Custom) 0.9548\n",
      "Recall (Custom): 0.9502\n",
      "F1-Score (Custom): 0.9524\n",
      "Confusion Matrix (Custom):\n",
      "[[39  3]\n",
      " [ 2 69]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialise and train\n",
    "rf = CustomRandomForest(n_estimators=100, max_depth=15,\n",
    "                        n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "# Evaluate\n",
    "acc_custom, prec_custom, rec_custom, f1_custom, cm_custom = evaluate(\n",
    "    y_test, y_pred)\n",
    "print(f'Accuracy (Custom): {acc_custom:.4f}')\n",
    "print(f'Precision: (Custom) {prec_custom:.4f}')\n",
    "print(f'Recall (Custom): {rec_custom:.4f}')\n",
    "print(f'F1-Score (Custom): {f1_custom:.4f}')\n",
    "print(f'Confusion Matrix (Custom):\\n{cm_custom}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b719e8",
   "metadata": {},
   "source": [
    "## 14. Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0dbbd494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
