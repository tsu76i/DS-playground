{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1b6610",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f86ef6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71260664",
   "metadata": {},
   "source": [
    "Decision trees and regression trees are collectively referred to as **CART**, which stands for **Classification and Regression Trees**.\n",
    "\n",
    "- **Decision Trees** are used for classification tasks, where the target variable is *categorical*.\n",
    "\n",
    "- **Regression Trees** are used for regression tasks, where the target variable is *continuous*.\n",
    "\n",
    "CART is a popular algorithm that can handle both types of tasks by optimising for different criteria:\n",
    "\n",
    "- For classification, CART minimises classification error, Gini impurity, or entropy.\n",
    "\n",
    "- For regression, CART minimises the mean squared error (MSE) or mean absolute error (MAE).\n",
    "\n",
    "Both types of trees follow the same core idea of splitting the data based on conditions to create homogeneous subsets, but their objectives differ depending on the problem type.\n",
    "In this notebook, we will build a predictive model using Decision Trees on the breast cancer dataset from the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4981d62",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15b3cafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (569, 30)\n",
      "Target shape: (569,)\n",
      "Features: \n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "# data = datasets.load_iris()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Features: \\n{feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38a2c4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mean radius",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean texture",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean perimeter",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean area",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean smoothness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean compactness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean concavity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean concave points",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean symmetry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean fractal dimension",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "radius error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "texture error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "perimeter error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "area error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "smoothness error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "compactness error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "concavity error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "concave points error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "symmetry error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fractal dimension error",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst radius",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst texture",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst perimeter",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst area",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst smoothness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst compactness",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst concavity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst concave points",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst symmetry",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "worst fractal dimension",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "diagnosis",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "acf29d9f-d0eb-4b55-b2d1-b5435be3cc1c",
       "rows": [
        [
         "0",
         "17.99",
         "10.38",
         "122.8",
         "1001.0",
         "0.1184",
         "0.2776",
         "0.3001",
         "0.1471",
         "0.2419",
         "0.07871",
         "1.095",
         "0.9053",
         "8.589",
         "153.4",
         "0.006399",
         "0.04904",
         "0.05373",
         "0.01587",
         "0.03003",
         "0.006193",
         "25.38",
         "17.33",
         "184.6",
         "2019.0",
         "0.1622",
         "0.6656",
         "0.7119",
         "0.2654",
         "0.4601",
         "0.1189",
         "0"
        ],
        [
         "1",
         "20.57",
         "17.77",
         "132.9",
         "1326.0",
         "0.08474",
         "0.07864",
         "0.0869",
         "0.07017",
         "0.1812",
         "0.05667",
         "0.5435",
         "0.7339",
         "3.398",
         "74.08",
         "0.005225",
         "0.01308",
         "0.0186",
         "0.0134",
         "0.01389",
         "0.003532",
         "24.99",
         "23.41",
         "158.8",
         "1956.0",
         "0.1238",
         "0.1866",
         "0.2416",
         "0.186",
         "0.275",
         "0.08902",
         "0"
        ],
        [
         "2",
         "19.69",
         "21.25",
         "130.0",
         "1203.0",
         "0.1096",
         "0.1599",
         "0.1974",
         "0.1279",
         "0.2069",
         "0.05999",
         "0.7456",
         "0.7869",
         "4.585",
         "94.03",
         "0.00615",
         "0.04006",
         "0.03832",
         "0.02058",
         "0.0225",
         "0.004571",
         "23.57",
         "25.53",
         "152.5",
         "1709.0",
         "0.1444",
         "0.4245",
         "0.4504",
         "0.243",
         "0.3613",
         "0.08758",
         "0"
        ],
        [
         "3",
         "11.42",
         "20.38",
         "77.58",
         "386.1",
         "0.1425",
         "0.2839",
         "0.2414",
         "0.1052",
         "0.2597",
         "0.09744",
         "0.4956",
         "1.156",
         "3.445",
         "27.23",
         "0.00911",
         "0.07458",
         "0.05661",
         "0.01867",
         "0.05963",
         "0.009208",
         "14.91",
         "26.5",
         "98.87",
         "567.7",
         "0.2098",
         "0.8663",
         "0.6869",
         "0.2575",
         "0.6638",
         "0.173",
         "0"
        ],
        [
         "4",
         "20.29",
         "14.34",
         "135.1",
         "1297.0",
         "0.1003",
         "0.1328",
         "0.198",
         "0.1043",
         "0.1809",
         "0.05883",
         "0.7572",
         "0.7813",
         "5.438",
         "94.44",
         "0.01149",
         "0.02461",
         "0.05688",
         "0.01885",
         "0.01756",
         "0.005115",
         "22.54",
         "16.67",
         "152.2",
         "1575.0",
         "0.1374",
         "0.205",
         "0.4",
         "0.1625",
         "0.2364",
         "0.07678",
         "0"
        ]
       ],
       "shape": {
        "columns": 31,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  diagnosis  \n",
       "0          0.4601                  0.11890          0  \n",
       "1          0.2750                  0.08902          0  \n",
       "2          0.3613                  0.08758          0  \n",
       "3          0.6638                  0.17300          0  \n",
       "4          0.2364                  0.07678          0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=X, columns=feature_names)\n",
    "df['diagnosis'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ddcbee40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "diagnosis",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "aad8389e-72c4-4ac2-a7af-abe69cb9d5a0",
       "rows": [
        [
         "1",
         "357"
        ],
        [
         "0",
         "212"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "diagnosis\n",
       "1    357\n",
       "0    212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ba008",
   "metadata": {},
   "source": [
    "In this dataset, the features represent the characteristics of breast cancer (e.g., radius, texture, etc.), while the target is a boolean value indicating whether the tumour is benign (0) or malignant (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3a784a",
   "metadata": {},
   "source": [
    "## 2. Train Test Split\n",
    "Train test split is a fundamental model validation technique in machine learning. It divides a dataset into two separate portions: a **training set** used to train a model, and a **testing set** used to evaluate how well the model can perform on unseen data. \n",
    "\n",
    "The typical split ratio is 80% for training and 20% for testing, though this can vary (70/30 or 90/10 are also common). The key principle is that the test set must remain completely separated during model training process, and should never be used to make decisions about the model or tune parameters. \n",
    "\n",
    "The split is usually done randomly to ensure both sets are representative of the overall dataset, and many libraries (such as scikit-learn) provide build-in functions that handle this process automatically while maintaining proper randomisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec7e5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: np.array, y: np.array, test_size: float = 0.2,\n",
    "                     random_state: int = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Split arrays or matrices into random train and test subsets.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Input features, a 2D array with rows (samples) and columns (features).\n",
    "        y (np.array): Target values/labels, a 1D array with rows (samples).\n",
    "        test_size (float): Proportion of the dataset to include in the test split. Must be between 0.0 and 1.0. default = 0.2\n",
    "        random_state (int): Seed for the random number generator to ensure reproducible results. default = None\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        A tuple containing:\n",
    "            - X_train (np.ndarray): Training set features.\n",
    "            - X_test (np.ndarray): Testing set features.\n",
    "            - y_train (np.ndarray): Training set target values.\n",
    "            - y_test (np.ndarray): Testing set target values.\n",
    "    \"\"\"\n",
    "    # Set a random seed if it exists\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Create a list of numbers from 0 to len(X)\n",
    "    indices = np.arange(len(X))\n",
    "\n",
    "    # Shuffle the indices\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Define the size of our test data from len(X)\n",
    "    test_size = int(test_size * len(X))\n",
    "\n",
    "    # Generate indices for test and train data\n",
    "    test_indices: list[int] = indices[:test_size]\n",
    "    train_indices: list[int] = indices[test_size:]\n",
    "\n",
    "    # Return: X_train, X_test, y_train, y_test\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a0681",
   "metadata": {},
   "source": [
    "## 3. Gini Impurity and Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707da51",
   "metadata": {},
   "source": [
    "### Gini Impurity\n",
    "Gini impurity is a measure of the likelihood that a randomly chosen sample from a dataset will be incorrectly classified. It quantifies how impure a node is, with values ranging from $0$ (minimum impurity) to $0.5$ (maximum impurity) for binary classification. For multi-class problems, however, the maximum impurity occurs when all classes are equally probable, and the value depends on the number of classes. The formula for Gini impurity is:\n",
    "\n",
    "\\begin{align*}\n",
    "G = 1 - \\sum_{i=1}^{k} p_{i}^{2}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $k$: Number of classes.\n",
    "- $p_{i}$: Proportion of samples belonging to class $i$ in the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06703f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    proportions = np.bincount(y) / len(y)\n",
    "    return 1 - np.sum(proportions**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a4748",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "Entropy measures the amount of uncertainty or randomness in the data. It is based on information theory and represents the expected amount of information required to classify a sample. For binary classification ranges from $0$ (minimum entropy) to $1$ (maximum entropy). For $k$ classes, the range is from $0$ to $log_{2}(k)$. The formula for entropy is:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "H = - \\sum_{i=1}^{k} p_{i} log_{2}(p_{i})\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $k$: Number of classes.\n",
    "- $p_{i}$: Proportion of samples belonging to class $i$ in the node.\n",
    "\n",
    "Gini tends to split nodes based on the most frequent classes, while entropy provides a more nuanced measure especially in cases with many classes or highly imbalanced distributions. Both metrics provide similar results, but Gini is often preferred for computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "538ecb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    proportions = np.bincount(y) / len(y)\n",
    "    proportions = proportions[proportions > 0]  # Avoid log(0)\n",
    "    return -np.sum(proportions * np.log2(proportions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d316d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis\n",
      "1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b348c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Impurity: 0.46753\n",
      "Entropy: 0.95264\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gini Impurity: {gini(y):.5f}\")\n",
    "print(f\"Entropy: {entropy(y):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57153d6a",
   "metadata": {},
   "source": [
    "## 4. Information Gain\n",
    "Information Gain is a metric used to measure the effectiveness of a feature in splitting a dataset into subsets that are more pure concerning the target variable. It quantifies the reduction in entropy or Gini impurity, and a higher information gain indicates a better feature for making splits.\n",
    "\n",
    "\\begin{align*}\n",
    "IG(S, A) = H(S) - \\sum_{i=1}^{n} \\dfrac{|S_i|}{|S|}H(S_{i})\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $H(S)$: Entropy (or Geni) of the original dataset $S$.\n",
    "- $S_{i}$: Subset of $S$ created by splitting on feature $A$ for the $i_{th}$ value or range of the feature.\n",
    "- $\\dfrac{|S_i|}{|S|}$: Proportion of samples in subset $S_{i}$.\n",
    "- $H(S_{i})$: Entropy (or Geni) of subset $S_{i}$.\n",
    "\n",
    "\n",
    "\n",
    "The following `information_gain` function calculates the difference between the metric for the parent node and the weighted average of the metrics for the child nodes (left and right splits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f306b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y, y_left, y_right, metric=\"gini\"):\n",
    "    \"\"\"\n",
    "    Calculate the information gain of a split.\n",
    "\n",
    "    Args:\n",
    "        y (array-like): Labels of the parent node.\n",
    "        y_left (array-like): Labels of the left child node after the split.\n",
    "        y_right (array-like): Labels of the right child node after the split.\n",
    "        metric (str, optional): Splitting criterion, either \"gini\" or \"entropy\". Defaults to \"gini\".\n",
    "\n",
    "    Returns:\n",
    "        float: Information gain resulting from the split.\n",
    "    \"\"\"\n",
    "    if metric == \"gini\":\n",
    "        parent_metric = gini(y)\n",
    "        left_metric = gini(y_left)\n",
    "        right_metric = gini(y_right)\n",
    "    else:  # metric == \"entropy\"\n",
    "        parent_metric = entropy(y)\n",
    "        left_metric = entropy(y_left)\n",
    "        right_metric = entropy(y_right)\n",
    "\n",
    "    weighted_metric = (\n",
    "        len(y_left) / len(y) * left_metric\n",
    "        + len(y_right) / len(y) * right_metric\n",
    "    )\n",
    "    return parent_metric - weighted_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc2d61",
   "metadata": {},
   "source": [
    "## 5. Find the Best Split\n",
    "This function identifies the best feature and threshold to split the data using the specified metric (Gini or Entropy).\n",
    "\n",
    "Steps are:\n",
    "\n",
    "1. Loop through all features.\n",
    "\n",
    "2. For each feature, iterate over all unique thresholds.\n",
    "\n",
    "3. Split the data into left and right subsets based on the threshold (skip invalid ones).\n",
    "\n",
    "4. Compute the Gini/Entropy for both subsets and calculate Information Gain.\n",
    "\n",
    "5. If the newly computed `info_gain` > `best_info_gain`, then update `best_info_gain` with the new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "59caece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X, y, metric=\"gini\", feature_names=None):\n",
    "    \"\"\"\n",
    "    Find the best split for a dataset.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Input features (2D array of shape [n_samples, n_features]).\n",
    "        y (array-like): Labels (1D array of shape [n_samples]).\n",
    "        metric (str, optional): Splitting criterion, either \"gini\" or \"entropy\". Defaults to \"gini\".\n",
    "        feature_names (list, optional): List of feature names. If None, indices are used. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the best split with keys:\n",
    "              - 'feature_index' (int): Index of the feature used for the split.\n",
    "              - 'feature_name' (str/int): Name or index of the feature.\n",
    "              - 'threshold' (float): Threshold value for the split.\n",
    "    \"\"\"\n",
    "    best_info_gain = float(\"-inf\")\n",
    "    best_split = None\n",
    "    n_features = X.shape[1]\n",
    "\n",
    "    # Iterate over all features.\n",
    "    for feature in range(n_features):\n",
    "        # Iterate over all unique thresholds for each feature.\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            # Split the data into left and right subsets based on the threshold.\n",
    "            left_mask = X[:, feature] <= threshold\n",
    "            right_mask = X[:, feature] > threshold\n",
    "\n",
    "            # Skip invalid splits.\n",
    "            if sum(left_mask) == 0 or sum(right_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            # Compute IG.\n",
    "            info_gain = information_gain(\n",
    "                y, y[left_mask], y[right_mask], metric)\n",
    "\n",
    "            # Update `best_info_gain` if `info_gain` > `best_info_gain`.\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_split = {\n",
    "                    \"feature_index\": feature,\n",
    "                    \"feature_name\": feature_names[feature] if feature_names is not None else feature,\n",
    "                    \"threshold\": threshold,\n",
    "                }\n",
    "\n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0163ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Split: {'feature_index': 20, 'feature_name': np.str_('worst radius'), 'threshold': np.float64(16.77)}\n"
     ]
    }
   ],
   "source": [
    "split = best_split(X, y, metric=\"gini\", feature_names=feature_names)\n",
    "print(\"Best Split:\", split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a73cf2",
   "metadata": {},
   "source": [
    "## 6. Build Tree\n",
    "This function resursively creates the tree structure as a nested dictionary with conditions (`feature` and `threshold`) and leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0dcfceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X, y, max_depth=None, depth=0, metric=\"gini\", feature_names=None):\n",
    "    \"\"\"\n",
    "    Build a decision tree using recursive splitting.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Input features (2D array of shape [n_samples, n_features]).\n",
    "        y (array-like): Labels (1D array of shape [n_samples]).\n",
    "        max_depth (int, optional): Maximum depth of the tree. Defaults to None (unlimited depth).\n",
    "        depth (int, optional): Current depth of the tree. Used internally for recursion. Defaults to 0.\n",
    "        metric (str, optional): Splitting criterion, either \"gini\" or \"entropy\". Defaults to \"gini\".\n",
    "        feature_names (list, optional): List of feature names. If None, indices are used. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: Nested dictionary representing the tree structure.\n",
    "              Nodes contain keys: 'type', 'feature', 'threshold', 'left', 'right'.\n",
    "              Leaf nodes contain keys: 'type', 'value'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Stop the recursion if all labels are identical or the maximum depth is reached.\n",
    "    if len(set(y)) == 1 or (max_depth is not None and depth == max_depth):\n",
    "        return {\"type\": \"leaf\", \"value\": np.argmax(np.bincount(y))}\n",
    "\n",
    "    # Find the best split.\n",
    "    split = best_split(X, y, metric, feature_names)\n",
    "    if not split:\n",
    "        return {\"type\": \"leaf\", \"value\": np.argmax(np.bincount(y))}\n",
    "\n",
    "    # Split the data into left and right subsets.\n",
    "    # Use feature_index for calculations.\n",
    "    left_mask = X[:, split[\"feature_index\"]] <= split[\"threshold\"]\n",
    "    right_mask = X[:, split[\"feature_index\"]] > split[\"threshold\"]\n",
    "\n",
    "    # Recursively build the left and right subtrees.\n",
    "    left_tree = build_tree(X[left_mask], y[left_mask],\n",
    "                           max_depth, depth + 1, metric, feature_names)\n",
    "    right_tree = build_tree(X[right_mask], y[right_mask],\n",
    "                            max_depth, depth + 1, metric, feature_names)\n",
    "\n",
    "    # Return the tree structure as a nested dictionary.\n",
    "    return {\n",
    "        \"type\": \"node\",\n",
    "        \"feature\": split[\"feature_name\"],\n",
    "        \"threshold\": split[\"threshold\"],\n",
    "        \"left\": left_tree,\n",
    "        \"right\": right_tree,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23418a",
   "metadata": {},
   "source": [
    "## 7. Traverse the Tree For Prediction\n",
    "This function traverses the tree to make predictions by following the tree from the root to a leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c11d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_tree(x, tree, feature_names=None):\n",
    "    \"\"\"\n",
    "    Traverse a decision tree to make a prediction for a single sample.\n",
    "\n",
    "    Args:\n",
    "        x (array-like): Single sample (1D array of features).\n",
    "        tree (dict): Decision tree structure.\n",
    "        feature_names (list, optional): List of feature names. Needed for name-to-index mapping. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted label.\n",
    "    \"\"\"\n",
    "    if tree[\"type\"] == \"leaf\":\n",
    "        return tree[\"value\"]\n",
    "\n",
    "    # Resolve feature index if feature_names is provided\n",
    "    feature_index = feature_names.index(\n",
    "        tree[\"feature\"]) if feature_names is not None else tree[\"feature\"]\n",
    "\n",
    "    if x[feature_index] <= tree[\"threshold\"]:\n",
    "        return traverse_tree(x, tree[\"left\"], feature_names)\n",
    "    else:\n",
    "        return traverse_tree(x, tree[\"right\"], feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b8b41",
   "metadata": {},
   "source": [
    "## 8. Accuracy\n",
    "We will also compute accuracy to evaluate our custom decision trees model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b89f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb2cef",
   "metadata": {},
   "source": [
    "## 9. Prediction\n",
    "This function predicts labels for all samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2cbdf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, tree, feature_names=None):\n",
    "    \"\"\"\n",
    "    Predict labels for the given dataset using a decision tree.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Input features (2D array for multiple samples or 1D array for a single sample).\n",
    "        tree (dict): Decision tree structure.\n",
    "        feature_names (list, optional): List of feature names. Needed for name-to-index mapping. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        array-like: Predicted labels (1D array for multiple samples or a single label for one sample).\n",
    "    \"\"\"\n",
    "    if len(X.shape) == 1:  # If a single sample is provided\n",
    "        return traverse_tree(X, tree, feature_names)\n",
    "    return np.array([traverse_tree(x, tree, feature_names) for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3c1e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': 'worst radius',\n",
      " 'left': {'feature': 'worst concave points',\n",
      "          'left': {'feature': 'radius error',\n",
      "                   'left': {'type': 'leaf', 'value': np.int64(1)},\n",
      "                   'right': {'type': 'leaf', 'value': np.int64(0)},\n",
      "                   'threshold': np.float64(0.8811),\n",
      "                   'type': 'node'},\n",
      "          'right': {'feature': 'worst texture',\n",
      "                    'left': {'type': 'leaf', 'value': np.int64(1)},\n",
      "                    'right': {'type': 'leaf', 'value': np.int64(0)},\n",
      "                    'threshold': np.float64(25.5),\n",
      "                    'type': 'node'},\n",
      "          'threshold': np.float64(0.1357),\n",
      "          'type': 'node'},\n",
      " 'right': {'feature': 'mean texture',\n",
      "           'left': {'feature': 'mean concave points',\n",
      "                    'left': {'type': 'leaf', 'value': np.int64(1)},\n",
      "                    'right': {'type': 'leaf', 'value': np.int64(0)},\n",
      "                    'threshold': np.float64(0.06211),\n",
      "                    'type': 'node'},\n",
      "           'right': {'feature': 'worst smoothness',\n",
      "                     'left': {'type': 'leaf', 'value': np.int64(1)},\n",
      "                     'right': {'type': 'leaf', 'value': np.int64(0)},\n",
      "                     'threshold': np.float64(0.08774),\n",
      "                     'type': 'node'},\n",
      "           'threshold': np.float64(16.07),\n",
      "           'type': 'node'},\n",
      " 'threshold': np.float64(16.77),\n",
      " 'type': 'node'}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pprint\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names.tolist()\n",
    "\n",
    "# Build the tree using Gini impurity\n",
    "tree_gini = build_tree(X, y, max_depth=3, metric=\"gini\",\n",
    "                       feature_names=feature_names)\n",
    "\n",
    "# Display the tree\n",
    "pprint.pprint(tree_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182341be",
   "metadata": {},
   "source": [
    "## 10. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0dd028a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9647058823529412\n",
      "Predicted label for single sample: 1\n",
      "Actual label for single sample: 1\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build the tree on training data\n",
    "tree_gini = build_tree(X_train, y_train, max_depth=3,\n",
    "                       metric=\"gini\", feature_names=feature_names)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = predict(X_test, tree_gini, feature_names=feature_names)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Predict a single sample\n",
    "single_sample = X_test[0]\n",
    "single_prediction = predict(\n",
    "    single_sample, tree_gini, feature_names=feature_names)\n",
    "print(\"Predicted label for single sample:\", single_prediction)\n",
    "print(\"Actual label for single sample:\", y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd49d9",
   "metadata": {},
   "source": [
    "## 11. Encapsulation\n",
    "\n",
    "To encapsulate the codes above, it will be a better idea to divide them into two different classes:\n",
    "1. `CustomDecisionTree` containing the core logic for building, traversing and predicting using the decision tree.\n",
    "2. `Node` representing a single node in the decision tree, either internal node or leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c597b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A class representing a node in the decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node_type, value=None, feature=None, threshold=None, left=None, right=None):\n",
    "        \"\"\"\n",
    "        Initialise a Node instance.\n",
    "\n",
    "\n",
    "        Args:\n",
    "            type (str): Type of the node (\"leaf\" or \"node\").\n",
    "            value (int or None): Predicted label for leaf nodes. None for internal nodes.\n",
    "            feature (int or None): Index of the feature for internal nodes. None for leaf nodes.\n",
    "            threshold (float or None): Threshold value for internal nodes. None for leaf nodes.\n",
    "            left (Node or None): Left child node. None for leaf nodes.\n",
    "            right (Node or None): Right child node. None for leaf nodes.\n",
    "        \"\"\"\n",
    "        self.type = node_type  # \"leaf\" or \"node\"\n",
    "        self.value = value  # For leaf nodes\n",
    "        self.feature = feature  # For internal nodes\n",
    "        self.threshold = threshold  # For internal nodes\n",
    "        self.left = left  # Left child\n",
    "        self.right = right  # Right child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1bcf9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDecisionTree:\n",
    "    \"\"\"\n",
    "    A class representing the decision tree model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=None, metric='gini'):\n",
    "        \"\"\"\n",
    "        Initialise a CustomDecisionTree instance.\n",
    "\n",
    "        Args:\n",
    "            max_depth (int or None): Maximum depth of the tree. None for unlimited depth.\n",
    "            metric (str): Splitting criterion, either \"gini\" or \"entropy\".\n",
    "            root (Node): Root node of the decision tree.\n",
    "            feature_names (list or None): List of feature names. None if not provided.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.metric = metric\n",
    "        self.root = None\n",
    "        self.feature_names = None\n",
    "\n",
    "    def gini(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the Gini impurity.\n",
    "\n",
    "        Args:\n",
    "            y (array-like): Array of labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Gini impurity.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        return 1 - np.sum(proportions ** 2)\n",
    "\n",
    "    def entropy(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the entropy.\n",
    "\n",
    "        Args:\n",
    "            y (array-like): Array of labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Entropy value.\n",
    "        \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        proportions = proportions[proportions > 0]  # Avoid log(0)\n",
    "        return -np.sum(proportions * np.log2(proportions))\n",
    "\n",
    "    def information_gain(self, y, y_left, y_right):\n",
    "        \"\"\"\n",
    "        Compute the information gain of a split.\n",
    "\n",
    "        Args:\n",
    "            y (array-like): Labels of the parent node.\n",
    "            y_left (array-like): Labels of the left child node.\n",
    "            y_right (array-like): Labels of the right child node.\n",
    "\n",
    "        Returns:\n",
    "            float: Information gain from the split.\n",
    "        \"\"\"\n",
    "        if self.metric == \"gini\":\n",
    "            parent_metric = self.gini(y)\n",
    "            left_metric = self.gini(y_left)\n",
    "            right_metric = self.gini(y_right)\n",
    "        else:  # metric == \"entropy\"\n",
    "            parent_metric = self.entropy(y)\n",
    "            left_metric = self.entropy(y_left)\n",
    "            right_metric = self.entropy(y_right)\n",
    "\n",
    "        weighted_metric = (\n",
    "            len(y_left) / len(y) * left_metric\n",
    "            + len(y_right) / len(y) * right_metric\n",
    "        )\n",
    "        return parent_metric - weighted_metric\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split the dataset.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Input features.\n",
    "            y (array-like): Labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: Best split details with keys 'feature_index' and 'threshold'.\n",
    "        \"\"\"\n",
    "        best_info_gain = float(\"-inf\")\n",
    "        best_split = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Iterate over all features.\n",
    "        for feature in range(n_features):\n",
    "            # Iterate over all unique thresholds for each feature.\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                # Split the data into left and right subsets based on the threshold.\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = X[:, feature] > threshold\n",
    "\n",
    "                # Skip invalid splits.\n",
    "                if sum(left_mask) == 0 or sum(right_mask) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Compute IG.\n",
    "                info_gain = self.information_gain(\n",
    "                    y, y[left_mask], y[right_mask])\n",
    "\n",
    "                # Update `best_info_gain` if `info_gain` > `best_info_gain`.\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = info_gain\n",
    "                    best_split = {\n",
    "                        \"feature_index\": feature,\n",
    "                        \"threshold\": threshold,\n",
    "                    }\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Build the decision tree recursively.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Input features.\n",
    "            y (array-like): Labels.\n",
    "            depth (int): Current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "            Node: Root node of the decision tree.\n",
    "        \"\"\"\n",
    "        # Stop recursion if all labels are identical or max depth is reached\n",
    "        if len(set(y)) == 1 or (self.max_depth is not None and depth == self.max_depth):\n",
    "            return Node(node_type=\"leaf\", value=np.argmax(np.bincount(y)))\n",
    "\n",
    "        # Find the best split\n",
    "        split = self.best_split(X, y)\n",
    "        if not split:\n",
    "            return Node(node_type=\"leaf\", value=np.argmax(np.bincount(y)))\n",
    "\n",
    "        # Split the data\n",
    "        left_mask = X[:, split[\"feature_index\"]] <= split[\"threshold\"]\n",
    "        right_mask = X[:, split[\"feature_index\"]] > split[\"threshold\"]\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left_tree = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_tree = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        # Store the feature index directly for easier traversal\n",
    "        feature_index = split[\"feature_index\"]\n",
    "\n",
    "        return Node(node_type=\"node\", feature=feature_index, threshold=split[\"threshold\"], left=left_tree, right=right_tree)\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit the decision tree model to the given data.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Input features.\n",
    "            y (array-like): Labels.\n",
    "            feature_names (list, optional): Names of the features. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Traverse the decision tree to make a prediction for a single sample.\n",
    "\n",
    "        Args:\n",
    "            x (array-like): Single sample.\n",
    "            node (Node): Current node.\n",
    "\n",
    "        Returns:\n",
    "            int: Predicted label.\n",
    "        \"\"\"\n",
    "        if node.type == \"leaf\":\n",
    "            return node.value\n",
    "\n",
    "        # node.feature is now the feature index\n",
    "        feature_index = node.feature\n",
    "        if x[feature_index] <= node.threshold:\n",
    "            return self.traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self.traverse_tree(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels for the given dataset.\n",
    "\n",
    "        Args:\n",
    "            X (array-like): Input features.\n",
    "\n",
    "        Returns:\n",
    "            array-like: Predicted labels.\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 1:\n",
    "            return self.traverse_tree(X, self.root)\n",
    "        return np.array([self.traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the predictions.\n",
    "\n",
    "        Args:\n",
    "            y_true (array-like): True labels.\n",
    "            y_pred (array-like): Predicted labels.\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy score.\n",
    "        \"\"\"\n",
    "        return np.mean(y_true == y_pred)\n",
    "\n",
    "    def print_tree(self, node=None, depth=0, prefix=\"Root: \"):\n",
    "        \"\"\"\n",
    "        Print the tree structure in a readable format.\n",
    "\n",
    "        Args:\n",
    "            node (Node, optional): Current node. Defaults to the root node.\n",
    "            depth (int, optional): Current depth. Defaults to 0.\n",
    "            prefix (str, optional): Prefix for the current node. Defaults to \"Root: \".\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "\n",
    "        if node.type == \"leaf\":\n",
    "            print(\"  \" * depth + prefix + f\"Predict {node.value}\")\n",
    "        else:\n",
    "            feature_name = self.feature_names[\n",
    "                node.feature] if self.feature_names is not None else f\"Feature_{node.feature}\"\n",
    "            print(\"  \" * depth + prefix +\n",
    "                  f\"{feature_name} <= {node.threshold:.4f}\")\n",
    "            if node.left:\n",
    "                self.print_tree(node.left, depth + 1, \"â”œâ”€ True: \")\n",
    "            if node.right:\n",
    "                self.print_tree(node.right, depth + 1, \"â””â”€ False: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c935f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9647\n",
      "Predicted: 1, Actual: 1\n",
      "\n",
      "Decision Tree Structure:\n",
      "Root: mean concave points <= 0.0507\n",
      "  â”œâ”€ True: worst radius <= 16.7700\n",
      "    â”œâ”€ True: radius error <= 0.6061\n",
      "      â”œâ”€ True: Predict 1\n",
      "      â””â”€ False: Predict 0\n",
      "    â””â”€ False: mean texture <= 15.7000\n",
      "      â”œâ”€ True: Predict 1\n",
      "      â””â”€ False: Predict 0\n",
      "  â””â”€ False: worst texture <= 19.4900\n",
      "    â”œâ”€ True: mean concave points <= 0.0853\n",
      "      â”œâ”€ True: Predict 1\n",
      "      â””â”€ False: Predict 0\n",
      "    â””â”€ False: worst area <= 709.0000\n",
      "      â”œâ”€ True: Predict 1\n",
      "      â””â”€ False: Predict 0\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the decision tree\n",
    "tree = CustomDecisionTree(max_depth=3, metric=\"gini\")\n",
    "tree.fit(X_train, y_train, feature_names)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = tree.accuracy(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Single prediction\n",
    "sample = X_test[0]\n",
    "single_prediction = tree.predict(sample)\n",
    "print(f\"Predicted: {single_prediction}, Actual: {y_test[0]}\")\n",
    "\n",
    "# Print the tree structure\n",
    "print(\"\\nDecision Tree Structure:\")\n",
    "tree.print_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
