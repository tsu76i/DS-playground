{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926943d6",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Loading Data](#2-loading-data)\n",
    "3. [CNN Architecture](#3-cnn-architecture)\n",
    "    - [Convolutional Layer](#convolutional-layer)\n",
    "    - [Pooling Layer](#pooling-layer)\n",
    "    - [Fully Connected Layer](#fully-connected-layer)\n",
    "4. [Flatten Layer](#4-flatten-layer)\n",
    "5. [Activation Function](#5-activation-function)\n",
    "    - [Rectified Linear Unit (ReLU)](#rectified-linear-unit-relu)\n",
    "    - [ReLU Derivative](#relu-derivative)\n",
    "6. [Output Layer](#6-output-layer)\n",
    "7. [Loss Function](#7-loss-function)\n",
    "    - [Cross-Entropy Loss](#cross-entropy-loss)\n",
    "8. [Evaluation Metric](#8-evaluation-metric)\n",
    "    - [Accuracy](#accuracy)\n",
    "9. [CNN Model](#9-cnn-model)\n",
    "    - [Architecture](#architecture)\n",
    "10. [Training and Evaluation](#10-training-and-evaluation)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f191d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from numpy.typing import NDArray\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6e100",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "A Convolutional Neural Network (CNN) is a specialised deep learning architecture designed to process and interpret visual data, such as images. CNNs are the backbone of modern image recognition, object detection, and other computer vision tasks.\n",
    "\n",
    "## 2. Loading Data\n",
    "The MNIST dataset (handwritten digits) is loaded, normalised to values between 0 and 1 then reshaped to include a channel dimension (for grayscale images, the value is 1). Labels are one-hot encoded for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb37500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "images, labels = mnist.data.reshape(-1, 28, 28), mnist.target.astype(np.uint8)\n",
    "\n",
    "# Standard train/test split = 80%:20%\n",
    "split_idx = int(len(images)*0.8)\n",
    "X_train, y_train = images[:split_idx], labels[:split_idx]\n",
    "X_test, y_test = images[split_idx:], labels[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41841071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(f'Label: {labels[0]}')\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb2199",
   "metadata": {},
   "source": [
    "- `image` -> Normalised (values between $0$ and $1$).\n",
    "- `labels` -> One-hot encoded ($5$ will be encoded as $[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e7b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_data(images: NDArray[np.int64], labels: NDArray[np.int64]) -> Tuple[NDArray[np.float64], NDArray[np.int64]]:\n",
    "    \"\"\"\n",
    "    Preprocess image and label data for neural network training.\n",
    "\n",
    "    Performs:\n",
    "    1. Image normalisation (0-255 -> 0.0-1.0).\n",
    "    2. Channel dimension addition.\n",
    "    3. One-hot encoding of labels.\n",
    "\n",
    "    Args:\n",
    "        images: Input image array of shape (n_samples, height, width).\n",
    "        labels: Integer label array of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - Processed images: shape (n_samples, height, width, 1).\n",
    "        - One-hot encoded labels: shape (n_samples, num_classes).\n",
    "    \"\"\"\n",
    "    images = images.astype(float) / 255.0\n",
    "    images = np.expand_dims(images, axis=-1)  # Add channel dimension\n",
    "    num_classes = len(np.unique(labels))\n",
    "    labels = np.eye(num_classes)[labels]\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "X_train, y_train = preprocess_data(X_train, y_train)\n",
    "X_test, y_test = preprocess_data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e666907",
   "metadata": {},
   "source": [
    "## 3. CNN Architecture\n",
    "CNNs are composed of three main types of layers, each with a specific role.\n",
    "\n",
    "- Convolutional Layer\n",
    "- Pooling Layer\n",
    "- Fully-Connected Layer\n",
    "\n",
    "### Convolutional Layer\n",
    "Convolutional Layer applies leanable filters (kernels) that slide over the input image to produce feature maps. Each filter is a small matrix (e.g., $3 \\times 3$ or $5 \\times 5$) of randomly initialised weights, and it detects patterns (edges, textures, shapes) in the local regions of the image. In this layer, we can specify two hyperparameters depending on our tasks:\n",
    "\n",
    "- **Stride**: The number of steps the filter moves at each slide. With $\\geq 2$ strides, we can reduce the spatial dimensions for computation (downsampling).\n",
    "- **Padding**: Adding borders (zeros) around the input to control the size of the output.\n",
    "    - **Valid padding**: No padding, output size is reduced.\n",
    "    - **Same Padding**: Padding ensures the output size matches the input size.\n",
    "\n",
    "For each spatial location, the filter computes a weighted sum of pixel values (dot product), plus a bias, and outputs the result as a new pixel in the feature map. The first output is a dot product of is the top left $3 \\times 3 $ matrix of the input and the filter, calculated as $0.37 \\times 0.57 + 0.95 \\times (-0.60) + 0.73 \\times 0.03 + \\dots + 0.83 \\times 0.90 = -0.28$.\n",
    "\n",
    "<center><img src=\"_img/conv2d_1.png\"/></center>\n",
    "\n",
    "Then, the filter moves by 1 column to calculate the second output value:\n",
    "\n",
    "<center><img src=\"_img/conv2d_2.png\"/></center>\n",
    "\n",
    "We repeat this process until we obtain all the values for the output matrix (Forward Propagation).\n",
    "\n",
    "During the training process, gradients of the loss function with respect to the filter weights and biases are computed via backpropagation. These gradients are used to update the filters, allowing the neural network to learn optimal feature extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83d94e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    \"\"\"\n",
    "    2D Convolutional Layer implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_filters: int, filter_size: int, input_shape: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialise convolutional layer parameters.\n",
    "\n",
    "        Args:\n",
    "            num_filters: Number of filters/kernels in layer\n",
    "            filter_size: Spatial dimensions of filters (filter_size x filter_size)\n",
    "            input_shape: Shape of input tensor (height, width, channels)\n",
    "        \"\"\"\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.input_shape = input_shape\n",
    "        self.filters = np.random.randn(\n",
    "            num_filters, filter_size, filter_size, input_shape[-1]) * 0.1\n",
    "        self.biases = np.zeros(num_filters)\n",
    "\n",
    "    def forward(self, input: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Perform forward pass through convolutional layer.\n",
    "\n",
    "        Computes:\n",
    "            output[b, i, j, f] = ∑∑∑ (input_region * filter) + bias.\n",
    "\n",
    "        Args:\n",
    "            input: Input tensor of shape (batch_size, height, width, in_channels).\n",
    "\n",
    "        Returns:\n",
    "            Output feature maps of shape (batch_size, out_height, out_width, num_filters).\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        batch_size, in_h, in_w, in_c = input.shape\n",
    "        out_h = in_h - self.filter_size + 1\n",
    "        out_w = in_w - self.filter_size + 1\n",
    "        self.output = np.zeros((batch_size, out_h, out_w, self.num_filters))\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                region = input[:, i:i+self.filter_size,\n",
    "                               j:j+self.filter_size, :]\n",
    "                self.output[:, i, j, :] = np.tensordot(\n",
    "                    region, self.filters, axes=([1, 2, 3], [1, 2, 3])\n",
    "                ) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Compute gradients during backpropagation.\n",
    "\n",
    "        Calculates:\n",
    "        1. Gradient w.r.t filters.\n",
    "        2. Gradient w.r.t biases.\n",
    "        3. Gradient w.r.t input (using full convolution).\n",
    "\n",
    "        Args:\n",
    "            grad: Gradient from next layer of shape (batch_size, out_h, out_w, num_filters).\n",
    "\n",
    "        Returns:\n",
    "            Gradient w.r.t input of shape (batch_size, in_h, in_w, in_channels).\n",
    "        \"\"\"\n",
    "        batch_size, out_h, out_w, num_filters = grad.shape\n",
    "        in_h, in_w, in_c = self.input.shape[1], self.input.shape[2], self.input.shape[3]\n",
    "\n",
    "        # Gradient w.r.t filters\n",
    "        grad_filters = np.zeros_like(self.filters)\n",
    "        for f in range(num_filters):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    region = self.input[:, i:i +\n",
    "                                        self.filter_size, j:j+self.filter_size, :]\n",
    "                    grad_filters[f] += np.sum(region * grad[:, i, j, f]\n",
    "                                              [:, None, None, None], axis=0)\n",
    "\n",
    "        # Gradient w.r.t biases\n",
    "        grad_biases = np.sum(grad, axis=(0, 1, 2))\n",
    "\n",
    "        # Gradient w.r.t input (vectorised implementation)\n",
    "        grad_input = np.zeros_like(self.input)\n",
    "        padded_grad = np.pad(grad, ((0, 0), (self.filter_size-1, self.filter_size-1),\n",
    "                                    (self.filter_size-1, self.filter_size-1), (0, 0)))\n",
    "        flipped_filters = np.flip(\n",
    "            self.filters, axis=(1, 2)).transpose(1, 2, 0, 3)\n",
    "\n",
    "        for i in range(in_h):\n",
    "            for j in range(in_w):\n",
    "                region = padded_grad[:, i:i +\n",
    "                                     self.filter_size, j:j+self.filter_size, :]\n",
    "                # Vectorised computation\n",
    "                grad_input[:, i, j, :] = np.sum(\n",
    "                    region[:, :, :, :, None] *\n",
    "                    flipped_filters[None, :, :, :, :],\n",
    "                    axis=(1, 2, 3)\n",
    "                )\n",
    "\n",
    "        self.grad_filters = grad_filters\n",
    "        self.grad_biases = grad_biases\n",
    "        return grad_input\n",
    "\n",
    "    def update(self, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Update layer parameters using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: Step size for parameter updates.\n",
    "        \"\"\"\n",
    "        self.filters -= learning_rate * self.grad_filters\n",
    "        self.biases -= learning_rate * self.grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af70998",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "Convolutional layers are often followed by a pooling layer to reduce the spatial dimensions (heights/widths) of feature maps while retaining important information. It slides a window over the feature map and replaces the each region with a single summary value, which is the maximum (or average) value in each region for max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191bab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling:\n",
    "    \"\"\"\n",
    "    Max Pooling Layer implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialise max pooling layer.\n",
    "\n",
    "        Args:\n",
    "            pool_size: Window size for pooling (pool_size x pool_size).\n",
    "        \"\"\"\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward(self, input: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Perform forward pass through max pooling layer.\n",
    "\n",
    "        Args:\n",
    "            input: Input tensor of shape (batch_size, height, width, channels).\n",
    "\n",
    "        Returns:\n",
    "            Downsampled output of shape (batch_size, out_h, out_w, channels).\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        batch_size, h, w, c = input.shape\n",
    "        out_h = h // self.pool_size\n",
    "        out_w = w // self.pool_size\n",
    "        output = np.zeros((batch_size, out_h, out_w, c))\n",
    "        self.mask = np.zeros_like(input)\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                h_start, h_end = i*self.pool_size, (i+1)*self.pool_size\n",
    "                w_start, w_end = j*self.pool_size, (j+1)*self.pool_size\n",
    "                region = input[:, h_start:h_end, w_start:w_end, :]\n",
    "                output[:, i, j, :] = np.max(region, axis=(1, 2))\n",
    "\n",
    "                # Create mask for backpropagation\n",
    "                mask_region = (region == output[:, i, j, :][:, None, None, :])\n",
    "                self.mask[:, h_start:h_end, w_start:w_end, :] = mask_region\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Distribute gradients during backpropagation.\n",
    "\n",
    "        Args:\n",
    "            grad: Gradient from next layer of shape (batch_size, out_h, out_w, channels).\n",
    "\n",
    "        Returns:\n",
    "            Gradient w.r.t input of shape (batch_size, height, width, channels).\n",
    "        \"\"\"\n",
    "        grad_input = np.zeros_like(self.input)\n",
    "        batch_size, out_h, out_w, c = grad.shape\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                h_start, h_end = i*self.pool_size, (i+1)*self.pool_size\n",
    "                w_start, w_end = j*self.pool_size, (j+1)*self.pool_size\n",
    "\n",
    "                # Distribute gradient to max positions\n",
    "                grad_region = grad[:, i:i+1, j:j+1, :]\n",
    "                grad_region = np.repeat(grad_region, self.pool_size, axis=1)\n",
    "                grad_region = np.repeat(grad_region, self.pool_size, axis=2)\n",
    "\n",
    "                mask_region = self.mask[:, h_start:h_end, w_start:w_end, :]\n",
    "                grad_input[:, h_start:h_end, w_start:w_end,\n",
    "                           :] += grad_region * mask_region\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d398754",
   "metadata": {},
   "source": [
    "### Fully Connected Layer\n",
    "In this layer, each neuron is connected to every element of the preceding layer's output. It performs a linear transformation (dot product with weights + bias), and passes the result to the next layer. For a single neuron:\n",
    "\n",
    "\\begin{align*}\n",
    "    z = \\sum_{i=1}^{b}x_{i}w_{i} + b\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $x_{i}$: Inputs from the previous layer.\n",
    "- $w_{i}$: Weights for each input.\n",
    "- $b$: Bias term.\n",
    "- $z$: Pre-activation value (weighted sum + bias).\n",
    "\n",
    "Fully connected layer computes this $z$ value for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea431c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Fully Connected (Dense) Layer implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialise dense layer parameters.\n",
    "\n",
    "        Args:\n",
    "            input_size: Number of input features.\n",
    "            output_size: Number of output neurons.\n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.biases = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, input: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Perform forward pass: output = input·W + b.\n",
    "\n",
    "        Args:\n",
    "            input: Input tensor of shape (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, grad: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Compute gradients during backpropagation.\n",
    "\n",
    "        Args:\n",
    "            grad: Gradient from next layer of shape (batch_size, output_size).\n",
    "\n",
    "        Returns:\n",
    "            Gradient w.r.t input of shape (batch_size, input_size).\n",
    "        \"\"\"\n",
    "        grad_input = np.dot(grad, self.weights.T)\n",
    "        self.grad_weights = np.dot(self.input.T, grad)\n",
    "        self.grad_biases = np.sum(grad, axis=0)\n",
    "        return grad_input\n",
    "\n",
    "    def update(self, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Update layer parameters using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: Step size for parameter updates.\n",
    "        \"\"\"\n",
    "        self.weights -= learning_rate * self.grad_weights\n",
    "        self.biases -= learning_rate * self.grad_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad0ad1",
   "metadata": {},
   "source": [
    "## 4. Flatten Layer\n",
    "Neural networks often have convolutional or pooling layers that output 2D or 3D feature maps. Fully connected layers, however, expect 1D output. The flatten layers bridges this gap by reshaping multidimentional input data into a one-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30705163",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    \"\"\"\n",
    "    Flatten Layer implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Flatten input tensor to 2D matrix.\n",
    "\n",
    "        Args:\n",
    "            input: Input tensor of shape (batch_size, *spatial_dims, channels).\n",
    "\n",
    "        Returns:\n",
    "            Flattened output of shape (batch_size, features).\n",
    "        \"\"\"\n",
    "        self.input_shape = input.shape\n",
    "        return input.reshape(input.shape[0], -1)\n",
    "\n",
    "    def backward(self, grad: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Reshape gradient to original input shape.\n",
    "\n",
    "        Args:\n",
    "            grad: Gradient from next layer of shape (batch_size, features).\n",
    "\n",
    "        Returns:\n",
    "            Gradient w.r.t input of original shape (batch_size, *spatial_dims, channels).\n",
    "        \"\"\"\n",
    "        return grad.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b4f67",
   "metadata": {},
   "source": [
    "## 5. Activation Function\n",
    "After the convolutional layers and the dense layers, an activation function is applied to introduce non-linearity, allowing the neural network to learn complex patterns.\n",
    "\n",
    "### Rectified Linear Unit (ReLU)\n",
    "ReLU is computationally efficient and it mitigates vanishing gradient, but it may cause 'Dying ReLU' problem where neurons can get stuck at 0.\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(Z) = \\text{max}(0, Z)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### ReLU Derivative\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sigma'(Z) =\n",
    "    \\begin{cases}\n",
    "    1 & \\text{if } Z > 0 \\\\\n",
    "    0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796b5d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit (ReLU) Activation Layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Apply element-wise ReLU activation: output = max(0, input).\n",
    "\n",
    "        Args:\n",
    "            input: Input tensor of any shape.\n",
    "\n",
    "        Returns:\n",
    "            Activated output of same shape as input.\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def backward(self, grad: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Compute gradient of ReLU: grad * (input > 0).\n",
    "\n",
    "        Args:\n",
    "            grad: Gradient from next layer.\n",
    "\n",
    "        Returns:\n",
    "            Gradient w.r.t input of same shape.\n",
    "        \"\"\"\n",
    "        return grad * (self.input > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07658362",
   "metadata": {},
   "source": [
    "## 6. Output Layer\n",
    "The output layer with a softmax activation function assigns probabilities to multiple classes. The softmax function transforms the raw scores (logits) of the output layer into probabilities, ensuring that they are non-negative and their sum equals 1.\n",
    "\n",
    "For a vector of logits $Z = [z_1, z_2, ..., z_k]$, the softmax function is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sigma(Z_i) = \\dfrac{e(z_{i})}{\\sum^{k}_{j=1}e(z_j)} \\text{ for } i=1, 2, ..., k\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\sigma(Z_i)$: Predicted probability of the $i$-th class.\n",
    "- $z_{i}$: Logit (raw score) for the $i$-th class.\n",
    "- $k$: Number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909ab551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Compute softmax activation for classification.\n",
    "\n",
    "    Args:\n",
    "        x: Input logits of shape (batch_size, num_classes).\n",
    "\n",
    "    Returns:\n",
    "        Probability distribution over classes of shape (batch_size, num_classes).\n",
    "    \"\"\"\n",
    "    exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2c860",
   "metadata": {},
   "source": [
    "## 7. Loss Function\n",
    "### Cross-Entropy Loss\n",
    "Cross-Entropy Loss is a loss function used for classification problems, particularly when the model outputs probabilities using a softmax activation in the final layer. It measures the difference between the true labels and the predicted probability distribution.\n",
    "\n",
    "For a single data point, the cross-entropy loss is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "    L = - \\sum^{k}_{i=1}y_{i}\\log{(\\hat y_{i})}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True lavel for the $i$-th class. If one-hot encoded, $y_{i} = 1$ for the corrected class, $y_{i} = 0$ otherwise.\n",
    "- $\\hat y_i$: Predicted probability for the $i$-th class.\n",
    "- $k$: Number of classes.\n",
    "\n",
    "For a batch of $m$ data point:\n",
    "\n",
    "\\begin{align*}\n",
    "    C = \\dfrac{1}{m} \\sum^{m}_{j=1} - \\sum^{k}_{i=1}y_{j, i}\\log{(\\hat y_{i})}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4679eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred: NDArray[np.float64], y_true: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Compute categorical cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        y_pred: Predicted probabilities of shape (batch_size, num_classes).\n",
    "        y_true: Ground truth one-hot labels of shape (batch_size, num_classes).\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss value.\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    log_probs = -np.log(y_pred[range(m), np.argmax(y_true, axis=1)])\n",
    "    return np.sum(log_probs) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd43c91",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metric\n",
    "### Accuracy\n",
    "Accuracy is the most common evaluation metric for classification problems, representing the percentage of correct predictions out of total predictions. It provides a simple measure of how often the classifier makes correct predictions across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82f4a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred: NDArray[np.int64], y_true: NDArray[np.int64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Compute classification accuracy.\n",
    "\n",
    "    Args:\n",
    "        y_pred: Predicted class probabilities of shape (batch_size, num_classes).\n",
    "        y_true: Ground truth one-hot labels of shape (batch_size, num_classes).\n",
    "\n",
    "    Returns:\n",
    "        Accuracy score between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b742b",
   "metadata": {},
   "source": [
    "## 9. CNN Model\n",
    "The following `ConvNet` class is a basic implementation of a Convolutional Neural Network. It defines a sequential neural network structure with the following main components:\n",
    "\n",
    "- **Convolutional Layer (Conv2D)**: Extracts spatial features from input images.\n",
    "- **Activation Layer (ReLU)**: Introduces non-linearity to the network.\n",
    "- **Pooling Layer (MaxPooling)**: Reduces spatial dimensions to decrease computation and avoid overfitting.\n",
    "- **Flatten Layer**: Converts 2D feature maps into 1D vectors for feeding into dense layers.\n",
    "- **Dense Layers**: Fully connected layers for classification.\n",
    "- **Forward, Backward**, and Update Methods: Handle forward propagation, backpropagation, and parameter updates.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"_img/cnn.png\">\n",
    "  <figcaption>\n",
    "    <em>\n",
    "      <a href=\"https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\" target=\"_blank\">\n",
    "        Source: Understanding of Convolutional Neural Network (CNN) — Deep Learning\n",
    "      </a>\n",
    "    </em>\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "### Architecture\n",
    "1. Convolutional Layer\n",
    "2. ReLU Activation\n",
    "3. Max Pooling Layer\n",
    "4. Flatten Layer\n",
    "5. First Dense Layer\n",
    "6. ReLU Activation\n",
    "7. Second Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7599b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet:\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network architecture for image classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialise CNN layers with fixed architecture.\n",
    "        \"\"\"\n",
    "        self.conv1 = Conv2D(num_filters=8, filter_size=3,\n",
    "                            input_shape=(28, 28, 1))\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPooling(pool_size=2)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(input_size=13*13*8, output_size=128)\n",
    "        self.relu2 = ReLU()\n",
    "        self.dense2 = Dense(input_size=128, output_size=10)\n",
    "        self.layers = [\n",
    "            self.conv1, self.relu1, self.pool1,\n",
    "            self.flatten, self.dense1, self.relu2, self.dense2\n",
    "        ]\n",
    "\n",
    "    def forward(self, x: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Perform forward pass through all layers.\n",
    "\n",
    "        Args:\n",
    "            x: Input image tensor of shape (batch_size, 28, 28, 1).\n",
    "\n",
    "        Returns:\n",
    "            Output logits of shape (batch_size, 10).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: NDArray[np.float64]) -> None:\n",
    "        \"\"\"\n",
    "        Backpropagate gradients through all layers.\n",
    "\n",
    "        Args:\n",
    "            grad: Gradient from loss function.\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def update_params(self, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Update all trainable parameters in the network.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: Step size for parameter updates.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update'):\n",
    "                layer.update(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9db617",
   "metadata": {},
   "source": [
    "## 10. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15e36440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0: Loss=2.3489, Accuracy=0.0938\n",
      "Epoch 1, Batch 100: Loss=1.6772, Accuracy=0.6094\n",
      "Epoch 1, Batch 200: Loss=0.7494, Accuracy=0.8438\n",
      "Epoch 1, Batch 300: Loss=0.5635, Accuracy=0.8594\n",
      "Epoch 1, Batch 400: Loss=0.3581, Accuracy=0.9062\n",
      "Epoch 1, Batch 500: Loss=0.4058, Accuracy=0.8125\n",
      "Epoch 1, Batch 600: Loss=0.2573, Accuracy=0.9375\n",
      "Epoch 1, Batch 700: Loss=0.3851, Accuracy=0.8750\n",
      "Epoch 1, Batch 800: Loss=0.3646, Accuracy=0.8750\n",
      "Epoch 2, Batch 0: Loss=0.3489, Accuracy=0.9062\n",
      "Epoch 2, Batch 100: Loss=0.2353, Accuracy=0.9062\n",
      "Epoch 2, Batch 200: Loss=0.2377, Accuracy=0.9531\n",
      "Epoch 2, Batch 300: Loss=0.3985, Accuracy=0.9062\n",
      "Epoch 2, Batch 400: Loss=0.1746, Accuracy=0.9219\n",
      "Epoch 2, Batch 500: Loss=0.3212, Accuracy=0.9062\n",
      "Epoch 2, Batch 600: Loss=0.1669, Accuracy=0.9375\n",
      "Epoch 2, Batch 700: Loss=0.3434, Accuracy=0.8750\n",
      "Epoch 2, Batch 800: Loss=0.2680, Accuracy=0.9062\n",
      "Epoch 3, Batch 0: Loss=0.2508, Accuracy=0.9219\n",
      "Epoch 3, Batch 100: Loss=0.1892, Accuracy=0.9219\n",
      "Epoch 3, Batch 200: Loss=0.1935, Accuracy=0.9531\n",
      "Epoch 3, Batch 300: Loss=0.3585, Accuracy=0.9375\n",
      "Epoch 3, Batch 400: Loss=0.1274, Accuracy=0.9219\n",
      "Epoch 3, Batch 500: Loss=0.2893, Accuracy=0.9062\n",
      "Epoch 3, Batch 600: Loss=0.1416, Accuracy=0.9688\n",
      "Epoch 3, Batch 700: Loss=0.3142, Accuracy=0.8750\n",
      "Epoch 3, Batch 800: Loss=0.2196, Accuracy=0.9531\n",
      "\n",
      "Final Test Accuracy: 0.9398\n"
     ]
    }
   ],
   "source": [
    "# Training Setup\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialise model\n",
    "model = ConvNet()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model.forward(X_batch)\n",
    "        y_pred = softmax(logits)\n",
    "        loss = cross_entropy_loss(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        grad = (y_pred - y_batch) / batch_size\n",
    "        model.backward(grad)\n",
    "        model.update_params(learning_rate)\n",
    "\n",
    "        # Print metrics\n",
    "        if i % 6400 == 0:\n",
    "            acc = accuracy(y_pred, y_batch)\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}, Batch {i//batch_size}: Loss={loss:.4f}, Accuracy={acc:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "test_pred = softmax(model.forward(X_test))\n",
    "test_acc = accuracy(test_pred, y_test)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4711f39",
   "metadata": {},
   "source": [
    "## 11. References\n",
    "\n",
    "1. ANDRADA. (2020). *How I taught myself Deep Learning: Vanilla NNs*. <br>\n",
    "https://www.kaggle.com/code/andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns/comments\n",
    "\n",
    "1. Arseny Turin. (2020). *Neural Network From Scratch*. <br>\n",
    "https://github.com/arsenyturin/Neural-Network-From-Scratch\n",
    "\n",
    "1. Bot Academy. (2021). *Neural Networks Explained from Scratch using Python* [YouTube Video]. <br>\n",
    "https://youtu.be/9RN2Wr8xvro\n",
    "\n",
    "1. Cole Stryker and Dave Bergmann. (2024). *What is backpropagation?* <br>\n",
    "https://www.ibm.com/think/topics/backpropagation\n",
    "\n",
    "1. Futurology — An Optimistic Future. (2020). *Convolutional Neural Networks Explained (CNN Visualized)*. <br>\n",
    "https://youtu.be/pj9-rr1wDhM\n",
    "\n",
    "1. GeeksforGeeks. (2025). *Backpropagation in Neural Network*. <br>\n",
    "https://www.geeksforgeeks.org/machine-learning/backpropagation-in-neural-network/\n",
    "\n",
    "1. Haroon Khan. (2024). *A beginner's builde to: Vanilla Neural Networks*.<br>\n",
    "https://www.linkedin.com/pulse/beginners-guide-vanilla-neural-networks-haroon-khan-qurjf/\n",
    "\n",
    "1. IBM. (2021). *What is a neural network?* [YouTube Video] <br>\n",
    "https://www.ibm.com/think/topics/neural-networks\n",
    "\n",
    "1. IBM Technology. (2022). *Neural Networks Explained in 5 minutes* [YouTube Video]. <br>\n",
    "https://youtu.be/jmmW0F0biz0\n",
    "\n",
    "1. IBM Technology. (2021). *What are Convolutional Neural Networks (CNNs)?* [YouTube Video]. <br>\n",
    "https://youtu.be/QzY57FaENXg\n",
    "\n",
    "1. IBM Technology. (2023). *What is Back Propagation* [YouTube Video]. <br>\n",
    "https://youtu.be/S5AGN9XfPK4\n",
    "\n",
    "1. Michael Nielsen. (2019). *Using neural nets to recognize handwritten digits*. <br>\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "1. Nitish Gupta. (2023). *Building VanillaNN: A Simplified Neural Network Framework from Scratch in Python*. <br>\n",
    "https://blog.guptanitish.com/building-vanillann-a-simplified-neural-network-framework-from-scratch-in-python-c82f39071ee0\n",
    "\n",
    "1. The Independent Code. (2021). *Convolutional Neural Network from Scratch | Mathematics & Python Code* [YouTube Video]. <br>\n",
    "https://youtu.be/Lakz2MoHy6o\n",
    "\n",
    "1. 3Blue1Brown. (2022). *But what is a convolution?* [YouTube Video]. <br>\n",
    "https://youtu.be/KuXjwB4LzSA\n",
    "\n",
    "1. 3Blue1Brown. (2017). *But what is a neural network? | Deep learning chapter 1* [YouTube Video]. <br>\n",
    "https://youtu.be/aircAruvnKk\n",
    "\n",
    "1. 3Blue1Brown. (2017). *Gradient descent, how neural networks learn | Deep Learning Chapter 2* [YouTube Video]. <br>\n",
    "https://youtu.be/IHZwWFHWa-w\n",
    "\n",
    "1. 3Blue1Brown. (2017). *Backpropagation, intuitively | Deep Learning Chapter 3* [YouTube Video]. <br>\n",
    "https://youtu.be/Ilg3gGewQ5U\n",
    "\n",
    "1. 3Blue1Brown. (2017). *Backpropagation calculus | Deep Learning Chapter 4* [YouTube Video]. <br>\n",
    "https://youtu.be/tIeHLnjs5U8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
