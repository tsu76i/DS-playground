{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9fbd05",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes Classifier from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f59cd",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Naive Bayes classifiers are probabilistic classification models based on Bayes' Theorem, assuming conditional independence between features given the class labels or values. Naive Bayes is a general framework; the specific variant should be chosen based on the nature of your data:\n",
    "\n",
    "- **Categorical Naive Bayes**\n",
    "\n",
    "    - **Features**: Categorical labels (e.g., colours, countries, product types).\n",
    "\n",
    "    - **Use Case**: Classification with discrete, categorically distributed features.\n",
    "\n",
    "- **Multinomial Naive Bayes**\n",
    "\n",
    "    - **Features**: Counts or frequencies (e.g., word occurrences, event counts).\n",
    "\n",
    "    - **Use** **Case**: Text classification, document classification, or any scenario where features are discrete counts.\n",
    "\n",
    "- **Gaussian Naive Bayes**\n",
    "\n",
    "    - **Features**: Continuous data (e.g., measurements, sensor readings).\n",
    "\n",
    "    - **Use Case**: Classification with numerical features assumed to follow a Gaussian distribution.\n",
    "\n",
    "- **Bernoulli Naive Bayes**\n",
    "\n",
    "    - **Features**: Binary features (e.g., True/False, 0/1).\n",
    "\n",
    "    - **Use Case**: Text classification (presence/absence of words), binary feature spaces.\n",
    "\n",
    "\n",
    "\n",
    "### Bayes' Theorem\n",
    "Bayes' theorem describes the probability of a class $C_{i}$ given a set of features $X = (x_{1}, x_{2},\\ldots,x_{N})$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_{i}|X) = \\dfrac{P(X|C_{i}) \\cdot P(C_{i})}{P(X)}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $P(C_{i}|X)$: Posterior probability of class $C_{i}$ given features $X$.\n",
    "- $P(X|C_{i})$: Likelihood of features $X$ given class $C_{i}$.\n",
    "- $P(C_{i})$: Prior probability of class $C_{i}$.\n",
    "- $P(X)$: Evidence (normalising constant, same for all classes)\n",
    "\n",
    "Gaussian Naive Bayes assumes features $X = (x_{1}, x_{2},\\ldots,x_{N})$ are conditionally independent given the class $C_{i}$ and features follow a Gaussian (normal) distribution within each class. Therefore, the likelihood is expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "P(x_j|C_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_{ij}^2}} \\exp\\left(-\\frac{(x_j - \\mu_{ij})^2}{2\\sigma_{ij}^2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "$\\mu_{ij}$ = Mean of feature $x_j$ in class $C_i$\n",
    "\n",
    "$\\sigma_{ij}^2$ = Variance of feature $x_j$ in class $C_i$\n",
    "\n",
    "\n",
    "Replacing $P(X|C_{i})$ in Bayes' theorem, the equation becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_{i}|X) = \\dfrac{P(C_{i}) \\cdot \\prod_{j=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma_{ij}^2}} \\exp\\left(-\\frac{(x_j - \\mu_{ij})^2}{2\\sigma_{ij}^2}\\right)}{P(X)}\n",
    "\\end{align*}\n",
    "\n",
    "Since $P(X)$ is constant for all classes,\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_{i}|X) \\propto P(C_{i}) \\cdot \\prod_{j=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma_{ij}^2}} \\exp\\left(-\\frac{(x_j - \\mu_{ij})^2}{2\\sigma_{ij}^2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "The symbol $\\propto$ denotes proportionality, meaning we ignore the denominator $P(X)$ when comparing probabilities across classes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
