{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c1dfd6d",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Classifier from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "    - [Bayes' Theorem](#bayes-theorem)\n",
    "2. [Loading Data](#2-loading-data)\n",
    "3. [Text Preprocessing](#3-text-preprocessing)\n",
    "4. [Prior Probability](#4-prior-probability)\n",
    "5. [Likelihood for Multinomial NB](#5-likelihood-for-multinomial-nb)\n",
    "6. [Posterior Probability for Multinomial NB](#6-posterior-probability-for-multinomial-nb)\n",
    "7. [Prediction](#7-prediction)\n",
    "8. [Evaluation Metrics](#8-evaluation-metrics)\n",
    "    - [Binary Confusion Matrix](#binary-confusion-matrix)\n",
    "    - [Multi-Class Confusion Matrix](#multi-class-confusion-matrix)\n",
    "    - [Accuracy](#accuracy)\n",
    "    - [Precision](#precision)\n",
    "    - [Recall](#recall)\n",
    "    - [F1-Score](#f1-score)\n",
    "9. [Train Test Split](#9-train-test-split)\n",
    "10. [Encapsulation](#10-encapsulation)\n",
    "11. [Comparison with Scikit-Learn](#11-comparison-with-scikit-learn)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4315beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78275491",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Naive Bayes classifiers are probabilistic classification models based on Bayes' Theorem, assuming conditional independence between features given the class labels or values. Naive Bayes is a general framework; the specific variant should be chosen based on the nature of your data:\n",
    "\n",
    "- **Categorical Naive Bayes**\n",
    "\n",
    "    - **Features**: Categorical labels (e.g., colours, countries, product types).\n",
    "\n",
    "    - **Use Case**: Classification with discrete, categorically distributed features.\n",
    "\n",
    "- **Multinomial Naive Bayes**\n",
    "\n",
    "    - **Features**: Counts or frequencies (e.g., word occurrences, event counts).\n",
    "\n",
    "    - **Use** **Case**: Text classification, document classification, or any scenario where features are discrete counts.\n",
    "\n",
    "- **Gaussian Naive Bayes**\n",
    "\n",
    "    - **Features**: Continuous data (e.g., measurements, sensor readings).\n",
    "\n",
    "    - **Use Case**: Classification with numerical features assumed to follow a Gaussian distribution.\n",
    "\n",
    "- **Bernoulli Naive Bayes**\n",
    "\n",
    "    - **Features**: Binary features (e.g., True/False, 0/1).\n",
    "\n",
    "    - **Use Case**: Text classification (presence/absence of words), binary feature spaces.\n",
    "\n",
    "\n",
    "\n",
    "### Bayes' Theorem\n",
    "Bayes' theorem describes the probability of a class $C_{i}$ given a document $d$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_{i}|d) = \\dfrac{P(d|C_{i}) \\cdot P(C_{i})}{P(d)}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $P(C_{i}|d)$: Posterior probability of class $C_{i}$ given document $d$.\n",
    "- $P(d|C_{i})$: Likelihood of document $d$ given class $C_{i}$.\n",
    "- $P(C_{i})$: Prior probability of class $C_{i}$.\n",
    "- $P(d)$: Probability of document $d$ (acts as a normalising constant).\n",
    "\n",
    "Multinomial Naive Bayes assumes word occurrences in $d$ are conditionally independent given $C_{i}$. For a document represented by word counts ${\\text{c}(w_1,d), \\text{c}(w_2,d), \\dots, \\text{c}(w_{V},d)}$, the likelihood is:\n",
    "\n",
    "\\begin{align*}\n",
    "P(d|C_{i}) = \\prod_{j=1}^{V} P(w_{j}|C_{i})^{\\text{c}(w_{j},d)}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $w_{j}$: $j$-th word in vocabulary.\n",
    "- $\\text{c}(w_{j},d)$: Frequency of $w_{j}$ in $d$.\n",
    "- $V$: Vocabulary size.\n",
    "\n",
    "Replacing $P(d|C_{i})$ in Bayes' theorem, the equation becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_{i}|d) = \\dfrac{P(C_{i}) \\cdot \\prod_{j=1}^{V} P(w_{j}|C_{i})^{\\text{c}(w_{j},d)}}{P(d)}\n",
    "\\end{align*}\n",
    "\n",
    "Since $P(d)$ is constant across classes:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_{i}|d) \\propto P(C_{i}) \\cdot \\prod_{j=1}^{V} P(w_{j}|C_{i})^{\\text{c}(w_{j},d)}\n",
    "\\end{align*}\n",
    "\n",
    "The symbol $\\propto$ denotes proportionality, meaning we ignore $P(d)$ when comparing probabilities across classes.\n",
    "\n",
    "## 2. Loading Data\n",
    "Dataset retrieved from [Kaggle - Spam Email](https://www.kaggle.com/datasets/mfaisalqureshi/spam-email?select=spam.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5357c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Message",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "d9a45e16-7b64-42cb-bac0-85434d5d2e67",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../_datasets/spam.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777fa816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "3b1898b0-0ce5-4818-8209-0edd0fcda7fa",
       "rows": [
        [
         "ham",
         "4825"
        ],
        [
         "spam",
         "747"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "Category\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57197fa",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "Before implementing our Multinomial Naive Bayes classifier, we usually need to perform text preprocessing to ensure effective spam email classification. For this project, we will:\n",
    "\n",
    "- Convert all text to lowercase.\n",
    "\n",
    "Depending on the project or dataset, we can additionally perform more text preprocessing, such as:\n",
    "- Remove stopwords (e.g,. 'a', 'the').\n",
    "- Lemmatisation.\n",
    "- Removing non-alphabetic characters.\n",
    "- Removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a76d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and preprocess email text for spam detection.\n",
    "\n",
    "    Args:\n",
    "        Raw email text.\n",
    "\n",
    "    Returns:\n",
    "        Cleaned and preprocessed text.\n",
    "    \"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de4fe1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Message",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "05d1cd0b-a8d8-483f-ae36-2b2eac980e10",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...",
         "go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni...",
         "ok lar... joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's",
         "free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005. text fa to 87121 to receive entry question(std txt rate)t&c's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say...",
         "u dun say so early hor... u c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though",
         "nah i don't think he goes to usf, he lives around here though"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  go until jurong point, crazy.. available only ...  \n",
       "1                      ok lar... joking wif u oni...  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3  u dun say so early hor... u c already then say...  \n",
       "4  nah i don't think he goes to usf, he lives aro...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['Message'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3149be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['clean_text'], df['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e14eae",
   "metadata": {},
   "source": [
    "Now, we apply `CountVectorizer` to convert the text data into a document-term matrix (also known as a Bag of Words), where each unique word in the dataset becomes a feature(column) and each value is the word count in a given document (row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0477dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectoriser = CountVectorizer()\n",
    "X_doc_term = vectoriser.fit_transform(X)\n",
    "X = pd.DataFrame(X_doc_term.toarray(),\n",
    "                 columns=vectoriser.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda58016",
   "metadata": {},
   "source": [
    "## 4. Prior Probability\n",
    "Class $C_{i}$ (`y`) has only two discrete variables: `ham` and `spam`:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_{i}=\\text{'ham'}) = \\dfrac{\\text{Count('ham')}}{\\text{Total Count}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_{i}=\\text{'spam'}) = \\dfrac{\\text{Count('spam')}}{\\text{Total Count}}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8f28b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 5572\n",
      "Counts: {'ham': 4825, 'spam': 747}\n"
     ]
    }
   ],
   "source": [
    "print(f'Total count: {len(df)}')\n",
    "print(f'Counts: {y.value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c04d6",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "P(\\text{'ham'}) = \\dfrac{4825}{5572} = 0.8659\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\text{'spam'}) = \\dfrac{747}{5572} = 0.1341\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2b1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_priors(y: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate prior probabilities for each class in the target variable.\n",
    "\n",
    "    Args:\n",
    "        y: Target variable containing class labels (strings).\n",
    "\n",
    "    Returns:\n",
    "        Prior probabilities for each class.\n",
    "    \"\"\"\n",
    "    return y.value_counts(normalize=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ff35c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ham': 0.8659368269921034, 'spam': 0.13406317300789664}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_priors(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795ddee",
   "metadata": {},
   "source": [
    "## 5. Likelihood for Multinomial NB\n",
    "\n",
    "For word $w_{j}$ and class $C_{i}$, the likelihood is calculated as:\n",
    "\n",
    "\\begin{align*}\n",
    "P(w_{j}|C_{i}) = \\frac{\n",
    "    \\text{count}(w_{j} \\text{ in } C_{i}) + \\alpha\n",
    "}{\n",
    "    \\text{total words in } C_{i} + V \\cdot \\alpha\n",
    "}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\text{count}(w_{j} \\text{ in } C_{i})$: Total occurrences of word $w_{j}$ in class $C_{i}$.\n",
    "- $\\text{total words in } C_{i}$: Sum of all word counts in class $C_{i}$.\n",
    "- $V$ : Vocabulary size.\n",
    "- $\\alpha$: Laplace smoothing parameter.\n",
    "\n",
    "For a document $d$ with word counts $\\{c_1, c_2, \\dots, c_{V}\\}$, the likelihood becomes:\n",
    "\\begin{align*}\n",
    "P(d|C_{i}) \\propto \\prod_{i=1}^{V} \\left[ P(w_{j}|C_{i}) \\right]^{c_i}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihoods(X: pd.DataFrame, y: pd.Series, alpha: float = 1.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate word likelihood probabilities for Gaussian Naive Bayes using vectorised operations.\n",
    "\n",
    "    Args:\n",
    "        X: Document-term matrix with words as columns and documents as rows.\n",
    "        y: Series of class labels corresponding to each document.\n",
    "        alpha: Laplace smoothing parameter (default=1.0).\n",
    "\n",
    "    Returns:\n",
    "        Word probabilities where:\n",
    "        - Rows represent classes\n",
    "        - Columns represent words\n",
    "        - Values are P(word|class)\n",
    "    \"\"\"\n",
    "    # Group by class and sum word counts\n",
    "    class_totals = X.groupby(y).sum()\n",
    "    total_words_per_class = class_totals.sum(axis=1)\n",
    "    vocab_size = len(X.columns)\n",
    "\n",
    "    # Vectorised calculation\n",
    "    numerator = class_totals + alpha\n",
    "    # Broadcast [:, np.newaxis] for efficient probability computation\n",
    "    denominator = total_words_per_class.values[:,\n",
    "                                               np.newaxis] + vocab_size * alpha\n",
    "    likelihoods = numerator / denominator\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e6129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "00",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "000",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "000pes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "008704050406",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "0089",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "0121",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "01223585236",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "01223585334",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "0125698789",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "02",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "61525855-aa77-4427-8ed6-39d84eaf5d11",
       "rows": [
        [
         "ham",
         "1.3951671410234946e-05",
         "1.3951671410234946e-05",
         "2.790334282046989e-05",
         "1.3951671410234946e-05",
         "1.3951671410234946e-05",
         "1.3951671410234946e-05",
         "1.3951671410234946e-05",
         "1.3951671410234946e-05",
         "2.790334282046989e-05",
         "1.3951671410234946e-05"
        ],
        [
         "spam",
         "0.0004210526315789474",
         "0.0011483253588516747",
         "3.8277511961722486e-05",
         "0.00011483253588516746",
         "7.655502392344497e-05",
         "7.655502392344497e-05",
         "7.655502392344497e-05",
         "0.00011483253588516746",
         "3.8277511961722486e-05",
         "0.0003444976076555024"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                00       000    000pes  008704050406      0089      0121  \\\n",
       "Category                                                                   \n",
       "ham       0.000014  0.000014  0.000028      0.000014  0.000014  0.000014   \n",
       "spam      0.000421  0.001148  0.000038      0.000115  0.000077  0.000077   \n",
       "\n",
       "          01223585236  01223585334  0125698789        02  \n",
       "Category                                                  \n",
       "ham          0.000014     0.000014    0.000028  0.000014  \n",
       "spam         0.000077     0.000115    0.000038  0.000344  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_likelihoods(X, y).iloc[:, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f88e0",
   "metadata": {},
   "source": [
    "## 6. Posterior Probability for Multinomial NB\n",
    "As we discussed [above](#1-introduction), the formula of posterior probability is:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C_{i}|d) \\propto P(C_{i}) \\cdot \\prod_{j=1}^{V} P(w_{j}|C_{i})^{\\text{c}(w_{j},d)}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $d$: Document.\n",
    "\n",
    "- $\\text{c}(w_{j},d)$ = Frequency of word $w_{j}$ in $d$.\n",
    "\n",
    "To prevent underflow, we use log probabilities:\n",
    "\n",
    "\\begin{align*}\n",
    "\\log P(C_{i}|d) = \\log P(C_{i}) + \\sum_{j=1}^{V} \\text{c}(w_{j},d) \\cdot \\log P(w_{j}|C_{i})\n",
    "\\end{align*}\n",
    "\n",
    "Some extra tricks:\n",
    "- Instead of iterative word-by-word computation, vectorised matrix operations are used to optimise posterior calculation.\n",
    "- `log_likelihoods_df.values`: Precomputed log(P(word|class)) matrix (classes × words).\n",
    "- `@ x_aligned`: Matrix-vector multiplication.\n",
    "- Computes $\\sum_{j=1}^{V} \\text{c}(w_{j},d) \\cdot \\log P(w_{j}|C_{i})$ for all classes simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd8888ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posterior(x_vec: np.ndarray,\n",
    "                        priors: Dict[str, float],\n",
    "                        log_likelihoods: np.ndarray,\n",
    "                        classes: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates log-posterior probabilities for each class.\n",
    "    Uses array operations for optimised performance.\n",
    "\n",
    "    Args:\n",
    "        x_vec: Feature vector for a single document (shape: [n_features]).\n",
    "        priors: Prior probabilities for each class (keys: class labels).\n",
    "        log_likelihoods: Log-likelihood matrix (shape: [n_classes, n_features]).\n",
    "        classes: Ordered list of class labels corresponding to log_likelihoods rows.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping class labels to log-posterior probabilities.\n",
    "    \"\"\"\n",
    "    log_priors = np.array([np.log(priors[c]) for c in classes])\n",
    "    word_contributions = log_likelihoods @ x_vec\n",
    "    log_posteriors = log_priors + word_contributions\n",
    "    return dict(zip(classes, log_posteriors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2b32f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ham': -0.11590046159203775, 'spam': -1.9995685516807546}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_posterior(X.iloc[0], calculate_priors(\n",
    "    y), calculate_likelihoods(X, y), y.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c08d5ec",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141132e5",
   "metadata": {},
   "source": [
    "`x_aligned = x.reindex(log_likelihoods_df.columns, fill_value=0).values` ensures that document word counts match the precomputed log-likelihood matrix columns. The missing words get `0` and are automatically ignored in multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d0bfdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: pd.DataFrame, y: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Predicts class labels using Multinomial Naive Bayes.\n",
    "\n",
    "    Args:\n",
    "        X: Document-term matrix with documents as rows and features as columns.\n",
    "        y: Target labels corresponding to document rows.\n",
    "\n",
    "    Returns:\n",
    "        Predicted class labels for each document in X.\n",
    "    \"\"\"\n",
    "    # Convert input to DataFrame if needed\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    priors = calculate_priors(y)\n",
    "    likelihoods_df = calculate_likelihoods(X, y)\n",
    "    log_likelihoods_df = np.log(likelihoods_df)\n",
    "    classes = list(y.unique())\n",
    "\n",
    "    # Precompute aligned indices for faster access\n",
    "    aligned_cols = log_likelihoods_df.columns\n",
    "    X_aligned = X.reindex(columns=aligned_cols, fill_value=0)\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(len(X)):\n",
    "        # Direct array access for speed\n",
    "        x_vec = X_aligned.iloc[i].values\n",
    "        posterior = calculate_posterior(\n",
    "            x_vec, priors, log_likelihoods_df.values, classes\n",
    "        )\n",
    "        predictions.append(max(posterior, key=posterior.get))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5908b877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X, y)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c750068",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics\n",
    "### Binary Confusion Matrix\n",
    "In a confusion matrix, the terms True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) describe the classification performance for binary classification. \n",
    "\n",
    "|                     | Predicted Negative  | Predicted Positive  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Negative** | True Negative (TN)  | False Positive (FP) |\n",
    "| **Actual Positive** | False Negative (FN) | True Positive (TP)  |\n",
    "\n",
    "\n",
    "1. True Positive (TP): The number of instances correctly predicted as positive (e.g., a disease correctly identified).\n",
    "\n",
    "2. True Negative (TN): The number of instances correctly predicted as negative (e.g., no disease correctly identified).\n",
    "\n",
    "3. False Positive (FP): The number of instances incorrectly predicted as positive (e.g., predicting disease when there isn't any).\n",
    "\n",
    "4. False Negative (FN): The number of instances incorrectly predicted as negative (e.g., missing a disease when it exists).\n",
    "\n",
    "### Multi-Class Confusion Matrix\n",
    "For multi-class classification, the concepts can be extended by treating one class as the \"positive\" class and all others as \"negative\" classes in a one-vs-all approach. Rows represent the actual classes (true labels), and columns represent the predicted classes. For a class $C$,\n",
    "1. True Positive (TP): The count in the diagonal cell corresponding to class $C$ ($\\text{matrix} [C][C]$).\n",
    "2. False Positive (FP): The sum of the column for class $C$, excluding the diagonal ($\\sum(\\text{matrix} [:, C]) - \\text{matrix} [C][C]$).\n",
    "3. False Negative (FN): The sum of the row for class $C$, excluding the diagonal ($\\sum(\\text{matrix} [C, :]) - \\text{matrix} [C][C]$).\n",
    "4. True Negative (TN): All other cells not in the row or column for class $C$ ($\\text{total} - (FP + FN + TP)$).\n",
    "\n",
    "|                  | Predicted Class 0 | Predicted Class 1 | Predicted Class 2 |\n",
    "| ---------------- | ----------------- | ----------------- | ----------------- |\n",
    "| **True Class 0** | 5                 | 2                 | 0                 |\n",
    "| **True Class 1** | 1                 | 6                 | 1                 |\n",
    "| **True Class 2** | 0                 | 2                 | 7                 |\n",
    "\n",
    "\n",
    "For Class 0:\n",
    "- TP = 5 (diagonal element for Class 0)\n",
    "- FP = 1 (sum of column 0 minus TP: 1 + 0)\n",
    "- FN = 2 (sum of row 0 minus TP: 2 + 0)\n",
    "- TN = 6 + 1 + 2 + 7 = 16 (all other cells not in row 0 or column 0)\n",
    "\n",
    "For Class 1:\n",
    "- TP = 6 (diagonal element for Class 1)\n",
    "- FP = 4 (sum of column 1 minus TP: 2 + 2)\n",
    "- FN = 2 (sum of row 1 minus TP: 1 + 1)\n",
    "- TN = 5 + 0 + 0 + 7 = 12 (all other cells not in row 1 or column 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ffb6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true: pd.Series, y_pred: List[str],\n",
    "                     class_names: List[str] = None) -> Tuple[NDArray[np.int64], List[str]]:\n",
    "    \"\"\"\n",
    "    Calculate the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: \n",
    "        - Confusion matrix.\n",
    "        - List of class names.\n",
    "    \"\"\"\n",
    "    # Encode labels as integers\n",
    "    unique_classes = np.unique(y_true)\n",
    "    if class_names is None:\n",
    "        class_names = [str(cls) for cls in unique_classes]\n",
    "    class_to_index = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "    n_classes = len(unique_classes)\n",
    "    matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        true_idx = class_to_index[true]\n",
    "        pred_idx = class_to_index[pred]\n",
    "        matrix[true_idx][pred_idx] += 1\n",
    "\n",
    "    return matrix, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b44053",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy is the most common evaluation metric for classification problems, representing the percentage of correct predictions out of total predictions. It provides a simple measure of how often the classifier makes correct predictions across all classes.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} = \\dfrac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Samples}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7298a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true: pd.Series, y_pred: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions by comparing true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth target values. Contains the actual class labels for each sample.\n",
    "        y_pred: Estimated target as returned by a classifier. Contains the predicted class labels for each sample.\n",
    "    Returns:\n",
    "        Classification accuracy (0.0 to 1.0).\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203732bd",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions made by the classifier.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Precision} = \\dfrac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7a24c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true: pd.Series, y_pred: List[str]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate precision for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Precision values for each class.\n",
    "    \"\"\"\n",
    "    cm, _ = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / (np.sum(cm, axis=0) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6f013",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall measures the proportion of true positive predications out of all actual positive cases.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Recall} = \\dfrac{\\text{True Positives (TP)} }{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2924bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true: pd.Series, y_pred: List[str]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate recall for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Recall values for each class.\n",
    "    \"\"\"\n",
    "    cm, _ = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / (np.sum(cm, axis=1) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64683b69",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "The F1-Score is the harmonic mean of precision and recall.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{F1-Score} = 2 \\times \\dfrac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "979417f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true: pd.Series, y_pred: List[str]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate F1-score for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        F1-scores for each class.\n",
    "    \"\"\"\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2da03274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true: pd.Series, y_pred: List[str],\n",
    "             class_names: List[str] = None) -> Tuple[float, float, float, float, NDArray[np.int64]]:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics including accuracy, precision, recall, and F1-score for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "        - Overall accuracy.\n",
    "        - Average precision.\n",
    "        - Average recall.\n",
    "        - Average F1-score.\n",
    "        - Confusion matrix.\n",
    "    \"\"\"\n",
    "    cm, class_names = confusion_matrix(y_true, y_pred, class_names)\n",
    "    acc = accuracy(y_true, y_pred)\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    # print(\"Class\\tPrecision\\tRecall\\tF1-Score\")\n",
    "    # for i, class_name in enumerate(class_names):\n",
    "    #     print(f\"{class_name}\\t{prec[i]:.4f}\\t\\t{rec[i]:.4f}\\t{f1[i]:.4f}\")\n",
    "    return acc, np.mean(prec), np.mean(rec), np.mean(f1), cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4602388",
   "metadata": {},
   "source": [
    "## 9. Train Test Split\n",
    "Train test split is a fundamental model validation technique in machine learning. It divides a dataset into two separate portions: a **training set** used to train a model, and a **testing set** used to evaluate how well the model can perform on unseen data. \n",
    "\n",
    "The typical split ratio is 80% for training and 20% for testing, though this can vary (70/30 or 90/10 are also common). The key principle is that the test set must remain completely separated during model training process, and should never be used to make decisions about the model or tune parameters. \n",
    "\n",
    "The split is usually done randomly to ensure both sets are representative of the overall dataset, and many libraries (such as scikit-learn) provide build-in functions that handle this process automatically while maintaining proper randomisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52e6a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: pd.DataFrame, y: pd.Series, test_size: float = 0.2,\n",
    "                     random_state: int = None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Split arrays or matrices into random train and test subsets.\n",
    "\n",
    "    Args:\n",
    "        X: Input features, a 2D array with rows (samples) and columns (features).\n",
    "        y: Target values/labels, a 1D array with rows (samples).\n",
    "        test_size: Proportion of the dataset to include in the test split. Must be between 0.0 and 1.0. default = 0.2\n",
    "        random_state: Seed for the random number generator to ensure reproducible results. default = None\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - X_train: Training set features.\n",
    "            - X_test: Testing set features.\n",
    "            - y_train: Training set target values.\n",
    "            - y_test: Testing set target values.\n",
    "    \"\"\"\n",
    "    # Set a random seed if it exists\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Create a list of numbers from 0 to len(X)\n",
    "    indices = np.arange(len(X))\n",
    "\n",
    "    # Shuffle the indices\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Define the size of our test data from len(X)\n",
    "    test_size = int(test_size * len(X))\n",
    "\n",
    "    # Generate indices for test and train data\n",
    "    test_indices: NDArray[np.int64] = indices[:test_size]\n",
    "    train_indices: NDArray[np.int64] = indices[test_size:]\n",
    "\n",
    "    # Return: X_train, X_test, y_train, y_test\n",
    "    return X.iloc[train_indices], X.iloc[test_indices], y.iloc[train_indices], y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6aef98",
   "metadata": {},
   "source": [
    "## 10. Encapsulation\n",
    "\n",
    "Note that instead of looping over each document and calling `calculate_posterior()` for each, the `predict()` method does this for all documents at once using matrix operations:\n",
    "\n",
    "- `log_priors` is the vectorised version of the log of prior probabilities.\n",
    "\n",
    "- `word_contributions` computes, for all documents and classes, the sum of the log-likelihoods weighted by the word counts (the main computation in `calculate_posterior()`).\n",
    "\n",
    "- `log_posteriors` adds the log priors to the word contributions, just like `calculate_posterior()` would for a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultinomialNB:\n",
    "    \"\"\"\n",
    "    Multinomial Naive Bayes classifier implementation with optimised vector operations.\n",
    "\n",
    "    Attributes:\n",
    "        alpha (float): Smoothing parameter (default = 1.0)\n",
    "        priors_ (Dict[str, float]): Prior probabilities per class\n",
    "        likelihoods_ (pd.DataFrame): Likelihood probabilities (shape: [n_classes, n_features])\n",
    "        log_likelihoods_ (np.ndarray): Precomputed log-likelihoods (shape: [n_classes, n_features])\n",
    "        classes_ (List[str]): Unique class labels\n",
    "        feature_names_ (pd.Index): Feature names from training data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0) -> None:\n",
    "        \"\"\"\n",
    "        Initialise Multinomial Naive Bayes classifier.\n",
    "\n",
    "        Args:\n",
    "            alpha: Smoothing parameter for Laplace smoothing (default = 1.0).\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.priors_ = None\n",
    "        self.likelihoods_ = None\n",
    "        self.log_likelihoods_ = None\n",
    "        self.classes_ = None\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        \"\"\"\n",
    "        Train Multinomial Naive Bayes model.\n",
    "        \n",
    "        Args:\n",
    "            X: Document-term matrix (documents x features).\n",
    "            y: Target class labels.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert input to DataFrame if needed\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        self.feature_names_ = X.columns\n",
    "        self.classes_ = y.unique().tolist()\n",
    "        self.priors_ = {cls: 1/len(self.classes_) for cls in self.classes_}\n",
    "        self.likelihoods_ = self._calculate_likelihoods(X, y)\n",
    "        self.log_likelihoods_ = np.log(self.likelihoods_.values)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predict class labels for documents in X.\n",
    "\n",
    "        Args:\n",
    "            X: Document-term matrix to predict.\n",
    "\n",
    "        Returns:\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        # Align features with training data\n",
    "        X_aligned = X.reindex(columns=self.feature_names_, fill_value=0)\n",
    "\n",
    "        # Precompute log priors\n",
    "        log_priors = np.array([np.log(self.priors_[c]) for c in self.classes_])\n",
    "\n",
    "        # Vectorised prediction (calculating posteriors here)\n",
    "        word_contributions = X_aligned @ self.log_likelihoods_.T\n",
    "        log_posteriors = log_priors + word_contributions\n",
    "        max_indices = np.argmax(log_posteriors, axis=1)\n",
    "\n",
    "        return [self.classes_[idx] for idx in max_indices]\n",
    "\n",
    "    def _calculate_priors(self, y: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate prior probabilities for each class in the target variable.\n",
    "\n",
    "        Args:\n",
    "            y: Target variable containing class labels (strings).\n",
    "\n",
    "        Returns:\n",
    "            Prior probabilities for each class.\n",
    "        \"\"\"\n",
    "        return y.value_counts(normalize=True).to_dict()\n",
    "\n",
    "    def _calculate_likelihoods(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute feature likelihoods (P(x_i|y)) with Laplace smoothing.\n",
    "\n",
    "        Returns:\n",
    "            Likelihood DataFrame (classes x features)\n",
    "        \"\"\"\n",
    "        class_totals = X.groupby(y).sum()\n",
    "        total_words_per_class = class_totals.sum(axis=1)\n",
    "        vocab_size = len(X.columns)\n",
    "\n",
    "        numerator = class_totals + self.alpha\n",
    "        denominator = total_words_per_class.values[:,\n",
    "                                                   np.newaxis] + vocab_size * self.alpha\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeedb38",
   "metadata": {},
   "source": [
    "Let's check the performance on the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de54fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Entire Dataset): 0.9855\n",
      "Precision (Entire Dataset): 0.9588\n",
      "Recall (Entire Dataset): 0.9809\n",
      "F1-Score (Entire Dataset): 0.9694\n",
      "Confusion Matrix (Entire Dataset):\n",
      "[[4763   62]\n",
      " [  19  728]]\n"
     ]
    }
   ],
   "source": [
    "model = CustomMultinomialNB(alpha=1.0)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "acc, prec, rec, f1, cm = evaluate(y, y_pred)\n",
    "print(f'Accuracy (Entire Dataset): {acc:.4f}')\n",
    "print(f'Precision (Entire Dataset): {prec:.4f}')\n",
    "print(f'Recall (Entire Dataset): {rec:.4f}')\n",
    "print(f'F1-Score (Entire Dataset): {f1:.4f}')\n",
    "print(f'Confusion Matrix (Entire Dataset):\\n{cm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02edc6c",
   "metadata": {},
   "source": [
    "Train the model on 80% of the dataset, then evaluate its performance on the remaining 20% (the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7b50a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Test): 0.9677\n",
      "Precision (Test): 0.9115\n",
      "Recall (Test): 0.9615\n",
      "F1-Score (Test): 0.9343\n",
      "Confusion Matrix (Test):\n",
      "[[936  29]\n",
      " [  7 142]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = CustomMultinomialNB(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "acc, prec, rec, f1, cm = evaluate(y_test, y_pred)\n",
    "print(f'Accuracy (Test): {acc:.4f}')\n",
    "print(f'Precision (Test): {prec:.4f}')\n",
    "print(f'Recall (Test): {rec:.4f}')\n",
    "print(f'F1-Score (Test): {f1:.4f}')\n",
    "print(f'Confusion Matrix (Test):\\n{cm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeddce0",
   "metadata": {},
   "source": [
    "## 11. Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18d7f91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['ham' 'ham' 'ham' ... 'ham' 'ham' 'ham']\n",
      "Accuracy: 0.9857\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       966\n",
      "        spam       0.94      0.95      0.95       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.97      0.97      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Predictions: {y_pred}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Classification report:\\n{classification_report(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
