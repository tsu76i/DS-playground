{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12858d89",
   "metadata": {},
   "source": [
    "# Categorical Naive Bayes Classifier from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "    - [Bayes' Theorem](#bayes-theorem)\n",
    "2. [Loading Data](#2-loading-data)\n",
    "3. [Prior Probability](#3-prior-probability)\n",
    "4. [Likelihood](#4-likelihood)\n",
    "5. [Posterior Probability](#5-posterior-probability)\n",
    "6. [Prediction](#6-prediction)\n",
    "7. [Evaluation Metrics](#7-evaluation-metrics)\n",
    "    - [Binary Confusion Matrix](#binary-confusion-matrix)\n",
    "    - [Multi-Class Confusion Matrix](#multi-class-confusion-matrix)\n",
    "    - [Accuracy](#accuracy)\n",
    "    - [Precision](#precision)\n",
    "    - [Recall](#recall)\n",
    "    - [F1-Score](#f1-score)\n",
    "8. [Encapsulation](#8-encapsulation)\n",
    "9. [Comparison with Scikit-Learn](#9-comparison-with-scikit-learn) \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e83ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe5a4c",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Naive Bayes classifiers are probabilistic classification models based on Bayes' Theorem, assuming conditional independence between features given the class labels or values. Naive Bayes is a general framework; the specific variant should be chosen based on the nature of your data:\n",
    "\n",
    "- **Multinomial Naïve Bayes**: Assumes features follow multinomial distributions; ideal when features are **discrete** values.\n",
    "\n",
    "- **Gaussian Naïve Bayes**: Assumes features follow a Gaussian (normal) distribution; used for **continuous** features. Fits the model by calculating the mean and standard deviation for each class.\n",
    "\n",
    "- **Bernoulli Naïve Bayes**: Works with **binary** features (e.g., True/False, 0/1).\n",
    "\n",
    "\n",
    "### Bayes' Theorem\n",
    "Bayes' theorem describes the probability of a class $C$ given a set of features $X = (x_{1}, x_{2},\\ldots,x_{n})$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C|X) = \\dfrac{P(X|C) \\cdot P(C)}{P(X)}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $P(C|X)$: Posterior probability of class $C$ given features $X$.\n",
    "- $P(X|C)$: Likelihood of features $X$ given class $C$.\n",
    "- $P(C)$: Prior probability of class $C$.\n",
    "- $P(X)$: Probability of features $X$ (acts as a normalising constant).\n",
    "\n",
    "Naive Bayes assumes features $X = (x_{1}, x_{2},\\ldots,x_{n})$ are conditionally independent given the class $C$, thus the likelihood is expressed as:\n",
    "\\begin{align*}\n",
    "P(X|C) = P(x_{1}, x_{2}, \\dots, x_{n}|C) = \\prod_{i=1}^{n}P(x_{i}|C)\n",
    "\\end{align*}\n",
    "\n",
    "Replacing $P(X|C)$ in Bayes' theorem, the equation becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C|X) = \\dfrac{P(C) \\cdot \\prod_{i=1}^{n} P(x_{i}|C)}{P(X)}\n",
    "\\end{align*}\n",
    "\n",
    "Since $P(X)$ is constant for all classes,\n",
    "\n",
    "\\begin{align*}\n",
    "P(C|X) \\propto P(C) \\cdot \\prod_{i=1}^{n} P(x_{i}|C)\n",
    "\\end{align*}\n",
    "\n",
    "The symbol $\\propto$ denotes proportionality, meaning we ignore the denominator $P(X)$ when comparing probabilities across classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe05ae",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e00c823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Outlook",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Temperature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Humidity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Windy",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Play",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b7da28c2-f110-4a20-8327-82ae73b50a4d",
       "rows": [
        [
         "0",
         "Sunny",
         "Hot",
         "High",
         "Weak",
         "No"
        ],
        [
         "1",
         "Sunny",
         "Hot",
         "High",
         "Strong",
         "No"
        ],
        [
         "2",
         "Overcast",
         "Hot",
         "High",
         "Weak",
         "Yes"
        ],
        [
         "3",
         "Rain",
         "Mild",
         "High",
         "Weak",
         "Yes"
        ],
        [
         "4",
         "Rain",
         "Cool",
         "Normal",
         "Weak",
         "Yes"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Outlook Temperature Humidity   Windy Play\n",
       "0     Sunny         Hot     High    Weak   No\n",
       "1     Sunny         Hot     High  Strong   No\n",
       "2  Overcast         Hot     High    Weak  Yes\n",
       "3      Rain        Mild     High    Weak  Yes\n",
       "4      Rain        Cool   Normal    Weak  Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../_datasets/weather_forecast.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb633f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Play', axis=1)\n",
    "y = df['Play']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddda1d2",
   "metadata": {},
   "source": [
    "## 3. Prior Probability\n",
    "Class $C$ (`y`) has only two discrete variables: `Yes` and `No`:\n",
    "\n",
    "\\begin{align*}\n",
    "P(C=\\text{'Yes'}) = \\dfrac{\\text{Count(Yes)}}{\\text{Total Count}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "P(C=\\text{'No'}) = \\dfrac{\\text{Count(No)}}{\\text{Total Count}}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae02a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count: 14\n",
      "Counts\": {'Yes': 9, 'No': 5}\n"
     ]
    }
   ],
   "source": [
    "print(f'Total count: {len(df)}')\n",
    "print(f'Counts\": {y.value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ffb3cd",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "P(\\text{'Yes'}) = \\dfrac{9}{14} = 0.6429\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\text{'No'}) = \\dfrac{5}{14} = 0.3571\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12118d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_priors(y: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate prior probabilities for each class in the target variable.\n",
    "\n",
    "    Args:\n",
    "        y (pd.Series): Target variable containing class labels (strings).\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Prior probabilities for each class.\n",
    "    \"\"\"\n",
    "    return y.value_counts(normalize=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d32d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Yes': 0.6428571428571429, 'No': 0.35714285714285715}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_priors(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e540a",
   "metadata": {},
   "source": [
    "## 4. Likelihood\n",
    "\n",
    "The likelihood quantifies how well parameter $\\theta$ explain the observed data. It is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta|x) = f(x|\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "where $f$ is the probability density/mass function.\n",
    "\n",
    "For each feature value and class, we calculate:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\text{Feature = value|Class})\n",
    "\\end{align*}\n",
    "\n",
    "For example:\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\text{Outlook = 'Sunny'|Play = 'Yes'}) = \\dfrac{\\text{Count(Outlook = 'Sunny'|Play = 'Yes')} + \\alpha}{\\text{Count(Play = 'Yes)} + n \\cdot \\alpha}\n",
    "\\end{align*}\n",
    "\n",
    "where $n$ is the number of features and $\\alpha$ is the smoothing parameter to handle zero probabilities (**Laplace Smoothing**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceff3cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihoods(X: pd.DataFrame, y: pd.Series,\n",
    "                          alpha: float = 1.0) -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Calculate conditional probabilities for feature values given each class.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix (DataFrame with categorical columns)\n",
    "        y (pd.Series): Target variable (Series of class labels)\n",
    "        alpha (float): Smoothing parameter for Laplace smoothing (default=1.0)\n",
    "\n",
    "    Returns:\n",
    "        Nested dictionary with structure:\n",
    "        {feature_name: {class_label: {feature_value: probability}}}\n",
    "    \"\"\"\n",
    "\n",
    "    likelihoods = {}\n",
    "    for feature in X.columns:  # For each column of X\n",
    "        likelihoods[feature] = {}\n",
    "        # Unique feature values in each column\n",
    "        unique_features = X[feature].unique()\n",
    "\n",
    "        for c in y.unique():  # Unique target values of y\n",
    "            class_subset = X[y == c]\n",
    "            total = len(class_subset)  # Count(C)\n",
    "\n",
    "            # Count frequencies (e.g., {'Sunny':3, 'Rain':2} for class 'No')\n",
    "            value_counts = class_subset[feature].value_counts()\n",
    "\n",
    "            # All features values are included, even if missing in subset\n",
    "            value_counts = value_counts.reindex(unique_features, fill_value=0)\n",
    "            probas = round((value_counts + alpha) / \\\n",
    "                (total + len(value_counts) * alpha), 4)\n",
    "\n",
    "            likelihoods[feature][c] = probas.to_dict()\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7414db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Outlook': {'No': {'Sunny': 0.5, 'Overcast': 0.125, 'Rain': 0.375},\n",
       "  'Yes': {'Sunny': 0.25, 'Overcast': 0.4167, 'Rain': 0.3333}},\n",
       " 'Temperature': {'No': {'Hot': 0.375, 'Mild': 0.375, 'Cool': 0.25},\n",
       "  'Yes': {'Hot': 0.25, 'Mild': 0.4167, 'Cool': 0.3333}},\n",
       " 'Humidity': {'No': {'High': 0.7143, 'Normal': 0.2857},\n",
       "  'Yes': {'High': 0.3636, 'Normal': 0.6364}},\n",
       " 'Windy': {'No': {'Weak': 0.4286, 'Strong': 0.5714},\n",
       "  'Yes': {'Weak': 0.6364, 'Strong': 0.3636}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_likelihoods(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6115f54",
   "metadata": {},
   "source": [
    "## 5. Posterior Probability\n",
    "As we discussed [above](#1-introduction), the formula of posterior probability is:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "P(C|X) \\propto P(C) \\prod_{i=1}^{n} P(x_{i}|C)\n",
    "\\end{align*}\n",
    "\n",
    "To prevent underflow, we use log probabilities:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{log } P(C|X) = \\text{log } P(C) + \\sum_{i=1}^{n} \\text{log } P(x_{i}|C)\n",
    "\\end{align*}\n",
    "\n",
    "In the following code, `.get(category, 1e-9)` tries to retrieve the probability for the specific value category from this dictionary. If the category was not seen in the training data for this class (i.e., it's missing from the dictionary), it returns a very small default value (1e-9) instead of raising an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10df8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posterior(x: Dict[str, str], priors: Dict[str, float], likelihoods: Dict[str, Dict[str, Dict[str, float]]],\n",
    "                        X_columns: List[str], classes: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate log-posterior probabilities for all classes given a sample.\n",
    "\n",
    "    Args:\n",
    "        x (Dict[str, str]): Input sample as dictionary {feature: value}.\n",
    "        priors (Dict[str, float]): Prior probabilities from calculate_priors().\n",
    "        likelihoods (Dict[str, Dict[str, Dict[str, float]]]): Conditional probabilities from calculate_likelihoods().\n",
    "        X_columns (List[str]): List of feature names.\n",
    "        classes (List[str]): List of possible class labels.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: Dictionary mapping each class to its log-posterior probability.\n",
    "    \"\"\"\n",
    "\n",
    "    log_posteriors = {}\n",
    "    for c in classes:\n",
    "        log_proba = np.log(priors[c])  # Log of prior\n",
    "        for feature in X_columns:  # Sum of the likelihood for each x given c\n",
    "            category = x[feature]\n",
    "            # Avoid log(0) if the feature does not exist\n",
    "            proba = likelihoods[feature][c].get(category, 1e-9)\n",
    "            log_proba += np.log(proba)\n",
    "        log_posteriors[c] = round(float(log_proba), 4)\n",
    "    return log_posteriors  # log-posterior probabilities for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7261fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'No': -3.8873, 'Yes': -4.6781}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_posterior(X.iloc[0], calculate_priors(\n",
    "    y), calculate_likelihoods(X, y), X.columns, y.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27707c",
   "metadata": {},
   "source": [
    "## 6. Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e7d981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: pd.DataFrame, y: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Predict class labels using Categorical Naive Bayes.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature matrix.\n",
    "        y (pd.Seris): Target variable.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Predicted class labels.\n",
    "    \"\"\"\n",
    "    priors = calculate_priors(y)\n",
    "    likelihoods = calculate_likelihoods(X, y)\n",
    "    classes = y.unique()\n",
    "    X_columns = X.columns\n",
    "\n",
    "    predictions = []\n",
    "    for row in X.itertuples(index=False):\n",
    "        posterior = calculate_posterior(\n",
    "            row._asdict(), priors, likelihoods, X_columns, classes)\n",
    "        predictions.append(max(posterior, key=posterior.get))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faafdca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'Yes',\n",
       " 'No']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca8e32",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics\n",
    "### Binary Confusion Matrix\n",
    "In a confusion matrix, the terms True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) describe the classification performance for binary classification. \n",
    "\n",
    "|                     | Predicted Negative  | Predicted Positive  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Negative** | True Negative (TN)  | False Positive (FP) |\n",
    "| **Actual Positive** | False Negative (FN) | True Positive (TP)  |\n",
    "\n",
    "\n",
    "1. True Positive (TP): The number of instances correctly predicted as positive (e.g., a disease correctly identified).\n",
    "\n",
    "2. True Negative (TN): The number of instances correctly predicted as negative (e.g., no disease correctly identified).\n",
    "\n",
    "3. False Positive (FP): The number of instances incorrectly predicted as positive (e.g., predicting disease when there isn't any).\n",
    "\n",
    "4. False Negative (FN): The number of instances incorrectly predicted as negative (e.g., missing a disease when it exists).\n",
    "\n",
    "### Multi-Class Confusion Matrix\n",
    "For multi-class classification, the concepts can be extended by treating one class as the \"positive\" class and all others as \"negative\" classes in a one-vs-all approach. Rows represent the actual classes (true labels), and columns represent the predicted classes. For a class $C$,\n",
    "1. True Positive (TP): The count in the diagonal cell corresponding to class $C$ ($\\text{matrix} [C][C]$).\n",
    "2. False Positive (FP): The sum of the column for class $C$, excluding the diagonal ($\\sum(\\text{matrix} [:, C]) - \\text{matrix} [C][C]$).\n",
    "3. False Negative (FN): The sum of the row for class $C$, excluding the diagonal ($\\sum(\\text{matrix} [C, :]) - \\text{matrix} [C][C]$).\n",
    "4. True Negative (TN): All other cells not in the row or column for class $C$ ($\\text{total} - (FP + FN + TP)$).\n",
    "\n",
    "|                  | Predicted Class 0 | Predicted Class 1 | Predicted Class 2 |\n",
    "| ---------------- | ----------------- | ----------------- | ----------------- |\n",
    "| **True Class 0** | 5                 | 2                 | 0                 |\n",
    "| **True Class 1** | 1                 | 6                 | 1                 |\n",
    "| **True Class 2** | 0                 | 2                 | 7                 |\n",
    "\n",
    "\n",
    "For Class 0:\n",
    "- TP = 5 (diagonal element for Class 0)\n",
    "- FP = 1 (sum of column 0 minus TP: 1 + 0)\n",
    "- FN = 2 (sum of row 0 minus TP: 2 + 0)\n",
    "- TN = 6 + 1 + 2 + 7 = 16 (all other cells not in row 0 or column 0)\n",
    "\n",
    "For Class 1:\n",
    "- TP = 6 (diagonal element for Class 1)\n",
    "- FP = 4 (sum of column 1 minus TP: 2 + 2)\n",
    "- FN = 2 (sum of row 1 minus TP: 1 + 1)\n",
    "- TN = 5 + 0 + 0 + 7 = 12 (all other cells not in row 1 or column 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54749d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true: pd.Series, y_pred: List[str],\n",
    "                     class_names: List[str] = None) -> Tuple[NDArray[np.int64], List[str]]:\n",
    "    \"\"\"\n",
    "    Calculate the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        y_true (pd.Series): True labels.\n",
    "        y_pred (List[str]): Predicted labels.\n",
    "        class_names (List[str], optional): List of class names. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: \n",
    "        - NDArray[np.int64]: Confusion matrix.\n",
    "        - List[str]: List of class names.\n",
    "    \"\"\"\n",
    "    # Encode labels as integers\n",
    "    unique_classes = np.unique(y_true)\n",
    "    if class_names is None:\n",
    "        class_names = [str(cls) for cls in unique_classes]\n",
    "    class_to_index = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "    n_classes = len(unique_classes)\n",
    "    matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        true_idx = class_to_index[true]\n",
    "        pred_idx = class_to_index[pred]\n",
    "        matrix[true_idx][pred_idx] += 1\n",
    "\n",
    "    return matrix, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a3f3dc",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy is the most common evaluation metric for classification problems, representing the percentage of correct predictions out of total predictions. It provides a simple measure of how often the classifier makes correct predictions across all classes.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} = \\dfrac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Samples}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63885798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true: pd.Series, y_pred: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions by comparing true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true (pd.Series): Ground truth target values. Contains the actual class labels for each sample.\n",
    "        y_pred (List[str])): Estimated target as returned by a classifier. Contains the predicted class labels for each sample.\n",
    "    Returns:\n",
    "        float: Classification accuracy as a percentage (0.0 to 100.0).\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c02142f",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions made by the classifier.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Precision} = \\dfrac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485b87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true: pd.Series, y_pred: List[str]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate precision for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true (pd.Series): True labels.\n",
    "        y_pred (List[str]): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        NDArray[np.float64]: Precision values for each class.\n",
    "    \"\"\"\n",
    "    cm, _ = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / (np.sum(cm, axis=0) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b9b5e",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall measures the proportion of true positive predications out of all actual positive cases.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Recall} = \\dfrac{\\text{True Positives (TP)} }{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4035fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true: pd.Series, y_pred: List[str]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate recall for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true (pd.Series): True labels.\n",
    "        y_pred (List[str]): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        NDArray[np.float64]: Recall values for each class.\n",
    "    \"\"\"\n",
    "    cm, _ = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / (np.sum(cm, axis=1) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad47296",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "The F1-Score is the harmonic mean of precision and recall.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{F1-Score} = 2 \\times \\dfrac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5851d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true: pd.Series, y_pred: List[str]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate F1-score for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true (pd.Series): True labels.\n",
    "        y_pred (List[str]): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        NDArray[np.float64]: F1-scores for each class.\n",
    "    \"\"\"\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b599f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true: pd.Series, y_pred: List[str],\n",
    "             class_names: List[str] = None) -> Tuple[float, float, float, float, NDArray[np.int64]]:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics including accuracy, precision, recall, and F1-score for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true (pd.Series): True labels.\n",
    "        y_pred (List[str]): Predicted labels.\n",
    "        class_names (List[str], optional): List of class names. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "        - float: Overall accuracy.\n",
    "        - float: Average precision.\n",
    "        - float: Average recall.\n",
    "        - float: Average F1-score.\n",
    "        - NDArray[np.int64]: Confusion matrix.\n",
    "    \"\"\"\n",
    "    cm, class_names = confusion_matrix(y_true, y_pred, class_names)\n",
    "    acc = accuracy(y_true, y_pred)\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    # print(\"Class\\tPrecision\\tRecall\\tF1-Score\")\n",
    "    # for i, class_name in enumerate(class_names):\n",
    "    #     print(f\"{class_name}\\t{prec[i]:.4f}\\t\\t{rec[i]:.4f}\\t{f1[i]:.4f}\")\n",
    "    return acc, np.mean(prec), np.mean(rec), np.mean(f1), cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d59e1",
   "metadata": {},
   "source": [
    "## 8. Encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00b37ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCategoricalNB:\n",
    "    \"\"\"\n",
    "    Categorical Naive Bayes classifier for discrete features.\n",
    "\n",
    "    This implementation handles categorical features directly without requiring label encoding.\n",
    "    Uses Laplace smoothing to handle unseen feature values during prediction.\n",
    "\n",
    "    Attributes:\n",
    "        alpha (float): Smoothing parameter (default=1.0).\n",
    "        priors_ (Dict[str, float]): Class prior probabilities.\n",
    "        likelihoods_ (Dict[str, Dict[str, Dict[str, float]]]): Feature likelihood probabilities.\n",
    "        classes_ (NDArray[np.str_]): Unique class labels.\n",
    "        feature_names_ (List[str]): Feature names from training data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float = 1.0) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the Categorical Naive Bayes classifier.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): Smoothing parameter for Laplace smoothing (default=1.0).\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.priors_ = None\n",
    "        self.likelihoods_ = None\n",
    "        self.classes_ = None\n",
    "        self.feature_names_ = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "        \"\"\"\n",
    "        Fit the model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Training data (categorical).\n",
    "            y (pd.Series): Target values (class labels).\n",
    "\n",
    "        Computes:\n",
    "            - Class prior probabilities (priors_).\n",
    "            - Feature likelihood probabilities (likelihoods_).\n",
    "        \"\"\"\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.feature_names_ = X.columns.to_list()\n",
    "        self.priors_ = self._calculate_priors(y)\n",
    "        self.likelihoods_ = self._calculate_likelihoods(X, y)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"\n",
    "        Predict class labels using Categorical Naive Bayes.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Predicted class labels.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If model hasn't been fitted.\n",
    "        \"\"\"\n",
    "        if self.priors_ is None or self.likelihoods_ is None:\n",
    "            raise ValueError('Model not fitted. Call .fit() first.')\n",
    "\n",
    "        predictions = []\n",
    "        for row in X.itertuples(index=False):\n",
    "            log_posteriors = self._calculate_posteriors(row._asdict())\n",
    "            predictions.append(max(log_posteriors, key=log_posteriors.get))\n",
    "        return predictions\n",
    "\n",
    "    def _calculate_priors(self, y: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate prior probabilities for each class in the target variable.\n",
    "\n",
    "        Args:\n",
    "            y (pd.Series): Target variable containing class labels (strings).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Prior probabilities for each class.\n",
    "        \"\"\"\n",
    "        return y.value_counts(normalize=True).to_dict()\n",
    "\n",
    "    def _calculate_likelihoods(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "        \"\"\"\n",
    "        Calculate conditional probabilities for feature values given each class.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix (DataFrame with categorical columns)\n",
    "            y (pd.Series): Target variable (Series of class labels)\n",
    "\n",
    "        Returns:\n",
    "            Nested dictionary with structure:\n",
    "            {feature_name: {class_label: {feature_value: probability}}}\n",
    "        \"\"\"\n",
    "        likelihoods = {}\n",
    "        for feature in self.feature_names_:  # For each column of X\n",
    "            likelihoods[feature] = {}\n",
    "\n",
    "            # Unique feature values in each column\n",
    "            unique_features = X[feature].unique()\n",
    "\n",
    "            for c in self.classes_:  # Unique target values of y\n",
    "                class_subset = X[y == c]\n",
    "                total = len(class_subset)  # Count(C)\n",
    "\n",
    "                # Count frequencies (e.g., {'Sunny':3, 'Rain':2} for class 'No')\n",
    "                value_counts = class_subset[feature].value_counts()\n",
    "\n",
    "                # All features values are included, even if missing in subset\n",
    "                value_counts = value_counts.reindex(\n",
    "                    unique_features, fill_value=0)\n",
    "\n",
    "                probas = round((value_counts + self.alpha) / \\\n",
    "                    (total + len(unique_features) + self.alpha), 4)\n",
    "\n",
    "                likelihoods[feature][c] = probas.to_dict()\n",
    "\n",
    "        return likelihoods\n",
    "\n",
    "    def _calculate_posteriors(self, x: Dict[str, str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate log-posterior probabilities for all classes given a sample.\n",
    "\n",
    "        Args:\n",
    "            x (Dict[str, str]): Input sample as dictionary {feature: value}.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary mapping each class to its log-posterior probability.\n",
    "        \"\"\"\n",
    "        log_posteriors = {}\n",
    "        for c in self.classes_:\n",
    "            log_proba = np.log(self.priors_[c])  # Log of prior\n",
    "            for feature in self.feature_names_:  # Sum of the likelihood for x given c\n",
    "                category = x[feature]\n",
    "                # Avoid log(0) if the feature does not exist\n",
    "                proba = self.likelihoods_[feature][c].get(category, 1e-9)\n",
    "                log_proba += np.log(proba)\n",
    "            log_posteriors[c] = round(log_proba, 4)\n",
    "        return log_posteriors  # log-posterior probabilities for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0271ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9286\n",
      "Precision: 0.9500\n",
      "Recall: 0.9000\n",
      "F1-Score: 0.9181\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [0 9]]\n"
     ]
    }
   ],
   "source": [
    "model = CustomCategoricalNB()\n",
    "model.fit(X, y)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "acc, prec, rec, f1, cm = evaluate(y, predictions)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824d72a",
   "metadata": {},
   "source": [
    "## 9. Comparison with Scikit-Learn\n",
    "Scikit-Learn's `CategoricalNB` only works with integer-encoded categorical features, therefore all categorical features and the target variable need to be converted to integers using `LabelEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b0778d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Outlook",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Temperature",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Humidity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Windy",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8e90c96a-5e24-4dba-a409-21a6bb0255ac",
       "rows": [
        [
         "0",
         "2",
         "1",
         "0",
         "1"
        ],
        [
         "1",
         "2",
         "1",
         "0",
         "0"
        ],
        [
         "2",
         "0",
         "1",
         "0",
         "1"
        ],
        [
         "3",
         "1",
         "2",
         "0",
         "1"
        ],
        [
         "4",
         "1",
         "0",
         "1",
         "1"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Outlook  Temperature  Humidity  Windy\n",
       "0        2            1         0      1\n",
       "1        2            1         0      0\n",
       "2        0            1         0      1\n",
       "3        1            2         0      1\n",
       "4        1            0         1      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical features and target\n",
    "label_encoders = {}\n",
    "for column in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Play', axis=1)\n",
    "y = df['Play']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a717e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1 1 1 1 0 1 1 1 1 1 0]\n",
      "Accuracy: 0.9286\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.80      0.89         5\n",
      "           1       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CategoricalNB()\n",
    "model.fit(X, y)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "accuracy = model.score(X, y)\n",
    "print(f'Predictions: {predictions}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Classification report:\\n{classification_report(y, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25f85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
