{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e713b74",
   "metadata": {},
   "source": [
    "# Categorical Gradient Boosting Machine from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "    - [Advantages](#advantages)\n",
    "    - [Limitations](#limitations)\n",
    "    - [Steps](#steps)\n",
    "1. [Loading Data](#2-loading-data)\n",
    "1. [Ordered Target Encoding](#3-ordered-target-encoding)\n",
    "1. [Finding the Best Split](#4-finding-the-best-split)\n",
    "1. [Building Trees](#5-building-trees)\n",
    "1. [Predictions (Trees)](#6-predictions-trees)\n",
    "1. [Training Model](#7-training-model)\n",
    "1. [Final Predictions](#8-final-predictions)\n",
    "1. [Evaluation Metrics](#9-evaluation-metrics)\n",
    "    - [Binary Confusion Matrix](#binary-confusion-matrix)\n",
    "    - [Multi-Class Confusion Matrix](#multi-class-confusion-matrix)\n",
    "    - [Accuracy](#accuracy)\n",
    "    - [Precision](#precision)\n",
    "    - [Recall](#recall)\n",
    "    - [F1-Score](#f1-score)\n",
    "1. [Encapsulation](#10-encapsulation)\n",
    "1. [Comparison with catboost](#11-comparison-with-catboost)\n",
    "1. [References](#12-references)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d73311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict, Any, Optional\n",
    "from tqdm import tqdm\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb7b07",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "**Categorical Gradient Boosting (CatBoost)** is the application of gradient boosting algorithms to datasets containing categorical features. CatBoost uses advanced target encoding technique called **ordered target statistics** to convert categorical values to numeric representations, while avoiding target leakage and overfitting. The encoding is based on the mean target value for each category, calculated in a way that prevents the model from '*seeing the future*' (i.e., using information from the current or future row to encode the category).\n",
    "\n",
    "\n",
    "**Gradient Boosting Machine (GBM)** is an ensemble machine learning model that builds a strong predictive model by sequentially combining multiple weak models (typically decision trees) in a stage-wise manner. The core idea is to iteratively add new models that correct the errors made by the existing ensemble, thereby improving overall predictive accuracy.\n",
    "\n",
    "Suppose we have a dataset ${(x_i, y_i)}^n_{i=1}$ where $x_i$ are the features and $y_i$ are the target values. The goal of gradient boosting is to find a function $F(x)$ that minimises a given differentiable loss function $L(y, F(x))$:\n",
    "\n",
    "\\begin{align*}\n",
    "    F(x) = F_0(x) + \\sum^{M}_{m=1}\\gamma_m h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial model (e.g., the mean of $y$).\n",
    "- $\\gamma_i$: Weight (step size) for the $m$-th weak learner, typically determined by minimising the loss function along the direction of $h_m(x)$.\n",
    "- $M$: Number of boosting iterations (e.g., the number of weak learners).\n",
    "- $h_m$: prediction from the $m$-th weak learner (e.g., a decision tree).\n",
    "\n",
    "\n",
    "### Advantages\n",
    "- Can process categorical variables directly without manual encoding.\n",
    "- Ordered boosting and target encoding reduce overfitting.\n",
    "- Symmetric tree structures enable efficient and swift predictions.\n",
    "\n",
    "### Limitations\n",
    "- Memory consumption may be significant especially for large datasets with many categorical features.\n",
    "- Computationally expensive and time-consuming for large datasets or a large number of trees.\n",
    "\n",
    "\n",
    "### Steps\n",
    "1. Identify categorical features in the dataset.\n",
    "    - Encode categorical variables.\n",
    "        - Use *target statistics* or *ordered target encoding* to avoid target leakage.\n",
    "        - For each categorical variable, the encoding for a given row is computed *using only information from previous rows* (ordered by a random permutation).\n",
    "1. Initialise the model:\n",
    "    - Start with a simple model, typically a constant value:\n",
    "        - For regression: the mean of the target variable.\n",
    "        - For binary classification: the log-odds of the positive classes.\n",
    "1. Calculate residuals (Negative Gradients):\n",
    "    - For each iteration, compute the residuals, which are the negative gradients of the loss function with respect to the current predictions.\n",
    "    - This step can be generalised to any differentiable loss function, not just the mean squared error.\n",
    "1. Fit a new weak model to predict the residuals:\n",
    "    - Train a weak learner (typically a symmetric or oblivious decision tree) to predict the rediduals.\n",
    "    - Tree splits involving categorical features:\n",
    "        - The algorithm searches for the optimal way to split categories, often by grouping categories based on theri target statistics.\n",
    "        - CatBoost considers all possible binary splits of the categories, using their encoded values to find the best split.\n",
    "1. Update the model:\n",
    "    - Add the predictions from the new weak learners to the current model's predictions, scaled by a learning rate (shrinkage parameter).\n",
    "    - The update is performed using the encoded categorical features and the ordered boosting mechanism.\n",
    "1. Repeat steps 3-5 for a pre-defined number of iterations.\n",
    "    At each iteration, categorical encodings and tree splits are recalculated as necessary, always using the ordered approach to avoid target leakage.\n",
    "1. Final prediction:\n",
    "    - The final model is the sum of the initial prediction and the scaled outputs of all weak learners.\n",
    "    - For categorical features, the final prediction includes the effect of native encoding and optimal splits discovered during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068774ff",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "Retrieved from [GitHub - ivalada/Categorical-Naive-Bayes-Implementation/dataset](https://github.com/ivalada/Categorical-Naive-Bayes-Implementation/tree/main/dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e77004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "size",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "material",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "color",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sleeves",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "demand",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "46ff7e2c-3c87-4c2e-9b09-ef0d13013f8e",
       "rows": [
        [
         "0",
         "S",
         "nylon",
         "white",
         "long",
         "medium"
        ],
        [
         "1",
         "XL",
         "polyester",
         "cream",
         "short",
         "high"
        ],
        [
         "2",
         "S",
         "silk",
         "blue",
         "short",
         "medium"
        ],
        [
         "3",
         "M",
         "cotton",
         "black",
         "short",
         "medium"
        ],
        [
         "4",
         "XL",
         "polyester",
         "orange",
         "long",
         "medium"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>material</th>\n",
       "      <th>color</th>\n",
       "      <th>sleeves</th>\n",
       "      <th>demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>nylon</td>\n",
       "      <td>white</td>\n",
       "      <td>long</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XL</td>\n",
       "      <td>polyester</td>\n",
       "      <td>cream</td>\n",
       "      <td>short</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>silk</td>\n",
       "      <td>blue</td>\n",
       "      <td>short</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>cotton</td>\n",
       "      <td>black</td>\n",
       "      <td>short</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XL</td>\n",
       "      <td>polyester</td>\n",
       "      <td>orange</td>\n",
       "      <td>long</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  size   material   color sleeves  demand\n",
       "0    S      nylon   white    long  medium\n",
       "1   XL  polyester   cream   short    high\n",
       "2    S       silk    blue   short  medium\n",
       "3    M     cotton   black   short  medium\n",
       "4   XL  polyester  orange    long  medium"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../_datasets/material.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb038b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"demand\", axis=1)\n",
    "y = df[\"demand\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4b1e1",
   "metadata": {},
   "source": [
    "## 3. Ordered Target Encoding\n",
    "The following function `ordered_target_encode()` transforms a categorical column into a numerical feature by encoding each value based on the mean of the target variable for all *previous* occurences of that category. This prevents target leakage and is especially effective for boosting algorithms. Given a dataset with $n$ rows, the ordered target encoding for row $i$ is:\n",
    "\n",
    "$$\n",
    "\\text{Encoded}_i = \n",
    "\\begin{cases}\n",
    "\\frac{S_{x_i}^{<i}}{N_{x_i}^{<i}}, & \\text{if } N_{x_i}^{<i} > 0 \\\\\n",
    "\\bar{y}, & \\text{if } N_{x_i}^{<i} = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i$: Categorical feature for each observation $i$.\n",
    "- $y_i$: Target variable for each observation $i$.\n",
    "- $S_{x_i}^{<i}$: Sum of target values for category $x_i$ in rows before $i$.\n",
    "- $N_{x_i}^{<i}$: Count of category $x_i$ in rows before $i$.\n",
    "- $\\bar y$: Global mean of the target variable.\n",
    "\n",
    "For example, suppose we have the following dataset:\n",
    "\n",
    "| Row | Category | Target |\n",
    "|-----|----------|--------|\n",
    "| 1   | A        | 10     |\n",
    "| 2   | B        | 20     |\n",
    "| 3   | A        | 30     |\n",
    "| 4   | B        | 40     |\n",
    "| 5   | A        | 50     |\n",
    "\n",
    "First, compute the global mean:\n",
    "\n",
    "$$\n",
    "\\bar y = \\dfrac{10+20+30+40+50}{5} = 30\n",
    "$$\n",
    "\n",
    "Now, compute the ordered target encoding for each row:\n",
    "\n",
    "| Row | Category | Target | Previous Sums $S_{x_i}^{<i}$ | Previous Counts $N_{x_i}^{<i}$ | Encoded Value |\n",
    "|-----|----------|--------|----------------------------|------------------------------|---------------|\n",
    "| 1   | A        | 10     | 0                          | 0                            | 30            |\n",
    "| 2   | B        | 20     | 0                          | 0                            | 30            |\n",
    "| 3   | A        | 30     | 10                         | 1                            | 10            |\n",
    "| 4   | B        | 40     | 20                         | 1                            | 20            |\n",
    "| 5   | A        | 50     | 10 + 30 = 40               | 2                            | 20            |\n",
    "\n",
    "- Row 1(A): No previous A, so use global mean $30$.\n",
    "- Row 2(B): No previous B, so use global mean $30$.\n",
    "- Row 3(A): One previous A (row 1, target $10$), so encoding is $\\frac{10}{1} = 10$.\n",
    "- Row 4(B): One previous B (row 2, target $20$), so encoding is $\\frac{20}{1} = 20$.\n",
    "- Row 5(A): Two previous A (row 1 and 3, targets $10$ and $30$), so encoding is $\\frac{10 + 30}{2} = 20$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb48c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_target_encode(df: pd.DataFrame, col: str, target: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Vectorised ordered target encoding for a single categorical column.\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame containing the data.\n",
    "        col: Name of the categorical column to encode.\n",
    "        target: Name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        Encoded column with ordered target encoding.\n",
    "    \"\"\"\n",
    "    global_mean = df[target].mean()\n",
    "    # Prepare cumulative sum and count per category\n",
    "    cumsum = df.groupby(col)[target].cumsum() - df[target]\n",
    "    cumcnt = df.groupby(col).cumcount()\n",
    "    enc = cumsum / cumcnt.replace(0, np.nan)\n",
    "    enc.fillna(global_mean, inplace=True)\n",
    "    return enc\n",
    "\n",
    "\n",
    "def apply_ordered_target_encoding(\n",
    "    df: pd.DataFrame, cat_cols: list[str], target: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply ordered target encoding to all categorical columns in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame containing the data.\n",
    "        cat_cols: List of categorical column names to encode.\n",
    "        target: Name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with encoded categorical columns.\n",
    "    \"\"\"\n",
    "    df_enc = df.copy()\n",
    "    for col in cat_cols:\n",
    "        df_enc[col] = ordered_target_encode(df_enc, col, target)\n",
    "    return df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea82f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered Target Encoding for 'Category':\n",
      "0    30.0\n",
      "1    30.0\n",
      "2    10.0\n",
      "3    20.0\n",
      "4    20.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "sample_data = {\n",
    "    \"Category\": [\"A\", \"B\", \"A\", \"B\", \"A\"],\n",
    "    \"Target\": [10, 20, 30, 40, 50],\n",
    "}\n",
    "df_sample = pd.DataFrame(sample_data)\n",
    "\n",
    "print(\n",
    "    f\"Ordered Target Encoding for 'Category':\\n{ordered_target_encode(df_sample, 'Category', 'Target')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef617c8f",
   "metadata": {},
   "source": [
    "## 4. Finding the Best Split\n",
    "The following functions search for the best feature and threshold to split data into two, such that the sum of variance of the resulting groups is minimised.\n",
    "\n",
    "For each feature $f$, consider all possible split points $s$. For each split:\n",
    "\n",
    "- Left group: $\\mathcal{L} = \\{i: X_{i, f} \\leq s\\}$\n",
    "- Right group: $\\mathcal{R} = \\{i: X_{i, f} > s\\}$\n",
    "\n",
    "Compute the weighted variance:\n",
    "\\begin{align*}\n",
    "    \\text{score}(f, s) = |\\mathcal{L}| \\cdot \\text{Var}(y_{\\mathcal{L}}) + |\\mathcal{R}| \\cdot \\text{Var}(y_{\\mathcal{R}})\n",
    "\\end{align*}\n",
    "\n",
    "Select the split with the lowest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b07d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(\n",
    "    X: NDArray[np.float64], y: NDArray[np.float64]\n",
    ") -> Tuple[int | None, float | None, NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Find the best feature and split value to minimise the weighted variance of the target.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        y: Target values of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (best_feature, best_split, best_left, best_right)\n",
    "            best_feature: Index of the best feature to split on.\n",
    "            best_split: Value of the best split.\n",
    "            best_left: Boolean mask for samples going to the left child.\n",
    "            best_right: Boolean mask for samples going to the right child.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    best_feature, best_split, best_score, best_left, best_right = (\n",
    "        None,\n",
    "        None,\n",
    "        np.inf,\n",
    "        None,\n",
    "        None,\n",
    "    )\n",
    "    for feature_idx in range(n_features):\n",
    "        values = np.unique(X[:, feature_idx])\n",
    "        if len(values) == 1:\n",
    "            continue\n",
    "        # Try all midpoints between sorted unique values\n",
    "        sorted_vals = np.sort(values)\n",
    "        splits = (sorted_vals[:-1] + sorted_vals[1:]) / 2\n",
    "        for split_val in splits:\n",
    "            left_idx = X[:, feature_idx] <= split_val\n",
    "            right_idx = ~left_idx\n",
    "            if not left_idx.any() or not right_idx.any():\n",
    "                continue\n",
    "            score = (\n",
    "                np.var(y[left_idx]) * left_idx.sum()\n",
    "                + np.var(y[right_idx]) * right_idx.sum()\n",
    "            )\n",
    "            if score < best_score:\n",
    "                best_feature = feature_idx\n",
    "                best_split = split_val\n",
    "                best_score = score\n",
    "                best_left = left_idx\n",
    "                best_right = right_idx\n",
    "    return best_feature, best_split, best_left, best_right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eaf60d",
   "metadata": {},
   "source": [
    "## 5. Building Trees\n",
    "This function recursively constructs a regression tree by splitting the data at each node using the best split found, until stopping criteria are met. \n",
    "\n",
    "Stopping Criteria:\n",
    "- Maximum depth reached.\n",
    "- Not enough samples to split.\n",
    "- All targets are identical.\n",
    "\n",
    "If stopping criteria are met, return the mean of $y$ at the node. Otherwise, split the data at the best feature and split value, and recursively build left and right subtrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aab5bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(\n",
    "    X: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    max_depth: int,\n",
    "    min_samples_split: int,\n",
    "    depth: int = 0,\n",
    ") -> Dict[str, Any] | float:\n",
    "    \"\"\"\n",
    "    Recursively build a decision tree for regression.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        y: Target values of shape (n_samples,).\n",
    "        max_depth: Maximum depth of the tree.\n",
    "        min_samples_split: Minimum number of samples required to split.\n",
    "        depth: Current depth of the tree. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        A decision tree node represented as a dictionary or a leaf value (float).\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    if depth >= max_depth or n_samples < min_samples_split or np.all(y == y[0]):\n",
    "        return np.mean(y)\n",
    "    best_feature, best_split, best_left, best_right = find_best_split(X, y)\n",
    "    if best_feature is None:\n",
    "        return np.mean(y)\n",
    "    return {\n",
    "        \"feature\": best_feature,\n",
    "        \"split\": best_split,\n",
    "        \"left\": build_tree(\n",
    "            X[best_left], y[best_left], max_depth, min_samples_split, depth + 1\n",
    "        ),\n",
    "        \"right\": build_tree(\n",
    "            X[best_right], y[best_right], max_depth, min_samples_split, depth + 1\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13686b01",
   "metadata": {},
   "source": [
    "## 6. Predictions (Trees)\n",
    "The `predict_tree()` function predicts the output for a single data sample $x$ by traversing the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c576492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree(node: Dict[str, Any], row: NDArray[np.float64]) -> float:\n",
    "    \"\"\"\n",
    "    Predict the target value for a single sample using the decision tree.\n",
    "\n",
    "    Args:\n",
    "        node: Decision tree node or leaf value.\n",
    "        row: Feature values of the sample.\n",
    "\n",
    "    Returns:\n",
    "        Predicted target value.\n",
    "    \"\"\"\n",
    "    while isinstance(node, dict):\n",
    "        if row[node[\"feature\"]] <= node[\"split\"]:\n",
    "            node = node[\"left\"]\n",
    "        else:\n",
    "            node = node[\"right\"]\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ac628",
   "metadata": {},
   "source": [
    "## 7. Training Model\n",
    "It fits a gradient boosting model with the multi-class log-loss (cross-entropy) as a loss function. At each step, the negative gradient (residuals) is approxiated by a tree.\n",
    "\n",
    "Let $F_{i, k}$ be the current score (logit) for sample $i$ and class $k$. At each boosting iteration:\n",
    "1. Compute softmax probabilities:\n",
    "    $$\n",
    "    P_{i, k} = \\dfrac{e(F_{i, k})}{\\sum^{K}_{l=1}e(F_{i, k})}\n",
    "    $$\n",
    "2. Compute residuals for each class:\n",
    "    $$\n",
    "    r_{i, k} = y_{i, k} - P_{i, k}\n",
    "    $$\n",
    "    where $y_{i, k}$ is $1$ if $y_i = k$, else $0$.\n",
    "3. Fit a tree to $r_{i, k}$ for each class $k$.\n",
    "4. Update:\n",
    "    $$\n",
    "    F_{i, k} \\leftarrow F_{i, k} + \\eta \\cdot \\text{tree}_k(X_i)\n",
    "    $$\n",
    "    where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d07a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    X: NDArray[np.float64],\n",
    "    y: NDArray[np.int64],\n",
    "    n_classes: int,\n",
    "    n_estimators: int,\n",
    "    learning_rate: float,\n",
    "    max_depth: int,\n",
    "    min_samples_split: int,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fit a gradient boosting model for multi-class classification.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        y: Target labels of shape (n_samples,).\n",
    "        n_classes: Number of classes.\n",
    "        n_estimators: Number of boosting iterations.\n",
    "        learning_rate: Learning rate for updates.\n",
    "        max_depth: Maximum depth of each tree.\n",
    "        min_samples_split: Minimum samples required to split a node.\n",
    "\n",
    "    Returns:\n",
    "        List of lists of trees for each boosting iteration and class.\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    F = np.zeros((N, n_classes), dtype=np.float64)\n",
    "    y_onehot = np.eye(n_classes)[y]\n",
    "    trees = []\n",
    "    for _ in tqdm(range(n_estimators)):\n",
    "        trees_m = []\n",
    "        P = np.exp(F - F.max(axis=1, keepdims=True))\n",
    "        P /= P.sum(axis=1, keepdims=True)\n",
    "        for k in range(n_classes):\n",
    "            residual = y_onehot[:, k] - P[:, k]\n",
    "            tree = build_tree(X, residual, max_depth, min_samples_split)\n",
    "            update = np.array([predict_tree(tree, row) for row in X])\n",
    "            F[:, k] += learning_rate * update\n",
    "            trees_m.append(tree)\n",
    "        trees.append(trees_m)\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6fd317",
   "metadata": {},
   "source": [
    "## 8. Final Predictions\n",
    "Final predictions using the ensemble of boosted trees.\n",
    "\n",
    "- For each tree in the sequence and each class, accumulate the predictions (logits).\n",
    "- Compute softmax probabilities from the logits:\n",
    "    $$\n",
    "        P_{i, k} = \\dfrac{e(F_{i, k})}{\\sum^{K}_{l=1}e(F_{i, k})}\n",
    "    $$\n",
    "- The predicted class is the one with the highest probability:\n",
    "    $$\n",
    "        \\hat y_i = \\arg \\max_k P_{i, k}\n",
    "    $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c005a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    X: NDArray[np.float64], trees: Dict[str, Any], n_classes: int, learning_rate: float\n",
    ") -> Tuple[List[int], List[float]]:\n",
    "    \"\"\"\n",
    "    Predict class labels and probabilities using the fitted gradient boosting model.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        trees: List of lists of trees from the fit function.\n",
    "        n_classes: Number of classes.\n",
    "        learning_rate: Learning rate used during training.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (predicted_labels, predicted_probabilities)\n",
    "            predicted_labels: Array of predicted class labels.\n",
    "            predicted_probabilities: Array of predicted class probabilities.\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    F = np.zeros((N, n_classes), dtype=np.float64)\n",
    "    for trees_m in trees:\n",
    "        for k, tree in enumerate(trees_m):\n",
    "            update = np.array([predict_tree(tree, row) for row in X])\n",
    "            F[:, k] += learning_rate * update\n",
    "    P = np.exp(F - F.max(axis=1, keepdims=True))\n",
    "    P /= P.sum(axis=1, keepdims=True)\n",
    "    return np.argmax(P, axis=1), P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "522d1ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:21<00:00, 44.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8022\n"
     ]
    }
   ],
   "source": [
    "features = [\"size\", \"material\", \"color\", \"sleeves\"]\n",
    "target = \"demand\"\n",
    "\n",
    "# Encode target as integer\n",
    "target_map = {v: i for i, v in enumerate(df[target].unique())}\n",
    "df[\"target_enc\"] = df[target].map(target_map)\n",
    "\n",
    "# Apply ordered target encoding\n",
    "df_enc = apply_ordered_target_encoding(df, features, \"target_enc\")\n",
    "\n",
    "# Prepare data for boosting\n",
    "X = df_enc[features].values.astype(np.float64)\n",
    "y = df[\"target_enc\"].values.astype(np.int64)\n",
    "n_classes = len(target_map)\n",
    "\n",
    "# Fit model\n",
    "trees = fit(\n",
    "    X,\n",
    "    y,\n",
    "    n_classes,\n",
    "    n_estimators=10,  # Increase for higher accuracy\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    min_samples_split=10,  # Increase to speed up on very large data\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred, y_proba = predict(X, trees, n_classes, learning_rate=0.1)\n",
    "\n",
    "# print(\"Predicted classes:\", y_pred)\n",
    "print(f\"Accuracy: {np.mean(y == y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0b661",
   "metadata": {},
   "source": [
    "## 9. Evaluation Metrics\n",
    "### Binary Confusion Matrix\n",
    "In a confusion matrix, the terms True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) describe the classification performance for binary classification. \n",
    "\n",
    "|                     | Predicted Negative  | Predicted Positive  |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| **Actual Negative** | True Negative (TN)  | False Positive (FP) |\n",
    "| **Actual Positive** | False Negative (FN) | True Positive (TP)  |\n",
    "\n",
    "\n",
    "1. True Positive (TP): The number of instances correctly predicted as positive (e.g., a disease correctly identified).\n",
    "\n",
    "2. True Negative (TN): The number of instances correctly predicted as negative (e.g., no disease correctly identified).\n",
    "\n",
    "3. False Positive (FP): The number of instances incorrectly predicted as positive (e.g., predicting disease when there isn't any).\n",
    "\n",
    "4. False Negative (FN): The number of instances incorrectly predicted as negative (e.g., missing a disease when it exists).\n",
    "\n",
    "### Multi-Class Confusion Matrix\n",
    "For multi-class classification, the concepts can be extended by treating one class as the \"positive\" class and all others as \"negative\" classes in a one-vs-all approach. Rows represent the actual classes (true labels), and columns represent the predicted classes. For a class $C$,\n",
    "1. True Positive (TP): The count in the diagonal cell corresponding to class $C$ ($\\text{matrix} [C][C]$).\n",
    "2. False Positive (FP): The sum of the column for class $C$, excluding the diagonal ($\\sum(\\text{matrix} [:, C]) - \\text{matrix} [C][C]$).\n",
    "3. False Negative (FN): The sum of the row for class $C$, excluding the diagonal ($\\sum(\\text{matrix} [C, :]) - \\text{matrix} [C][C]$).\n",
    "4. True Negative (TN): All other cells not in the row or column for class $C$ ($\\text{total} - (FP + FN + TP)$).\n",
    "\n",
    "|                  | Predicted Class 0 | Predicted Class 1 | Predicted Class 2 |\n",
    "| ---------------- | ----------------- | ----------------- | ----------------- |\n",
    "| **True Class 0** | 5                 | 2                 | 0                 |\n",
    "| **True Class 1** | 1                 | 6                 | 1                 |\n",
    "| **True Class 2** | 0                 | 2                 | 7                 |\n",
    "\n",
    "\n",
    "For Class 0:\n",
    "- TP = 5 (diagonal element for Class 0)\n",
    "- FP = 1 (sum of column 0 minus TP: 1 + 0)\n",
    "- FN = 2 (sum of row 0 minus TP: 2 + 0)\n",
    "- TN = 6 + 1 + 2 + 7 = 16 (all other cells not in row 0 or column 0)\n",
    "\n",
    "For Class 1:\n",
    "- TP = 6 (diagonal element for Class 1)\n",
    "- FP = 4 (sum of column 1 minus TP: 2 + 2)\n",
    "- FN = 2 (sum of row 1 minus TP: 1 + 1)\n",
    "- TN = 5 + 0 + 0 + 7 = 12 (all other cells not in row 1 or column 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a28800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(\n",
    "    y_true: NDArray[np.int64], y_pred: NDArray[np.int64], class_names: List[str] = None\n",
    ") -> Tuple[NDArray[np.int64], List[str]]:\n",
    "    \"\"\"\n",
    "    Calculate the confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        - Confusion matrix.\n",
    "        - List of class names.\n",
    "    \"\"\"\n",
    "    # Encode labels as integers\n",
    "    unique_classes = np.unique(y_true)\n",
    "    if class_names is None:\n",
    "        class_names = [str(cls) for cls in unique_classes]\n",
    "    class_to_index = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "\n",
    "    n_classes = len(unique_classes)\n",
    "    matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        true_idx = class_to_index[true]\n",
    "        pred_idx = class_to_index[pred]\n",
    "        matrix[true_idx][pred_idx] += 1\n",
    "\n",
    "    return matrix, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ad073",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Accuracy is the most common evaluation metric for classification problems, representing the percentage of correct predictions out of total predictions. It provides a simple measure of how often the classifier makes correct predictions across all classes.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy} = \\dfrac{\\text{True Positives (TP)} + \\text{True Negatives (TN)}}{\\text{Total Samples}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a79282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true: NDArray[np.int64], y_pred: NDArray[np.int64]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of predictions by comparing true and predicted labels.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth target values. Contains the actual class labels for each sample.\n",
    "        y_pred: Estimated target as returned by a classifier. Contains the predicted class labels for each sample.\n",
    "    Returns:\n",
    "        Classification accuracy as a percentage (0.0 to 100.0).\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0653c1",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions made by the classifier.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Precision} = \\dfrac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1569cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(\n",
    "    y_true: NDArray[np.int64], y_pred: NDArray[np.int64]\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate precision for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Precision values for each class.\n",
    "    \"\"\"\n",
    "    cm, _ = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / (np.sum(cm, axis=0) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e410f74",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall measures the proportion of true positive predications out of all actual positive cases.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Recall} = \\dfrac{\\text{True Positives (TP)} }{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6741653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true: NDArray[np.int64], y_pred: NDArray[np.int64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate recall for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Recall values for each class.\n",
    "    \"\"\"\n",
    "    cm, _ = confusion_matrix(y_true, y_pred)\n",
    "    return np.diag(cm) / (np.sum(cm, axis=1) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfdd0a5",
   "metadata": {},
   "source": [
    "### F1-Score\n",
    "The F1-Score is the harmonic mean of precision and recall.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{F1-Score} = 2 \\times \\dfrac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "685cb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(\n",
    "    y_true: NDArray[np.int64], y_pred: NDArray[np.int64]\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Calculate F1-score for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        F1-scores for each class.\n",
    "    \"\"\"\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b2a5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    y_true: NDArray[np.int64], y_pred: NDArray[np.int64], class_names: List[str] = None\n",
    ") -> Tuple[float, float, float, float, NDArray[np.int64]]:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics including accuracy, precision, recall, and F1-score for each class.\n",
    "\n",
    "    Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        - Overall accuracy.\n",
    "        - Average precision.\n",
    "        - Average recall.\n",
    "        - Average F1-score.\n",
    "        - Confusion matrix.\n",
    "    \"\"\"\n",
    "    cm, class_names = confusion_matrix(y_true, y_pred, class_names)\n",
    "    acc = accuracy(y_true, y_pred)\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    # print(\"Class\\tPrecision\\tRecall\\tF1-Score\")\n",
    "    # for i, class_name in enumerate(class_names):\n",
    "    #     print(f\"{class_name}\\t{prec[i]:.4f}\\t\\t{rec[i]:.4f}\\t{f1[i]:.4f}\")\n",
    "    return acc, np.mean(prec), np.mean(rec), np.mean(f1), cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3cc1a4",
   "metadata": {},
   "source": [
    "## 10. Encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "341d8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCatBoost:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 100,\n",
    "        learning_rate: float = 0.1,\n",
    "        max_depth: int = 3,\n",
    "        min_samples_split: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the CustomCatBoost model with hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            n_estimators: Number of boosting iterations.\n",
    "            learning_rate: Learning rate for updates.\n",
    "            max_depth: Maximum depth of each tree.\n",
    "            min_samples_split: Minimum samples required to split a node.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def ordered_target_encode(\n",
    "        self, df: pd.DataFrame, col: str, target: str\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Vectorised ordered target encoding for a single categorical column.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame containing the data.\n",
    "            col: Name of the categorical column to encode.\n",
    "            target: Name of the target column.\n",
    "\n",
    "        Returns:\n",
    "            Encoded column with ordered target encoding.\n",
    "        \"\"\"\n",
    "        global_mean = df[target].mean()\n",
    "        cumsum = df.groupby(col)[target].cumsum() - df[target]\n",
    "        cumcnt = df.groupby(col).cumcount()\n",
    "        enc = cumsum / cumcnt.replace(0, np.nan)\n",
    "        enc.fillna(global_mean, inplace=True)\n",
    "        return enc\n",
    "\n",
    "    def apply_ordered_target_encoding(\n",
    "        self, df: pd.DataFrame, cat_cols: List[str], target: str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply ordered target encoding to all categorical columns in a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df: Input DataFrame containing the data.\n",
    "            cat_cols: List of categorical column names to encode.\n",
    "            target: Name of the target column.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with encoded categorical columns.\n",
    "        \"\"\"\n",
    "        df_enc = df.copy()\n",
    "        for col in cat_cols:\n",
    "            df_enc[col] = self.ordered_target_encode(df_enc, col, target)\n",
    "        return df_enc\n",
    "\n",
    "    def find_best_split(\n",
    "        self, X: NDArray[np.float64], y: NDArray[np.float64]\n",
    "    ) -> Tuple[Optional[int], Optional[float], NDArray[np.bool_], NDArray[np.bool_]]:\n",
    "        \"\"\"\n",
    "        Find the best feature and split value to minimise the weighted variance of the target.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "            y: Target values of shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "                best_feature: Index of the best feature to split on.\n",
    "                best_split: Value of the best split.\n",
    "                best_left: Boolean mask for samples going to the left child.\n",
    "                best_right: Boolean mask for samples going to the right child.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        best_feature, best_split, best_score, best_left, best_right = (\n",
    "            None,\n",
    "            None,\n",
    "            np.inf,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "        for feature_idx in range(n_features):\n",
    "            values = np.unique(X[:, feature_idx])\n",
    "            if len(values) == 1:\n",
    "                continue\n",
    "            sorted_vals = np.sort(values)\n",
    "            splits = (sorted_vals[:-1] + sorted_vals[1:]) / 2\n",
    "            for split_val in splits:\n",
    "                left_idx = X[:, feature_idx] <= split_val\n",
    "                right_idx = ~left_idx\n",
    "                if not left_idx.any() or not right_idx.any():\n",
    "                    continue\n",
    "                score = (\n",
    "                    np.var(y[left_idx]) * left_idx.sum()\n",
    "                    + np.var(y[right_idx]) * right_idx.sum()\n",
    "                )\n",
    "                if score < best_score:\n",
    "                    best_feature = feature_idx\n",
    "                    best_split = split_val\n",
    "                    best_score = score\n",
    "                    best_left = left_idx\n",
    "                    best_right = right_idx\n",
    "        return best_feature, best_split, best_left, best_right\n",
    "\n",
    "    def build_tree(\n",
    "        self, X: NDArray[np.float64], y: NDArray[np.float64], depth: int = 0\n",
    "    ) -> Dict[str, Any] | float:\n",
    "        \"\"\"\n",
    "        Recursively build a decision tree for regression.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "            y: Target values of shape (n_samples,).\n",
    "            depth: Current depth of the tree. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            A decision tree node represented as a dictionary or a leaf value (float).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        if (\n",
    "            depth >= self.max_depth\n",
    "            or n_samples < self.min_samples_split\n",
    "            or np.all(y == y[0])\n",
    "        ):\n",
    "            return np.mean(y)\n",
    "        best_feature, best_split, best_left, best_right = self.find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.mean(y)\n",
    "        return {\n",
    "            \"feature\": best_feature,\n",
    "            \"split\": best_split,\n",
    "            \"left\": self.build_tree(X[best_left], y[best_left], depth + 1),\n",
    "            \"right\": self.build_tree(X[best_right], y[best_right], depth + 1),\n",
    "        }\n",
    "\n",
    "    def predict_tree(\n",
    "        self, node: Dict[str, Any] | float, row: NDArray[np.float64]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Predict the target value for a single sample using the decision tree.\n",
    "\n",
    "        Args:\n",
    "            node: Decision tree node or leaf value.\n",
    "            row: Feature values of the sample.\n",
    "\n",
    "        Returns:\n",
    "            Predicted target value.\n",
    "        \"\"\"\n",
    "        while isinstance(node, dict):\n",
    "            if row[node[\"feature\"]] <= node[\"split\"]:\n",
    "                node = node[\"left\"]\n",
    "            else:\n",
    "                node = node[\"right\"]\n",
    "        return node\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: NDArray[np.float64],\n",
    "        y: NDArray[np.int64],\n",
    "        n_classes: int,\n",
    "        cat_cols: Optional[List[str]] = None,\n",
    "        df: Optional[pd.DataFrame] = None,\n",
    "        target_col: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Fit the gradient boosting model for multi-class classification.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "            y: Target labels of shape (n_samples,).\n",
    "            n_classes: Number of classes.\n",
    "            cat_cols: List of categorical columns for ordered target encoding. Defaults to None.\n",
    "            df: DataFrame containing original data for encoding. Defaults to None.\n",
    "            target_col: Target column name in DataFrame. Defaults to None.\n",
    "        \"\"\"\n",
    "        if cat_cols is not None and df is not None and target_col is not None:\n",
    "            df_enc = self.apply_ordered_target_encoding(df, cat_cols, target_col)\n",
    "            for col in cat_cols:\n",
    "                if col in df_enc.columns:\n",
    "                    X[:, df.columns.get_loc(col)] = df_enc[col].values\n",
    "\n",
    "        N = X.shape[0]\n",
    "        F = np.zeros((N, n_classes), dtype=np.float64)\n",
    "        y_onehot = np.eye(n_classes)[y]\n",
    "        self.trees = []\n",
    "        for _ in tqdm(range(self.n_estimators)):\n",
    "            trees_m = []\n",
    "            P = np.exp(F - F.max(axis=1, keepdims=True))\n",
    "            P /= P.sum(axis=1, keepdims=True)\n",
    "            for k in range(n_classes):\n",
    "                residual = y_onehot[:, k] - P[:, k]\n",
    "                tree = self.build_tree(X, residual)\n",
    "                update = np.array([self.predict_tree(tree, row) for row in X])\n",
    "                F[:, k] += self.learning_rate * update\n",
    "                trees_m.append(tree)\n",
    "            self.trees.append(trees_m)\n",
    "\n",
    "    def predict(\n",
    "        self, X: NDArray[np.float64], n_classes: int\n",
    "    ) -> Tuple[NDArray[np.int64], NDArray[np.float64]]:\n",
    "        \"\"\"\n",
    "        Predict class labels and probabilities using the fitted gradient boosting model.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "            n_classes: Number of classes.\n",
    "\n",
    "        Returns:\n",
    "                predicted_labels: Array of predicted class labels.\n",
    "                predicted_probabilities: Array of predicted class probabilities.\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        F = np.zeros((N, n_classes), dtype=np.float64)\n",
    "        for trees_m in self.trees:\n",
    "            for k, tree in enumerate(trees_m):\n",
    "                update = np.array([self.predict_tree(tree, row) for row in X])\n",
    "                F[:, k] += self.learning_rate * update\n",
    "        P = np.exp(F - F.max(axis=1, keepdims=True))\n",
    "        P /= P.sum(axis=1, keepdims=True)\n",
    "        return np.argmax(P, axis=1), P\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X: NDArray[np.float64],\n",
    "        y: NDArray[np.int64],\n",
    "        n_classes: int,\n",
    "        cat_cols: Optional[List[str]] = None,\n",
    "        df: Optional[pd.DataFrame] = None,\n",
    "        target_col: Optional[str] = None,\n",
    "    ) -> Tuple[NDArray[np.int64], NDArray[np.float64]]:\n",
    "        \"\"\"\n",
    "        Fit the model and predict class labels and probabilities.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "            y: Target labels of shape (n_samples,).\n",
    "            n_classes: Number of classes.\n",
    "            cat_cols: List of categorical columns for ordered target encoding. Defaults to None.\n",
    "            df: DataFrame containing original data for encoding. Defaults to None.\n",
    "            target_col: Target column name in DataFrame. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "                predicted_labels: Array of predicted class labels.\n",
    "                predicted_probabilities: Array of predicted class probabilities.\n",
    "        \"\"\"\n",
    "        self.fit(X, y, n_classes, cat_cols, df, target_col)\n",
    "        return self.predict(X, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c88bbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [10:24<04:20, 43.47s/it]"
     ]
    }
   ],
   "source": [
    "cat_features = [\"size\", \"material\", \"color\", \"sleeves\"]\n",
    "target = \"demand\"\n",
    "\n",
    "# Encode target as integer\n",
    "target_map = {v: i for i, v in enumerate(df[target].unique())}\n",
    "df[\"target_enc\"] = df[target].map(target_map)\n",
    "\n",
    "# Instantiate model\n",
    "model_custom = CustomCatBoost(\n",
    "    n_estimators=20, learning_rate=0.2, max_depth=4, min_samples_split=5\n",
    ")\n",
    "\n",
    "# Apply ordered target encoding\n",
    "df_enc = model_custom.apply_ordered_target_encoding(df, cat_features, \"target_enc\")\n",
    "\n",
    "# Prepare data\n",
    "X = df_enc[cat_features].values.astype(np.float64)\n",
    "y = df[\"target_enc\"].values.astype(np.int64)\n",
    "n_classes = len(target_map)\n",
    "\n",
    "# Fit model\n",
    "model_custom.fit(X, y, n_classes, cat_cols=None, df=None, target_col=None)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred, y_proba = model_custom.predict(X, n_classes)\n",
    "acc_custom, prec_custom, rec_custom, f1_custom, cm_custom = evaluate(y, y_pred)\n",
    "print(f\"Accuracy: (Custom) {acc_custom:.4f}\")\n",
    "print(f\"Precision: (Custom) {prec_custom:.4f}\")\n",
    "print(f\"Recall (Custom): {rec_custom:.4f}\")\n",
    "print(f\"F1-Score (Custom): {f1_custom:.4f}\")\n",
    "print(f\"Confusion Matrix (Custom):\\n{cm_custom}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16f575",
   "metadata": {},
   "source": [
    "## 11. Comparison with catboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,  # noqa: F811\n",
    "    confusion_matrix,  # noqa: F811\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "\n",
    "df_cb = pd.read_csv(\"../../_datasets/material.csv\", index_col=0)\n",
    "\n",
    "cat_features = [\"size\", \"material\", \"color\", \"sleeves\"]\n",
    "target = \"demand\"\n",
    "\n",
    "# Prepare features and target\n",
    "X_cb = df_cb.drop(\"demand\", axis=1)\n",
    "y_cb = df_cb[\"demand\"]\n",
    "\n",
    "\n",
    "# Create CatBoost Pool (optional, but recommended for categorical data)\n",
    "train_pool = Pool(X_cb, y_cb, cat_features=cat_features)\n",
    "\n",
    "# Initialise and fit the classifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=20,\n",
    "    learning_rate=0.2,\n",
    "    depth=4,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"MultiClass\",\n",
    "    verbose=0,\n",
    "    random_seed=42,\n",
    ")\n",
    "model.fit(train_pool)\n",
    "\n",
    "# Predict class labels and probabilities\n",
    "y_pred = model.predict(X_cb).flatten()\n",
    "preds_proba = model.predict_proba(X_cb)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "acc_cb = accuracy_score(y_cb, y_pred)\n",
    "prec_cb = precision_score(y_cb, y_pred, average=\"weighted\")\n",
    "rec_cb = recall_score(y_cb, y_pred, average=\"weighted\")\n",
    "f1_cb = f1_score(y_cb, y_pred, average=\"weighted\")\n",
    "cm_cb = confusion_matrix(y_cb, y_pred)\n",
    "\n",
    "print(f\"Accuracy: (Custom) {acc_custom:.4f}\")\n",
    "print(f\"Precision: (Custom) {prec_custom:.4f}\")\n",
    "print(f\"Recall (Custom): {rec_custom:.4f}\")\n",
    "print(f\"F1-Score (Custom): {f1_custom:.4f}\")\n",
    "print(f\"Confusion Matrix (Custom):\\n{cm_custom}\")\n",
    "print(\"----------\")\n",
    "print(f\"Accuracy: (CatBoost) {acc_cb:.4f}\")\n",
    "print(f\"Precision: (CatBoost) {prec_cb:.4f}\")\n",
    "print(f\"Recall (CatBoost): {rec_cb:.4f}\")\n",
    "print(f\"F1-Score (CatBoost): {f1_cb:.4f}\")\n",
    "print(f\"Confusion Matrix (CatBoost):\\n{cm_cb}\")\n",
    "print(classification_report(y_cb, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b2458c",
   "metadata": {},
   "source": [
    "## 12. References\n",
    "1. Andreas Mueller. (2020). *Applied ML 2020 - 08 - Gradient Boosting.* <br>\n",
    "https://www.youtube.com/watch?v=yrTW5YTmFjw\n",
    "\n",
    "1. Artem Oppermann. (2023). *What is CatBoost?* <br>\n",
    "https://builtin.com/machine-learning/catboost\n",
    "\n",
    "1. Bex Tuychiev. (2023). *A Guide to The Gradient Boosting Algorithm.* <br>\n",
    "https://www.datacamp.com/tutorial/guide-to-the-gradient-boosting-algorithm\n",
    "\n",
    "1. CatBoost. (n.d.). *Quick start*.<br>\n",
    "https://catboost.ai/docs/en/concepts/python-quickstart\n",
    "\n",
    "1. DataMListic. (2023). *Gradient Boosting with Regression Trees Explained* [YouTube Video]. <br>\n",
    "https://youtu.be/KXOTSkPL2X4\n",
    "\n",
    "1. GeeksforGeeks. (2025). *CatBoost in Machine Learning*. <br>\n",
    "https://www.geeksforgeeks.org/machine-learning/catboost-ml/\n",
    "\n",
    "1. GeeksforGeeks. (2024). *CatBoost's Categorical Encoding: One-Hot vs. Target Encoding*. <br>\n",
    "https://www.geeksforgeeks.org/machine-learning/catboosts-categorical-encoding-one-hot-vs-target-encoding/\n",
    "\n",
    "1. M Iqbal. (2025). *CatBoost Explained: Intuition, Advantages, and Math Behind Classification & Regression 🚀* <br>\n",
    "https://youtu.be/DGwLsx47Quc\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2023). *CatBoost Part 1: Ordered Target Encoding* [YouTube Video]. <br>\n",
    "https://youtu.be/2xudPOBz-vs\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2023). *CatBoost Part 2: Building and Using Trees* [YouTube Video]. <br>\n",
    "https://youtu.be/3Bg2XRFOTzg\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *Gradient Boost Part 1 (of 4): Regression Main Ideas* [YouTube Video]. <br>\n",
    "https://youtu.be/3CC4N4z3GJc\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *Gradient Boost Part 2 (of 4): Regression Details* [YouTube Video]. <br>\n",
    "https://youtu.be/2xudPOBz-vs\n",
    "\n",
    "1. Terence Parr and Jeremy Howard. (n.d.). *How to explain gradient boosting.* <br>\n",
    "https://explained.ai/gradient-boosting/index.html\n",
    "\n",
    "1. Tomonori Masui. (2022). *All You Need to Know about Gradient Boosting Algorithm − Part 1. Regression.* <br>\n",
    "https://medium.com/data-science/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
