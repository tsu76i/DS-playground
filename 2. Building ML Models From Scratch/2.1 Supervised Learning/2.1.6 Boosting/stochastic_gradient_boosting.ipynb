{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264433fc",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Boosting from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9af912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5af0e9",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This notebook is an extension of [Gradient Boosting Machine from Scratch](https://github.com/tsu76i/DS-playground/tree/main/2.%20Building%20ML%20Models%20From%20Scratch/2.1%20Supervised%20Learning/2.1.6%20Boosting/gradient_boosting_machine.ipynb).\n",
    "\n",
    "\n",
    "**Gradient Boosting Machine (GBM)** is an ensemble machine learning model that builds a strong predictive model by sequentially combining multiple weak models (typically decision trees) in a stage-wise manner. The core idea is to iteratively add new models that correct the errors made by the existing ensemble, thereby improving overall predictive accuracy.\n",
    "\n",
    "Suppose we have a dataset ${(x_i, y_i)}^n_{i=1}$ where $x_i$ are the features and $y_i$ are the target values. The goal of gradient boosting is to find a function $F(x)$ that minimises a given differentiable loss function $L(y, F(x))$:\n",
    "\n",
    "\\begin{align*}\n",
    "    F(x) = F_0(x) + \\sum^{M}_{m=1}\\gamma_m h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial model (e.g., the mean of $y$).\n",
    "- $\\gamma_i$: Weight (step size) for the $m$-th weak learner, typically determined by minimising the loss function along the direction of $h_m(x)$.\n",
    "- $M$: Number of boosting iterations (e.g., the number of weak learners).\n",
    "- $h_m$: prediction from the $m$-th weak learner (e.g., a decision tree).\n",
    "\n",
    "\n",
    "**Stochastic Gradient Boosting** enhances the gradient boosting approach by introducing randomness into the training process to improve model generalisation, reduce overfitting and increase predictive accuracy. Stochasticity in SGB is as follows:\n",
    "\n",
    "- **Subsampling the training data**: Instead of training each new tree on the entire dataset, a random subset (usually 40 - 80%) is selected *without replacement* for each iteration.\n",
    "- **Feature subsampling**: At each split in a tree, only a random subset of features is considered, further increasing diversity among the trees.\n",
    "\n",
    "### Steps\n",
    "1. Initialise the model\n",
    "    - Start with a simple model, typically a constant value:\n",
    "        - For regression: the mean of the target variable.\n",
    "        - For binary classification: the log-odds of the positive classes.\n",
    "1. Calculate residuals (Negative Gradients)\n",
    "    - For each iteration, compute the residuals, which are the negative gradients of the loss function with respect to the current predictions.\n",
    "    - This step can be generalised to any differentiable loss function, not just the mean squared error.\n",
    "1. Fit a new weak model to predict the residuals.\n",
    "    - **At each boosting iteration, randomly select a subset of the training data without replacement.**\n",
    "    - Train a weak learner (typically a shallow decision tree) to predict the rediduals, **but only using the selected subsample**.\n",
    "    - **The subsample fraciton (e.g., 40 - 80%) is a key hypermarameter.**\n",
    "    - The weak learner focuses on correcting the errors made by the current ensemble.\n",
    "    - **At each node split, randomly select a subset of features to consider**.\n",
    "1. Update the model\n",
    "    - Add the predictions from the new weak learners to the current model's predictions, scaled by a learning rate (shrinkage parameter).\n",
    "    - This update moves the ensemble closer to the true values by correcting previous errors.\n",
    "1. Repeat steps 2-4 for a pre-defined number of iterations\n",
    "1. Final prediction\n",
    "    - The sum of the initial prediction and the scaled outputs of all weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5abed0",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "Retrieved from [GitHub - YBI Foundation](https://github.com/YBI-Foundation/Dataset/blob/main/Admission%20Chance.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16be77af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Serial No",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GRE Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOEFL Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "University Rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " SOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOR ",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CGPA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chance of Admit ",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "eed2ca36-d9a4-4c94-9e53-35c36d279bdb",
       "rows": [
        [
         "0",
         "1",
         "337",
         "118",
         "4",
         "4.5",
         "4.5",
         "9.65",
         "1",
         "0.92"
        ],
        [
         "1",
         "2",
         "324",
         "107",
         "4",
         "4.0",
         "4.5",
         "8.87",
         "1",
         "0.76"
        ],
        [
         "2",
         "3",
         "316",
         "104",
         "3",
         "3.0",
         "3.5",
         "8.0",
         "1",
         "0.72"
        ],
        [
         "3",
         "4",
         "322",
         "110",
         "3",
         "3.5",
         "2.5",
         "8.67",
         "1",
         "0.8"
        ],
        [
         "4",
         "5",
         "314",
         "103",
         "2",
         "2.0",
         "3.0",
         "8.21",
         "0",
         "0.65"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No  GRE Score  TOEFL Score  University Rating   SOP  LOR   CGPA  \\\n",
       "0          1        337          118                  4   4.5   4.5  9.65   \n",
       "1          2        324          107                  4   4.0   4.5  8.87   \n",
       "2          3        316          104                  3   3.0   3.5  8.00   \n",
       "3          4        322          110                  3   3.5   2.5  8.67   \n",
       "4          5        314          103                  2   2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/YBI-Foundation/Dataset/refs/heads/main/Admission%20Chance.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "049a1481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (400, 8)\n",
      "Target shape: (400,)\n",
      "Features: \n",
      "['Serial No', 'GRE Score', 'TOEFL Score', 'University Rating', ' SOP', 'LOR ', 'CGPA', 'Research']\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "feature_names = df.columns[:-1].tolist()  # All columns except the last one\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "print(f'Features: \\n{feature_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f7953",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics\n",
    "We will use the same evaluation metrics as [Gradient Boosting Machine from Scratch](https://github.com/tsu76i/DS-playground/tree/main/2.%20Building%20ML%20Models%20From%20Scratch/2.1%20Supervised%20Learning/2.1.6%20Boosting/gradient_boosting_machine.ipynb).\n",
    "### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a6cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(y_true: pd.Series, y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c6efdd",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2a44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(y_true: pd.Series, y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54371223",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e00c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAE(y_true: pd.Series, y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db62b30",
   "metadata": {},
   "source": [
    "<a id=\"r-squared\"></a>\n",
    "### R-Squared($R^2$)\n",
    "$\\hat y$: Precicted target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1596040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2(y_true: pd.Series, y_pred: NDArray[np.float64]) -> float:\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aba1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true: pd.Series, y_pred: NDArray[np.float64]) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate and return evaluation metrics for a regression model, including MSE, RMSE, MAE, and R-squared.\n",
    "\n",
    "     Args:\n",
    "        y_true): True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "    Returns:\n",
    "        - mse: Mean Squared Error (MSE), indicating the average of the squared differences between predicted and true values.\n",
    "        - rmse: Root Mean Squared Error (RMSE), indicating the standard deviation of the residuals.\n",
    "        - mae: Mean Absolute Error (MAE), representing the average absolute difference between predicted and true values.\n",
    "        - r2: R-squared (coefficient of determination), showing the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n",
    "    \"\"\"\n",
    "    mse = calculate_MSE(y_true, y_pred)\n",
    "    rmse = calculate_RMSE(y_true, y_pred)\n",
    "    mae = calculate_MAE(y_true, y_pred)\n",
    "    r2 = calculate_r2(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acedfcf",
   "metadata": {},
   "source": [
    "## 3. From Gradient Boosting to Stochastic Gradient Boosting\n",
    "Stochastic Gradient Boosting shares its core functionalities with the classical gradient boosting method except for:\n",
    "1. Random Row subsampling (samples).\n",
    "- At each boosting iteration, instead of using the entire dataset to fit the new tree, randomly select a fraction of the data (e.g., 40 - 80%) *without replacement* and fit the tree only on the subset.\n",
    "    - Add a new `sub_samples` parameter (default = $0.8$).\n",
    "    - In the `fit()` method, before building each tree, randomly select a subset of the data using `np.random.choice`.\n",
    "    - Use only selected subset to compute residuals and fit the tree.\n",
    "2. Random Column subsampling (features).\n",
    "- At each split in the tree, use only a random subset of features rather than all features. This is especially for high-dimensional data and further increases model diversity. \n",
    "    - Add a new `sub_features` parameter (default = $0.8$).\n",
    "    - In the `_best_split()` method, at each node, randomly select a subset of features to consider for splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3f0334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "class CustomSGBRegressor:\n",
    "    \"\"\"\n",
    "    Custom Stochastic Gradient Boosting for regression with decision trees.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 3,\n",
    "        learning_rate: float = 0.1,\n",
    "        max_depth: int = 3,\n",
    "        min_samples_leaf: int = 1,\n",
    "        sub_samples: float = 0.8,   # Added\n",
    "        sub_features: float = 0.8,   # Added\n",
    "        random_state: int = None    # Added\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the CustomSGBRegressor.\n",
    "\n",
    "        Args:\n",
    "            n_estimators: Number of boosting rounds.\n",
    "            learning_rate: Learning rate (shrinkage).\n",
    "            max_depth: Maximum depth of each tree.\n",
    "            min_samples_leaf: Minimum samples per leaf.\n",
    "            sub_samples: Friction of row subsampling. Default = 0.8.\n",
    "            sub_features: Friction of feature subsampling. Default = 0.8.\n",
    "            random_state: Seed for the random number generator to ensure reproducibility.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.models: List[Dict[str, Any] | float] = []\n",
    "        self.initial_prediction: float = 0.0\n",
    "        self.sub_samples = sub_samples      # Added\n",
    "        self.sub_features = sub_features    # Added\n",
    "        self.random_state = random_state    # Added\n",
    "\n",
    "    def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -> None:\n",
    "        \"\"\"\n",
    "        Fit the gradient boosting regressor to the data.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "            y: Target values, shape (n_samples,).\n",
    "        \"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        self.models = []\n",
    "        self.initial_prediction = float(np.mean(y))\n",
    "        predictions = np.full_like(y, self.initial_prediction, dtype=float)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "            # Stochastic row subsampling\n",
    "            n_samples = int(self.sub_samples * X.shape[0])\n",
    "            sample_indices = np.random.choice(\n",
    "                X.shape[0], n_samples, replace=False)\n",
    "            X_sub, residuals_sub = X[sample_indices], residuals[sample_indices]\n",
    "            tree = self._build_tree(\n",
    "                # Build trees with subsamples\n",
    "                X_sub, residuals_sub, self.max_depth, self.min_samples_leaf)\n",
    "            self.models.append(tree)\n",
    "            # Predictions on the entire dataset\n",
    "            update = self._predict_tree_batch(tree, X)\n",
    "            predictions += self.learning_rate * update\n",
    "\n",
    "    def predict(self, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Predict target values for given feature matrix.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted values.\n",
    "        \"\"\"\n",
    "        y_pred = np.full(X.shape[0], self.initial_prediction, dtype=np.float64)\n",
    "        for tree in self.models:\n",
    "            y_pred += self.learning_rate * self._predict_tree_batch(tree, X)\n",
    "        return y_pred\n",
    "\n",
    "    def _variance(self, y: NDArray[np.float64]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the variance of the target values.\n",
    "\n",
    "        Args:\n",
    "            y: Target values.\n",
    "\n",
    "        Returns:\n",
    "            float: Variance of y.\n",
    "        \"\"\"\n",
    "        return np.var(y)\n",
    "\n",
    "    def _split_dataset(self, X: NDArray[np.float64],\n",
    "                       y: NDArray[np.float64], feature_index: int,\n",
    "                       threshold: float) -> Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]:\n",
    "        \"\"\"\n",
    "        Splits the dataset based on a feature and threshold.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "            y: Target values, shape (n_samples,).\n",
    "            feature_index: Index of the feature to split on.\n",
    "            threshold: Threshold value for the split.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing X_left, y_left, X_right, y_right after the split.\n",
    "        \"\"\"\n",
    "        left_mask = X[:, feature_index] < threshold\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "    def _best_split(self, X: NDArray[np.float64],\n",
    "                    y: NDArray[np.float64], min_samples_leaf: int) -> Tuple[int | None, float | None]:\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split the dataset, minimising weighted variance.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "            y: Target values.\n",
    "            min_samples_leaf: Minimum number of samples required at a leaf node.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (best_feature, best_threshold). Returns (None, None) if no valid split is found.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        # Stochastic feature subsampling\n",
    "        n_features = int(self.sub_features * n)\n",
    "        feature_indices = np.random.choice(n, n_features, replace=False)\n",
    "\n",
    "        best_feature, best_threshold, best_var = None, None, float('inf')\n",
    "        for feature in feature_indices:\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                _, y_left, _, y_right = self._split_dataset(\n",
    "                    X, y, feature, threshold)\n",
    "                if len(y_left) < min_samples_leaf or len(y_right) < min_samples_leaf:\n",
    "                    continue\n",
    "                var_left = self._variance(y_left)\n",
    "                var_right = self._variance(y_right)\n",
    "                var_split = (len(y_left) * var_left +\n",
    "                             len(y_right) * var_right) / m\n",
    "                if var_split < best_var:\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    best_var = var_split\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _build_tree(self, X: NDArray[np.float64], y: NDArray[np.float64],\n",
    "                    max_depth: int, min_samples_leaf: int, depth: int = 0) -> Dict[str, Any] | float:\n",
    "        \"\"\"\n",
    "        Recursively build a regression tree.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix.\n",
    "            y: Target values.\n",
    "            max_depth: Maximum depth of the tree.\n",
    "            min_samples_leaf: Minimum samples required at a leaf node.\n",
    "            depth: Current depth of the tree (default is 0).\n",
    "\n",
    "        Returns:\n",
    "            Tree as a nested dictionary, or a float if a leaf node.\n",
    "        \"\"\"\n",
    "        if depth >= max_depth or len(y) <= min_samples_leaf:\n",
    "            return float(np.mean(y))\n",
    "        feature, threshold = self._best_split(X, y, min_samples_leaf)\n",
    "        if feature is None:\n",
    "            return float(np.mean(y))\n",
    "        X_left, y_left, X_right, y_right = self._split_dataset(\n",
    "            X, y, feature, threshold)\n",
    "        return {\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'left': self._build_tree(X_left, y_left, max_depth, min_samples_leaf, depth + 1),\n",
    "            'right': self._build_tree(X_right, y_right, max_depth, min_samples_leaf, depth + 1)\n",
    "        }\n",
    "\n",
    "    def _predict_tree(self, tree: Dict[str, Any] | float, x: NDArray[np.float64]) -> float:\n",
    "        \"\"\"\n",
    "        Predict the target value for a single sample using the regression tree.\n",
    "\n",
    "        Args:\n",
    "            tree: The regression tree or a leaf value.\n",
    "            x: Feature vector for a single sample.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted value.\n",
    "        \"\"\"\n",
    "        while isinstance(tree, dict):\n",
    "            if x[tree['feature']] < tree['threshold']:\n",
    "                tree = tree['left']\n",
    "            else:\n",
    "                tree = tree['right']\n",
    "        return float(tree)\n",
    "\n",
    "    def _predict_tree_batch(self, tree: Dict[str, Any] | float, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Predict target values for a batch of samples using the regression tree.\n",
    "\n",
    "        Args:\n",
    "            tree: The regression tree or a leaf value.\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted values for all samples.\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_tree(tree, x) for x in X], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3d6de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Custom): 0.0037\n",
      "RMSE (Custom): 0.0605\n",
      "MAE (Custom): 0.0408\n",
      "R-Squared (Custom): 0.8582\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model_custom = CustomSGBRegressor(n_estimators=200,\n",
    "                                  learning_rate=0.1,\n",
    "                                  max_depth=2,\n",
    "                                  random_state=42,\n",
    "                                  min_samples_leaf=1,\n",
    "                                  sub_features=0.8,  # Added\n",
    "                                  sub_samples=0.8)  # Added\n",
    "model_custom.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_custom = model_custom.predict(X_test)\n",
    "mse_custom, rmse_custom, mae_custom, r2_custom = evaluate(\n",
    "    y_test, y_pred_custom)\n",
    "print(f'MSE (Custom): {mse_custom:.4f}')\n",
    "print(f'RMSE (Custom): {rmse_custom:.4f}')\n",
    "print(f'MAE (Custom): {mae_custom:.4f}')\n",
    "print(f'R-Squared (Custom): {r2_custom:.4f}')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f25bd",
   "metadata": {},
   "source": [
    "## 4. Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eced203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (SK): 0.0035\n",
      "RMSE (SK): 0.0588\n",
      "MAE (SK): 0.0427\n",
      "R-Squared (SK): 0.8662\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Instantiate and fit the sklearn model\n",
    "sklearn_gbm = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    subsample=0.8\n",
    ")\n",
    "sklearn_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_sklearn = sklearn_gbm.predict(X_test)\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "rmse_sklearn = root_mean_squared_error(y_test, y_pred_sklearn)\n",
    "mae_sklearn = mean_absolute_error(y_test, y_pred_sklearn)\n",
    "r2_sklearn = r2_score(y_test, y_pred_sklearn)\n",
    "print(f'MSE (SK): {mse_sklearn:.4f}')\n",
    "print(f'RMSE (SK): {rmse_sklearn:.4f}')\n",
    "print(f'MAE (SK): {mae_sklearn:.4f}')\n",
    "print(f'R-Squared (SK): {r2_sklearn:.4f}')\n",
    "print('----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
