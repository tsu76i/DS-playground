{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a18c9f1",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3fd6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e97bef",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "**Extreme Gradient Boosting (XGBoost)** is an advanced implementation of the gradient boosting framework, specifically designed for speed, efficiency, and scalability in supervise machine learning tasks such as classification and regression. It is particularly renowed for its performance on structured (tabular) data and has become a standard tool in data science competitions (e.g., Kaggle) and industry applications.\n",
    "\n",
    "XGBoost is based on **Gradient Boosting Machine (GBM)**, which is an ensemble machine learning model that builds a strong predictive model by sequentially combining multiple weak models (typically decision trees) in a stage-wise manner. The core idea is to iteratively add new models that correct the errors made by the existing ensemble, thereby improving overall predictive accuracy.\n",
    "\n",
    "Suppose we have a dataset ${(x_i, y_i)}^n_{i=1}$ where $x_i$ are the features and $y_i$ are the target values. The goal of XGBoost is to find a function $F(x)$ that minimises a given regularised objective function $L$:\n",
    "\n",
    "\\begin{align*}\n",
    "    F(x) = F_0(x) +  \\sum^{M}_{m=1} \\eta \\cdot h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial model (e.g., the mean of $y$).\n",
    "- $\\eta$: Learning rate.\n",
    "- $M$: Number of boosting iterations (e.g., the number of weak learners).\n",
    "- $h_m$: prediction from the $m$-th weak learner (e.g., a decision tree).\n",
    "\n",
    "\n",
    "And the regularised objective function to be minimised:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L} = \\sum^{n}_{i=1}l(y_i, \\hat y_i) + \\sum^{K}_{k=1}\\Omega(f_k)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $l(y_i, \\hat y_i)$: Loss function for the $i$-th instance (**Training Loss**).\n",
    "- $\\Omega(f_k)$: Regularisation term for the $k$-th tree $f_k$ (**Regularisation Term**).\n",
    "- $K$: Number of trees in the ensemble.\n",
    "\n",
    "<!-- Suppose we have a dataset ${(x_i, y_i)}^n_{i=1}$ where $x_i$ are the features and $y_i$ are the target values. The goal of gradient boosting is to find a function $F(x)$ that minimises a given differentiable loss function $L(y, F(x))$:\n",
    "\n",
    "\\begin{align*}\n",
    "    F(x) = F_0(x) + \\sum^{M}_{m=1}\\gamma_m h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial model (e.g., the mean of $y$).\n",
    "- $\\gamma_i$: Weight (step size) for the $m$-th weak learner, typically determined by minimising the loss function along the direction of $h_m(x)$.\n",
    "- $M$: Number of boosting iterations (e.g., the number of weak learners).\n",
    "- $h_m$: prediction from the $m$-th weak learner (e.g., a decision tree). -->\n",
    "\n",
    "### Differences: Gradient Boosting vs. XGBoost\n",
    "- Gradient Boosting fits trees to the negative gradient (residuals) of the loss function, typically using only first-order derivatives and mean values at leaves.\n",
    "\n",
    "- XGBoost uses both first and second derivatives (gradient and Hessian), incorporates regularisation, and computes optimal leaf weights analytically. It also uses a more sophisticated split criterion (gain) and regularisation to control overfitting\n",
    "\n",
    "### Advantages\n",
    "- XGBoost adds L1 (Lasso) and L2 (Ridge) regularisation to the loss function, which helps prevent overfitting and improves overall model performance.\n",
    "- Rather than stopping splits when no further gain is achieved, XGBoost grows trees to a maximum depth and then prunes them backward, removing splits with negative gain (depth-first approach).\n",
    "- Natively handles missing values by learning the optimal direction to take when a value is missing.\n",
    "- Supports parallel tree construction and distributed computation, making it much faster than traditional gradient boosting implementations.\n",
    "\n",
    "### Limitations\n",
    "- Memory usage can be high with large datasets.\n",
    "- Not ideal for text or images. XGBoost is suited for tabular data than unstructured inputs.\n",
    "- Sensitive to hypermarameters (e.g., learning rate, number of trees, regularisation parameter).\n",
    "\n",
    "### Steps\n",
    "1. Initialise the model:\n",
    "    - Start with a simple model, typically a constant value:\n",
    "        - For regression: the mean of the target variable.\n",
    "        - For binary classification: the log-odds of the positive classes.\n",
    "1. Calculate residuals (Negative Gradients) and Hessian:\n",
    "    - For each iteration, compute the the negative gradients (and Hessian for second-order methods) of the loss function  with respect to the current predictions.\n",
    "1. Fit a new weak model to predict the residuals:\n",
    "    - Train a weak learner (typically a shallow decision tree) to predict the rediduals.\n",
    "    - The weak learner focuses on correcting the errors made by the current ensemble.\n",
    "1. Update the model:\n",
    "    - Add the predictions from the new weak learners to the current model's predictions, scaled by a learning rate (shrinkage parameter).\n",
    "    - This update moves the ensemble closer to the true values by correcting previous errors.\n",
    "1. Repeat steps 2-4 for a pre-defined number of iterations.\n",
    "1. Final prediction:\n",
    "    - The sum of the initial prediction and the scaled outputs of all weak learners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24129c5",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "Retrieved from [GitHub - YBI Foundation](https://github.com/YBI-Foundation/Dataset/blob/main/Admission%20Chance.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0650a71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Serial No",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GRE Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOEFL Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "University Rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " SOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOR ",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CGPA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chance of Admit ",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "71e73753-e4ff-4c04-8b33-ad9ea127b8f6",
       "rows": [
        [
         "0",
         "1",
         "337",
         "118",
         "4",
         "4.5",
         "4.5",
         "9.65",
         "1",
         "0.92"
        ],
        [
         "1",
         "2",
         "324",
         "107",
         "4",
         "4.0",
         "4.5",
         "8.87",
         "1",
         "0.76"
        ],
        [
         "2",
         "3",
         "316",
         "104",
         "3",
         "3.0",
         "3.5",
         "8.0",
         "1",
         "0.72"
        ],
        [
         "3",
         "4",
         "322",
         "110",
         "3",
         "3.5",
         "2.5",
         "8.67",
         "1",
         "0.8"
        ],
        [
         "4",
         "5",
         "314",
         "103",
         "2",
         "2.0",
         "3.0",
         "8.21",
         "0",
         "0.65"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No  GRE Score  TOEFL Score  University Rating   SOP  LOR   CGPA  \\\n",
       "0          1        337          118                  4   4.5   4.5  9.65   \n",
       "1          2        324          107                  4   4.0   4.5  8.87   \n",
       "2          3        316          104                  3   3.0   3.5  8.00   \n",
       "3          4        322          110                  3   3.5   2.5  8.67   \n",
       "4          5        314          103                  2   2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/YBI-Foundation/Dataset/refs/heads/main/Admission%20Chance.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb6017af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (400, 8)\n",
      "Target shape: (400,)\n",
      "Features: \n",
      "['Serial No', 'GRE Score', 'TOEFL Score', 'University Rating', ' SOP', 'LOR ', 'CGPA', 'Research']\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "feature_names = df.columns[:-1].tolist()  # All columns except the last one\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "print(f'Features: \\n{feature_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391c0ba",
   "metadata": {},
   "source": [
    "## 3. Loss Function\n",
    "In XGBoost, the loss function is used to calculate the training loss, which is then combined with regularisation terms to form the overall objective function that is minimised during training. The gradients used to fit new trees are computed with respect to the training loss, but the final optimisation considers both the loss and regularisation. \n",
    "\n",
    "### Regression\n",
    "The most common loss function for regression is the mean squared error (MSE):\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y, \\hat y) = \\dfrac{1}{n}\\sum^{n}_{i=1}(y_i - \\hat y_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True value.\n",
    "- $\\hat y_i$: Predicted value.\n",
    "- $n$: Number of samples.\n",
    "\n",
    "### Binary Classification\n",
    "The standard loss function for binary classification in XGBoost is the binary cross-entropy (log loss):\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y, \\hat p) = - \\dfrac{1}{n} \\sum^{n}_{i=1}\\left[y_i \\log{(\\hat p_i) + (1-y_i) \\log{(1- \\hat p_i)}} \\right]\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True label ($0$ or $1$).\n",
    "- $\\hat p_i$: Predicted probability for class 1 (after applying the sigmoid function to the raw score).\n",
    "- $n$: Number of samples.\n",
    "\n",
    "### Multi-class Classification\n",
    "For multi-class classification (with $K$ classes), XGBoost uses the multi-class cross-entropy (also called softmax loss):\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y, \\hat P) = - \\dfrac{1}{n} \\sum^{n}_{i=1} \\sum^{K}_{k=1} \\mathbb{I}(y_i = k) \\log{(\\hat p_{ik})}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True class label for sample $i$.\n",
    "- $\\hat p_i$: Predicted probability that sample $i$ belongs to class $k$ (output of the softmax function).\n",
    "- $\\mathbb{I}(y_i = k)$: Indicator function, equal to $1$ if $y_i = k$ and $0$ otherwise.\n",
    "- $n$: Number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc6057",
   "metadata": {},
   "source": [
    "## 4. Initialising Model\n",
    "First, we need to initialise the model with a constant function that minimises the loss (initial predictions).\n",
    "\n",
    "\\begin{align*}\n",
    "    F_0(x) = \\arg \\min_{\\gamma}\\sum^{n}_{i=1}L(y_i, \\gamma)\n",
    "\\end{align*}\n",
    "\n",
    "For squared error, the best constant is the mean of the target values. Thus,\n",
    "\n",
    "\\begin{align*}\n",
    "    F_0(x) = \\bar y = \\dfrac{1}{n}\\sum^{n}_{i=1}y_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "02e4db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(y: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Initialise predictions with the mean of the target values.\n",
    "\n",
    "    Parameters:\n",
    "        y: Target values, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Array of initial predictions, each set to mean of y.\n",
    "    \"\"\"\n",
    "    return np.full_like(y, np.mean(y), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b1df07",
   "metadata": {},
   "source": [
    "## 5. Gradient and Hessian Computation\n",
    "We will need the first derivative (gradient) and the second derivative (Hessian) of the loss function for each sample $i$. They will be used later in the algorithm.\n",
    "\n",
    "- $g_i = \\dfrac{\\partial l(y_i, \\hat y_i)}{\\partial \\hat y_i} = \\dfrac{\\partial}{\\partial \\hat y_i} \\dfrac{1}{2}(y_i-\\hat y_i)^2 = \\hat y_i - y_i$\n",
    "- $h_i = \\dfrac{\\partial^2 l(y_i, \\hat y_i)}{\\partial \\hat y_i ^2} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a87151ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_and_hessians(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> Tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Compute gradients and Hessians for squared error loss.\n",
    "\n",
    "    Args:\n",
    "        y_true: True target values, shape (n_samples,).\n",
    "        y_pred: Predicted values, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        A tuple with gradients and Hessians, both of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    gradients = y_pred - y_true\n",
    "    hessians = np.ones_like(y_true)\n",
    "    return gradients, hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b8789",
   "metadata": {},
   "source": [
    "## 5. Finding the Best Split\n",
    "In XGBoost, the split is chosen to maximise the gain. The gain formula is derived using a second-order Taylor expansion of the loss function.\n",
    "\n",
    "### Regularised Objective Function\n",
    "For a tree at boosting iteration $t$, the regularised objective function is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}^{(t)} = \\sum^{n}_{i=1}l(y_i, \\hat y_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $l$: Loss function (e.g., mean squared error).\n",
    "- $\\hat y_i^{(t-1)}$: Prediction from previous trees.\n",
    "- $f_t$: New tree.\n",
    "- $\\Omega(f_t)$: Regularisation term.\n",
    "\n",
    "### Second-Order Taylor Expansion\n",
    "Expand the loss function around the current prediction $\\hat y_i^{(t-1)}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y_i, \\hat y_i^{(t-1)} + f_t(x_i)) \\approx l(y_i, \\hat y_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $g_i = \\dfrac{\\partial l(y_i, \\hat y_i)}{\\partial \\hat y_i}$\n",
    "- $h_i = \\dfrac{\\partial^2 l(y_i, \\hat y_i)}{\\partial \\hat y_i ^2}$\n",
    "\n",
    "Assume the new tree $f_t$ assigns a constant score $w_j$ to all samples in leaft $j$:\n",
    "\\begin{align*}\n",
    "    f_t(x_i) = w_{q(x_i)}\n",
    "\\end{align*}\n",
    "\n",
    "where $q(x_i)$ maps sample $i$ to its leaf.\n",
    "\n",
    "### Regularisation Term\n",
    "The regularisation term for both L1 and L2 is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Omega(f_t) = \\gamma T + \\frac{1}{2} \\lambda \\sum^{T}_{j=1}w_j^2 + \\alpha \\sum^{T}_{j=1}|w_j|\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $T$: Number of leaves.\n",
    "- $\\gamma$: Penalty for the number of leaves (tree complexity).\n",
    "- $\\lambda$ L2 regularisation parameter.\n",
    "- $\\alpha$: L1 regularisation parameter.\n",
    "\n",
    "However, for simplicity, only L2 regularisation will be considered in this notebook. Thus:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Omega(f_t) = \\gamma T + \\frac{1}{2} \\lambda \\sum^{T}_{j=1}w_j^2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### Total Objective Function\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{\\tilde L}^{(t)} = \\sum^{T}_{j=1} \\left[\n",
    "        G_j w_j + \\frac{1}{2}(H_j+\\lambda)w_j^2\n",
    "        \\right] + \\gamma T\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $G_j = \\sum_{i \\in I_j} g_i$: Sum of gradients in leaf $j$.\n",
    "- $H_j = \\sum_{i \\in I_j} h_i$: Sum of Hessians in leaf $j$.\n",
    "\n",
    "### Optimising Leaf Weights\n",
    "\n",
    "To optimise leaf weight, minimise $\\mathcal{\\tilde L^{(t)}}$ with respect to $w_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial \\mathcal{\\tilde L^{(t)}}}{\\partial w_j} = G_j + (H_j + \\lambda)w_j = 0 \\rightarrow w_j^* = - \\dfrac{G_j}{H_j + \\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "Plug $w_j^*$ back into the objective:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{\\tilde L}^{(t)} = - \\dfrac{1}{2} \\sum^{T}_{j=1} \n",
    "        \\dfrac{G_j^2}{H_j + \\lambda}\n",
    "         + \\gamma T\n",
    "\\end{align*}\n",
    "\n",
    "Suppose a node is split into left ($L$) and right ($R$) children, the gain is the reduction in the objective:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{Gain} = \\dfrac{1}{2} \\left( \n",
    "        \\dfrac{G^2_L}{H_L + \\lambda} + \\dfrac{G^2_R}{H_R + \\lambda} - \\dfrac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\n",
    "        \\right) - \\gamma\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $G_L, H_L$: Sums for the left child.\n",
    "- $G_R, H_R$: Sums for the right child.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45e2fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X: NDArray[np.float64], gradients: NDArray[np.float64], hessians: NDArray[np.float64],\n",
    "               min_samples_leaf: int, lambda_: float, gamma: float) -> Tuple[int, float, float]:\n",
    "    \"\"\"\n",
    "    Find the best split for a node in the XGBoost tree.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        gradients: Gradients for each sample, shape (n_samples,).\n",
    "        hessians: Hessians for each sample, shape (n_samples,).\n",
    "        min_samples_leaf: Minimum number of samples required in a leaf.\n",
    "        lambda_ : L2 regularisation parameter.\n",
    "        gamma: Minimum loss reduction required to make a split.\n",
    "\n",
    "    Returns:\n",
    "        A tuple with best feature index, best threshold, and best gain.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    best_feature, best_threshold, best_gain = None, None, -np.inf\n",
    "    for feature in range(n):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            left_mask = X[:, feature] < threshold\n",
    "            right_mask = ~left_mask\n",
    "            if left_mask.sum() < min_samples_leaf or right_mask.sum() < min_samples_leaf:\n",
    "                continue\n",
    "            G_L, H_L = gradients[left_mask].sum(), hessians[left_mask].sum()\n",
    "            G_R, H_R = gradients[right_mask].sum(), hessians[right_mask].sum()\n",
    "            gain = 0.5 * (G_L**2 / (H_L + lambda_) + G_R**2 / (H_R + lambda_) -\n",
    "                          (G_L + G_R)**2 / (H_L + H_R + lambda_)) - gamma\n",
    "            if gain > best_gain:\n",
    "                best_feature, best_threshold, best_gain = feature, threshold, gain\n",
    "    return best_feature, best_threshold, best_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c98af",
   "metadata": {},
   "source": [
    "## 6. Building Trees\n",
    "This function recursively constructs a regression tree by splitting the data at each node using the best split found, until stopping criteria are met. \n",
    "\n",
    "The formula for the optimal leaf value is:\n",
    "\n",
    "\\begin{align*}\n",
    "    w^* = - \\dfrac{\\sum g_i}{\\sum h_i + \\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "- Stopping Criteria: \n",
    "    1. The tree stops growing if `max_depth` is reached.\n",
    "    2. The number of samples is less than or equal to `min_sample_leaf`.\n",
    "- If no valid split is found or gain is non-positive, a leaf is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9687dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X: NDArray[np.float64], gradients: NDArray[np.float64], hessians: NDArray[np.float64],\n",
    "               max_depth: int, min_samples_leaf: int, lambda_: float,\n",
    "               gamma: float, depth: int = 0) -> Dict[str, Any] | float:\n",
    "    \"\"\"\n",
    "    Recursively build a decision tree for XGBoost.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        gradients: Gradients for each sample, shape (n_samples,).\n",
    "        hessians: Hessians for each sample, shape (n_samples,).\n",
    "        max_depth: Maximum depth of the tree.\n",
    "        min_samples_leaf: Minimum number of samples required in a leaf.\n",
    "        lambda_ : L2 regularisation parameter.\n",
    "        gamma: Minimum loss reduction required to make a split.\n",
    "        depth: Current depth of the tree. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tree structure as nested dictionaries, or a float for leaf value.\n",
    "    \"\"\"\n",
    "\n",
    "    if depth >= max_depth or len(gradients) <= min_samples_leaf:\n",
    "        return -gradients.sum() / (hessians.sum() + lambda_)\n",
    "    feature, threshold, gain = best_split(\n",
    "        X, gradients, hessians, min_samples_leaf, lambda_, gamma)\n",
    "    if feature is None or gain <= 0:\n",
    "        return -gradients.sum() / (hessians.sum() + lambda_)\n",
    "    left_mask = X[:, feature] < threshold\n",
    "    right_mask = ~left_mask\n",
    "    return {\n",
    "        'feature': feature,\n",
    "        'threshold': threshold,\n",
    "        'left': build_tree(X[left_mask], gradients[left_mask], hessians[left_mask],\n",
    "                           max_depth, min_samples_leaf, lambda_, gamma, depth + 1),\n",
    "        'right': build_tree(X[right_mask], gradients[right_mask], hessians[right_mask],\n",
    "                            max_depth, min_samples_leaf, lambda_, gamma, depth + 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbfd8dc",
   "metadata": {},
   "source": [
    "## 7. Predictions (Trees)\n",
    "The two functions `predict_tree()` and `predict_tree_batch()` are used to make predictions with a regression decision tree represented as a nested dictionary.\n",
    "\n",
    "- `predict_tree()`: Predicts the output for a single data sample $x$ by traversing the decision tree.\n",
    "- `predict_tree_batch()`: Generates predictions for a batch of samples by applying the `predict_tree` function to each sample in the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "649c38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree(tree: Dict[str, Any] | float, x: NDArray[np.float64]) -> float:\n",
    "    \"\"\"\n",
    "    Predict the output for a single sample using a decision tree.\n",
    "\n",
    "    Args:\n",
    "        tree: Tree structure or leaf value.\n",
    "        x: Feature vector, shape (n_features,).\n",
    "\n",
    "    Returns:\n",
    "        Predicted value for the sample.\n",
    "    \"\"\"\n",
    "    while isinstance(tree, dict):\n",
    "        if x[tree['feature']] < tree['threshold']:\n",
    "            tree = tree['left']\n",
    "        else:\n",
    "            tree = tree['right']\n",
    "    return tree\n",
    "\n",
    "\n",
    "def predict_tree_batch(tree: Dict[str, Any] | float, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Predict outputs for a batch of samples using a decision tree.\n",
    "\n",
    "    Args:\n",
    "        tree: Tree structure or leaf value.\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        Predicted values, shape (n_samples,).\n",
    "    \"\"\"\n",
    "    return np.array([predict_tree(tree, x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41895775",
   "metadata": {},
   "source": [
    "## 8. Training Model\n",
    "During the training process of gradient boosting:\n",
    "1. Initialise model predictions as $ F_0(x) = \\bar y $.\n",
    "1. Boosting loop from $ m=1 $ to $ M $ (number of boosting rounds `n_estimators`):\n",
    "    - Compute gradients and hessians.\n",
    "    - Fit a tree $ h_m^{(x)} $ to the gradients and hessians, optimising the regularised objective function:\n",
    "\n",
    "      $$\n",
    "      h_m^{(x)} = \\text{Tree}(X, \\{g_i^{(m)}, h_i^{(m)}\\})\n",
    "      $$\n",
    "\n",
    "      The tree structure and leaf values are chosen to maximise the gain:\n",
    "\n",
    "      $$\n",
    "      \\text{Gain} = \\frac{1}{2} \\left( \n",
    "          \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\n",
    "          \\right) - \\gamma\n",
    "      $$\n",
    "\n",
    "      where $G_L$, $H_L$ and $G_R$, $H_R$ are the sums of gradients and hessians for the left and right splits respectively.\n",
    "\n",
    "    - Update predictions.\n",
    "\n",
    "      The model is updated additively:\n",
    "\n",
    "      $$\n",
    "      F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\n",
    "      $$\n",
    "\n",
    "      where $\\eta$ is the learning rate.\n",
    "1. The function returns the initial prediction (mean of $y$), the list of fitted trees, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d6e41d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X: NDArray[np.float64], y: NDArray[np.float64], n_estimators: int = 10, learning_rate: float = 0.1,\n",
    "        max_depth: int = 3, min_samples_leaf: int = 1, lambda_: float = 1.0, gamma: float = 0.0\n",
    "        ) -> Tuple[float, List[Dict[str, Any] | float], float]:\n",
    "    \"\"\"\n",
    "    Fit an XGBoost-like model for regression.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        y: Target values, shape (n_samples,).\n",
    "        n_estimators: Number of boosting rounds. Defaults to 10.\n",
    "        learning_rate: Learning rate. Defaults to 0.1.\n",
    "        max_depth: Maximum depth of each tree. Defaults to 3.\n",
    "        min_samples_leaf: Minimum samples per leaf. Defaults to 1.\n",
    "        lambda_: L2 regularisation parameter. Defaults to 1.0.\n",
    "        gamma: Minimum loss reduction for a split. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, List[Dict[str, Any] | float], float]: Initial prediction (mean of y), list of fitted trees, and learning rate.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    y_pred = initialise_model(y)\n",
    "    for m in range(n_estimators):\n",
    "        gradients, hessians = compute_gradients_and_hessians(y, y_pred)\n",
    "        tree = build_tree(\n",
    "            X, gradients, hessians, max_depth, min_samples_leaf, lambda_, gamma)\n",
    "        update = predict_tree_batch(tree, X)\n",
    "        y_pred += learning_rate * update\n",
    "        models.append(tree)\n",
    "    return np.mean(y), models, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f23847",
   "metadata": {},
   "source": [
    "## 9. Final Predictions\n",
    "The final predictions after $M$ trees is:\n",
    "\\begin{align*}\n",
    "    F_M(x) = F_0(x) + \\sum^{M}_{m=1} \\eta \\cdot h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial prediction (mean of $y$).\n",
    "- $h_m(x)$: Prediction of the $m$-th tree.\n",
    "- $\\eta$: Learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee09ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: NDArray[np.float64], initial_prediction: float, models: List[Dict[str, Any] | float],\n",
    "            learning_rate: float) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Predict outputs for a batch of samples using the fitted model.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        initial_prediction: Initial prediction (mean of y from training).\n",
    "        models: List of fitted trees.\n",
    "        learning_rate: Learning rate.\n",
    "\n",
    "    Returns:\n",
    "        Predicted values, shape (n_samples,).\n",
    "    \"\"\"\n",
    "    y_pred = np.full(X.shape[0], initial_prediction, dtype=float)\n",
    "    for tree in models:\n",
    "        y_pred += learning_rate * predict_tree_batch(tree, X)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede9694",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metrics\n",
    "### Mean Squared Error (MSE)\n",
    "Mean Squared Error measures the average squared difference between predicted ($\\hat y$) and actual ($y$) values. Large errors are penalised heavily. Smaller MSE indicates better predictions.\n",
    "\n",
    "\\begin{align*}\n",
    "MSE = \\dfrac{1}{n} \\sum_{i=1}^{n}(\\hat y_{i} = y_{i})^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d78e0cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5379b",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "Square root of MSE. It provides error in the same unit as the target variable ($y$) and easier to interpret.\n",
    "\n",
    "\\begin{align*}\n",
    "RMSE = \\sqrt{(MSE)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "36600241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071edce7",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "Mean Absolute Error measures the average absolute difference between predicted ($\\hat y$) and actual ($y$) values. It is less sensitive to outliers than MSE. Smaller MAE indicates better predictions.\n",
    "\n",
    "\\begin{align*}\n",
    "MAE = \\dfrac{1}{n} \\sum_{i=1}^{n}|\\hat y_{i} = y_{i}|\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "adf4e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fad800",
   "metadata": {},
   "source": [
    "<a id=\"r-squared\"></a>\n",
    "### R-Squared($R^2$)\n",
    "\n",
    "R-squared indicated the proportion of variance in the dependent variable that is predictable from the independent variables. Value ranges from 0 to 1. Closer to 1 indicates a better fit.\n",
    "\n",
    "\n",
    "\n",
    "Residual Sum of Squares ($SS_{residual}$): \n",
    "\\begin{align*}\n",
    "SS_{residual} = \\sum_{i=1}^{n} (y_{i} - \\hat y_{i})^{2}\n",
    "\\end{align*}\n",
    "\n",
    "Total Sum of Squares ($SS_{total}$): \n",
    "\\begin{align*}\n",
    "SS_{total} = \\sum_{i=1}^{n} (y_{i} - \\bar y_{i})^{2}\n",
    "\\end{align*}\n",
    "\n",
    "$R^2$ is computed as:\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "R^2 = 1 - \\dfrac{SS_{residual}}{SS_{total}} = 1 - \\dfrac{\\sum_{i=1}^{n} (y_{i} - \\hat y_{i})^{2}}{\\sum_{i=1}^{n} (y_{i} - \\bar y_{i})^{2}}\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "$y$: Actual target values.\n",
    "\n",
    "$\\bar y$: Mean of the actual target values.\n",
    "\n",
    "$\\hat y$: Precicted target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7fa4c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a2c7f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate and return evaluation metrics for a regression model, including MSE, RMSE, MAE, and R-squared.\n",
    "\n",
    "     Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "    Returns:\n",
    "        - mse: Mean Squared Error (MSE), indicating the average of the squared differences between predicted and true values.\n",
    "        - rmse: Root Mean Squared Error (RMSE), indicating the standard deviation of the residuals.\n",
    "        - mae: Mean Absolute Error (MAE), representing the average absolute difference between predicted and true values.\n",
    "        - r2: R-squared (coefficient of determination), showing the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n",
    "    \"\"\"\n",
    "    mse = calculate_MSE(y_true, y_pred)\n",
    "    rmse = calculate_RMSE(y_true, y_pred)\n",
    "    mae = calculate_MAE(y_true, y_pred)\n",
    "    r2 = calculate_r2(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502aaeb6",
   "metadata": {},
   "source": [
    "## 11. Encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "29d3723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomXGBoost:\n",
    "    \"\"\"\n",
    "    Custom Extreme Gradient Boosting for regression with decision trees.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators: int = 10, learning_rate: float = 0.1,\n",
    "                 max_depth: int = 3, min_samples_leaf: int = 1, lambda_: float = 1.0,\n",
    "                 gamma: float = 0.0) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the CustomXGBoost regressor with specified hyperparameters.\n",
    "\n",
    "        Parameters:\n",
    "            n_estimators: Number of boosting rounds (trees).\n",
    "            learning_rate: Shrinkage factor for each tree's contribution.\n",
    "            max_depth: Maximum depth of each regression tree.\n",
    "            min_samples_leaf: Minimum number of samples required in a leaf node.\n",
    "            lambda_: L2 regularisation parameter for leaf weights.\n",
    "            gamma: Minimum loss reduction required to make a split.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.initial_prediction = None\n",
    "        self.models: List[Dict[str, Any] | float] = []\n",
    "\n",
    "    def _compute_gradients_and_hessians(self, y_true: NDArray[np.float64],\n",
    "                                        y_pred: NDArray[np.float64]) -> Tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "        \"\"\"\n",
    "        Compute gradients and Hessians for squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y_true: True target values, shape (n_samples,).\n",
    "            y_pred: Predicted values, shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            A tuple with gradients and Hessians, both of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        gradients = y_pred - y_true\n",
    "        hessians = np.ones_like(y_true)\n",
    "        return gradients, hessians\n",
    "\n",
    "    def _best_split(self, X: NDArray[np.float64], gradients: NDArray[np.float64],\n",
    "                    hessians: NDArray[np.float64]) -> Tuple[int, float, float]:\n",
    "        \"\"\"\n",
    "        Find the best split for a node in the XGBoost tree.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "            gradients: Gradients for each sample, shape (n_samples,).\n",
    "            hessians: Hessians for each sample, shape (n_samples,).\n",
    "        Returns:\n",
    "            A tuple with best feature index, best threshold, and best gain.\n",
    "        \"\"\"\n",
    "        best_feature, best_threshold, best_gain = None, None, -np.inf\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] < threshold\n",
    "                right_mask = ~left_mask\n",
    "                if left_mask.sum() < self.min_samples_leaf or right_mask.sum() < self.min_samples_leaf:\n",
    "                    continue\n",
    "                G_L, H_L = gradients[left_mask].sum(\n",
    "                ), hessians[left_mask].sum()\n",
    "                G_R, H_R = gradients[right_mask].sum(\n",
    "                ), hessians[right_mask].sum()\n",
    "                gain = 0.5 * (\n",
    "                    G_L**2 / (H_L + self.lambda_) +\n",
    "                    G_R**2 / (H_R + self.lambda_) -\n",
    "                    (G_L + G_R)**2 / (H_L + H_R + self.lambda_)\n",
    "                ) - self.gamma\n",
    "                if gain > best_gain:\n",
    "                    best_feature, best_threshold, best_gain = feature, threshold, gain\n",
    "        return best_feature, best_threshold, best_gain\n",
    "\n",
    "    def _build_tree(self, X: NDArray[np.float64], gradients: NDArray[np.float64],\n",
    "                    hessians: NDArray[np.float64], depth: int = 0) -> Dict[str, Any] | float:\n",
    "        \"\"\"\n",
    "        Recursively build a decision tree for XGBoost.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "            gradients: Gradients for each sample, shape (n_samples,).\n",
    "            hessians: Hessians for each sample, shape (n_samples,).\n",
    "            depth: Current depth of the tree. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            Tree structure as nested dictionaries, or a float for leaf value.\n",
    "        \"\"\"\n",
    "        if depth >= self.max_depth or len(gradients) <= self.min_samples_leaf:\n",
    "            return -gradients.sum() / (hessians.sum() + self.lambda_)\n",
    "        feature, threshold, gain = self._best_split(X, gradients, hessians)\n",
    "        if feature is None or gain <= 0:\n",
    "            return -gradients.sum() / (hessians.sum() + self.lambda_)\n",
    "        left_mask = X[:, feature] < threshold\n",
    "        right_mask = ~left_mask\n",
    "        return {\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'left': self._build_tree(\n",
    "                X[left_mask], gradients[left_mask], hessians[left_mask], depth + 1\n",
    "            ),\n",
    "            'right': self._build_tree(\n",
    "                X[right_mask], gradients[right_mask], hessians[right_mask], depth + 1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def _predict_tree(self, tree: Dict[str, Any] | float, x: NDArray[np.float64]) -> float:\n",
    "        \"\"\"\n",
    "        Predict the output for a single sample using a decision tree.\n",
    "\n",
    "        Args:\n",
    "            tree: Tree structure or leaf value.\n",
    "            x: Feature vector, shape (n_features,).\n",
    "\n",
    "        Returns:\n",
    "            Predicted value for the sample.\n",
    "        \"\"\"\n",
    "        while isinstance(tree, dict):\n",
    "            if x[tree['feature']] < tree['threshold']:\n",
    "                tree = tree['left']\n",
    "            else:\n",
    "                tree = tree['right']\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def _predict_tree_batch(self, tree: Dict[str, Any] | float, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Predict outputs for a batch of samples using a decision tree.\n",
    "\n",
    "        Args:\n",
    "            tree: Tree structure or leaf value.\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted values, shape (n_samples,).\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_tree(tree, x) for x in X])\n",
    "\n",
    "    def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -> None:\n",
    "        \"\"\"\n",
    "        Fit an XGBoost-like model for regression.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "            y: Target values, shape (n_samples,).\n",
    "        \"\"\"\n",
    "        self.models = []\n",
    "        y_pred = np.full_like(y, np.mean(y), dtype=float)\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        for _ in range(self.n_estimators):\n",
    "            gradients, hessians = self._compute_gradients_and_hessians(\n",
    "                y, y_pred)\n",
    "            tree = self._build_tree(X, gradients, hessians)\n",
    "            update = self._predict_tree_batch(tree, X)\n",
    "            y_pred += self.learning_rate * update\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Predict outputs for a batch of samples using the fitted model.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted values, shape (n_samples,).\n",
    "        \"\"\"\n",
    "        y_pred = np.full(X.shape[0], self.initial_prediction, dtype=float)\n",
    "        for tree in self.models:\n",
    "            y_pred += self.learning_rate * self._predict_tree_batch(tree, X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d5e3c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Custom): 0.0038\n",
      "RMSE (Custom): 0.0618\n",
      "MAE (Custom): 0.0436\n",
      "R-Squared (Custom): 0.8522\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model_custom = CustomXGBoost(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=2, lambda_=0.01)\n",
    "model_custom.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_custom = model_custom.predict(X_test)\n",
    "mse_custom, rmse_custom, mae_custom, r2_custom = evaluate(\n",
    "    y_test, y_pred_custom)\n",
    "print(f'MSE (Custom): {mse_custom:.4f}')\n",
    "print(f'RMSE (Custom): {rmse_custom:.4f}')\n",
    "print(f'MAE (Custom): {mae_custom:.4f}')\n",
    "print(f'R-Squared (Custom): {r2_custom:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be9db8",
   "metadata": {},
   "source": [
    "## 12. Comparison with xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53d8f486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Custom): 0.0038\n",
      "MSE (xgboost): 0.0039\n",
      "RMSE (Custom): 0.0618\n",
      "RMSE (xgboost): 0.0623\n",
      "MAE (Custom): 0.0436\n",
      "MAE (xgboost): 0.0417\n",
      "R-Squared (Custom): 0.8522\n",
      "R-Squared (xgboost): 0.8495\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Instantiate and fit the xgboost model\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=100,        # Number of boosting rounds (trees)\n",
    "    learning_rate=0.1,       # Step size shrinkage\n",
    "    max_depth=5,             # Maximum tree depth\n",
    "    subsample=0.8,           # Row sampling\n",
    "    colsample_bytree=0.8,    # Feature sampling\n",
    "    objective='reg:squarederror',  # Loss function for regression\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_xgboost = xgb.predict(X_test)\n",
    "mse_xgboost = mean_squared_error(y_test, y_pred_xgboost)\n",
    "rmse_xgboost = root_mean_squared_error(y_test, y_pred_xgboost)\n",
    "mae_xgboost = mean_absolute_error(y_test, y_pred_xgboost)\n",
    "r2_xgboost = r2_score(y_test, y_pred_xgboost)\n",
    "print(f'MSE (Custom): {mse_custom:.4f}')\n",
    "print(f'MSE (xgboost): {mse_xgboost:.4f}')\n",
    "print(f'RMSE (Custom): {rmse_custom:.4f}')\n",
    "print(f'RMSE (xgboost): {rmse_xgboost:.4f}')\n",
    "print(f'MAE (Custom): {mae_custom:.4f}')\n",
    "print(f'MAE (xgboost): {mae_xgboost:.4f}')\n",
    "print(f'R-Squared (Custom): {r2_custom:.4f}')\n",
    "print(f'R-Squared (xgboost): {r2_xgboost:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d64226",
   "metadata": {},
   "source": [
    "## 13. References\n",
    "\n",
    "--- \n",
    "\n",
    "1. Andreas Mueller. (2020). *Applied ML 2020 - 08 - Gradient Boosting.* <br>\n",
    "https://www.youtube.com/watch?v=yrTW5YTmFjw\n",
    "\n",
    "1. Bex Tuychiev. (2023). *A Guide to The Gradient Boosting Algorithm.* <br>\n",
    "https://www.datacamp.com/tutorial/guide-to-the-gradient-boosting-algorithm\n",
    "\n",
    "1. DataMListic. (2023). *Gradient Boosting with Regression Trees Explained* [YouTube Video]. <br>\n",
    "https://youtu.be/lOwsMpdjxog\n",
    "\n",
    "1. DMLC XGBOOST. (2022). *Introduction to Boosted Trees*. <br>\n",
    "https://xgboost.readthedocs.io/en/stable/tutorials/model.html\n",
    "\n",
    "1. GeeksforGeeks. (2025). *XGBoost*. <br>\n",
    "https://www.geeksforgeeks.org/machine-learning/xgboost/\n",
    "\n",
    "1. IBM. (2024). *What is XGBoost?* <br>\n",
    "https://www.ibm.com/think/topics/xgboost\n",
    "\n",
    "Jason Brownlee. (2021). *How to Develop Your First XGBoost Model in Python*.<br>\n",
    "https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "\n",
    "1. Nilimesh Halder. (2023). *Unpacking XGBoost: A Comprehensive Guide to Enhanced Gradient Boosting in Machine Learning*. <br>\n",
    "https://blog.gopenai.com/unpacking-xgboost-a-comprehensive-guide-to-enhanced-gradient-boosting-in-machine-learning-c145acec09fc\n",
    "\n",
    "1. nVIDIA. (n.d.). *What is XGBoost?* <br>\n",
    "https://www.nvidia.com/en-us/glossary/xgboost/\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *Gradient Boost Part 1 (of 4): Regression Main Ideas* [YouTube Video]. <br>\n",
    "https://youtu.be/3CC4N4z3GJc\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *Gradient Boost Part 2 (of 4): Regression Details* [YouTube Video]. <br>\n",
    "https://youtu.be/2xudPOBz-vs\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *XGBoost Part 1 (of 4): Regression* [YouTube Video]. <br>\n",
    "https://youtu.be/OtD8wVaFm6E\n",
    "\n",
    "1. Terence Parr and Jeremy Howard. (n.d.). *How to explain gradient boosting.* <br>\n",
    "https://explained.ai/gradient-boosting/index.html\n",
    "\n",
    "1. The IoT Academy. (2024). *What is the XGBoost Algorithm in ML  Explained With Steps*. <br>\n",
    "https://www.theiotacademy.co/blog/xgboost-algorithm/\n",
    "\n",
    "1. Tomonori Masui. (2022). *All You Need to Know about Gradient Boosting Algorithm  Part 1. Regression.* <br>\n",
    "https://medium.com/data-science/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502\n",
    "\n",
    "1. Wiens, M., Verone-Boyle, A., Henscheid, N., Podichetty, J. T., & Burton, J. (2025). A Tutorial and Use Case Example of the eXtreme Gradient Boosting (XGBoost) Artificial Intelligence Algorithm for Drug Development Applications. Clinical and translational science, 18(3), e70172. <br>\n",
    "https://doi.org/10.1111/cts.70172"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79527559",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
