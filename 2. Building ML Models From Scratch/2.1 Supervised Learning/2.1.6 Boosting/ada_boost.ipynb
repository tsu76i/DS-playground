{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66f2663",
   "metadata": {},
   "source": [
    "# Adaptive Boosting from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.typing import NDArray\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29553c26",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Adaptive Boosting (AdaBoost) is a foundational ensemble learning algorithm designed to improve the accuracy of machine learning models by combining multiple **weak classifiers** (often decision stumps - decision trees with a single split) into a single **strong classifier**. Althought AdaBoost is primarily for binary classification, it has been extended to handle multiclass problems and regression tasks in some variants. However, its core mechanism and main use case remain in binary classification.\n",
    "\n",
    "### Advantages:\n",
    "- Turn weak models into a strong classifier.\n",
    "- Less overfitting.\n",
    "- No need for parameter tuning.\n",
    "\n",
    "### Limitations:\n",
    "- Sensitive to outliers as misclassified samples get higher weights.\n",
    "- Primarily for binary classification.\n",
    "\n",
    "### Steps:\n",
    "1. Initialise weights.\n",
    "2. For each boosting round (M iterations),\n",
    "    - Train a weak lerner (decision stump).\n",
    "    - Compute weighted error.\n",
    "    - Calculate lerner weights $\\alpha$.\n",
    "    - Update sample weights.\n",
    "    - Repeat for the maximum number of iterations or until weighted error is sufficiently low.\n",
    "3. Predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1022b",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74e4048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "y = np.where(y == 0, -1, 1)     # AdaBoost expects labels as -1 and +1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394850d",
   "metadata": {},
   "source": [
    "## 3. Initialising Weights\n",
    "All training samples are initialised with equal weight:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_i = \\dfrac{1}{N}\n",
    "\\end{align*}\n",
    "\n",
    "where $N$ is the number of samples. For $N = 5$, the initial weights of the sample will be:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_i = \\dfrac{1}{5} = 0.2\n",
    "\\end{align*}\n",
    "\n",
    "The `np.full` function from NumPy library can generate an array of the specified length with every entry set to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa0075d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_weights(n_samples: int) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Initialise all sample weights equally.\n",
    "    \"\"\"\n",
    "    return np.full(n_samples, 1 / n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3676a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For N = 5: [0.2 0.2 0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(f'For N = 5: {initialise_weights(5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f604e6",
   "metadata": {},
   "source": [
    "## 4. Finding the Best Stump\n",
    "<!-- The following `find_best_stump` function searches all features and possible thresholds, and for each, tries both polarities (direction of the inequality). It predicts labels, computes the weighted error, and keeps the stump with the lowest error. -->\n",
    "The following `find_best_stump` function implements the decision stump: It exhaustively searches for the best one-level split across all features and possible thresholds consdering both directions (polarities), and selects the split that minimises the weighted classification error.\n",
    "\n",
    "1. Initialise variables.\n",
    "2. Loop over all features and thresholds (unique values).\n",
    "3. Loop over both polarities: $[1, -1]$.\n",
    "4. Make predictions.\n",
    "    - Initialise all predictions to $+1$.\n",
    "    - For polarity $1$: set to $-1$ if $\\text{value} < \\text{threshold}$.\n",
    "    - Otherwise: set to $+1$.\n",
    "5. Calculate weighted error.\n",
    "\\begin{align*}\n",
    "    \\epsilon_m = \\dfrac{\\sum^{N}_{i=1} w_i \\cdot \\mathbb{I}(h_m(x_i) \\neq y_i)}{\\sum^{N}_{i=1}w_i}\n",
    "\\end{align*}\n",
    "\n",
    "    where:\n",
    "    - $h_m$: $m$-th weak learner.\n",
    "    - $y_i$: True label.\n",
    "    - $\\mathbb{I}$: Indicator function.\n",
    "\n",
    "    In fact, weighted error is just a sum of weights for misclassified samples.\n",
    "6. If the error rate is smaller than `min_error`, update the value (`min_error = error`), best stump and best prediction.\n",
    "7. Return `best_stump`, `min_error`, and `best_predictions` with the least error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8b9fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_stump(X, y, sample_weights):\n",
    "    \"\"\"\n",
    "    Find the decision stump (feature, threshold, polarity) that minimises weighted error.\n",
    "    Returns: feature_index, threshold, polarity, min_error, predictions\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    min_error = float('inf')\n",
    "    best_stump = {}\n",
    "    best_predictions = None\n",
    "\n",
    "    for feature_i in range(n_features):  # Each feature\n",
    "        feature_vals = X[:, feature_i]  # All values in the selected features\n",
    "        thresholds = np.unique(feature_vals)  # Unique values in feature_vals\n",
    "        for threshold in thresholds:\n",
    "            for polarity in [1, -1]:\n",
    "                # Predict: 1 if (polarity * feature) < (polarity * threshold), else -1\n",
    "                predictions = np.ones(n_samples)\n",
    "                if polarity == 1:\n",
    "                    predictions[feature_vals < threshold] = -1\n",
    "                else:\n",
    "                    predictions[feature_vals > threshold] = -1\n",
    "\n",
    "                # Calculate weighted error\n",
    "                misclassified = predictions != y\n",
    "                error = np.sum(sample_weights[misclassified])\n",
    "\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    best_stump = {\n",
    "                        \"feature_index\": feature_i,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"polarity\": polarity\n",
    "                    }\n",
    "                    best_predictions = predictions.copy()\n",
    "    return best_stump, min_error, best_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28b618",
   "metadata": {},
   "source": [
    "## 5. Learner Weights\n",
    "For the current learner $m$, the learner weight $\\alpha_m$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\alpha_m = \\dfrac{1}{2} \\text{ln} \\left( \\dfrac{1-\\epsilon_m + \\text{c}}{\\epsilon_m + \\text{c}} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\epsilon_m$: Error rate calculated inside the `find_best_stump()` function.\n",
    "- $c$: Small constant added to avoid division by zero. Set to $1 \\times 10^{-10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80c5922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(error):\n",
    "    \"\"\"\n",
    "    Compute the weight of the weak learner (alpha).\n",
    "    \"\"\"\n",
    "    c = 1e-10  # constant\n",
    "    return 0.5 * np.log((1 - error + c) / (error + c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081afa49",
   "metadata": {},
   "source": [
    "## 6. Updating Sample Weights\n",
    "After calculating the learner weight $\\alpha_m$, we update the old weight $w_m$ such that:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_i \\leftarrow w_i \\cdot \\text{e}^{-\\alpha_m y_i h_m(x_i)}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "$w_i$: Current weight of sample $i$.\n",
    "$\\alpha_m$: Weight of the weak learner $m$. \n",
    "$h_m(x_i)$: Prediction for sample $i$ ($-1$ or $+1$).\n",
    "\n",
    "The weights are increased for misclassified samples, and are decreased for correctly classified ones:\n",
    "- If the prediction is **correct** $(y_i = h_m(x_i))$, then $y_i \\cdot h_m(x_i) = 1$, so the weight is **decreased**:\n",
    "\\begin{align*}\n",
    "    w_i \\leftarrow w_i \\cdot \\text{e}^{-\\alpha}\n",
    "\\end{align*}\n",
    "\n",
    "- If the prediction is **incorrect** $(y_i \\neq h_m(x_i))$, then $y_i \\cdot h_m(x_i) = -1$, so the weight is **increased**:\n",
    "\\begin{align*}\n",
    "    w_i \\leftarrow w_i \\cdot \\text{e}^{\\alpha}\n",
    "\\end{align*}\n",
    "\n",
    "The function returns the normalised weights (all sample weights sum to 1) for the next AdaBoost iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bb4ca197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(sample_weights, alpha, y, predictions):\n",
    "    \"\"\"\n",
    "    Update sample weights: increase for misclassified, decrease for correct.\n",
    "    \"\"\"\n",
    "    sample_weights *= np.exp(-alpha * y * predictions)\n",
    "    sample_weights /= np.sum(sample_weights)  # Normalisation\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1785e13",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "The training loop runs for the specified number of weak learners `n_weak_learners`. After all iterations, it returns a list of all trained stumps `stumps` with their parameters, and a list of the corresponding weights for each stump `alphas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d5e87126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_train(X, y, n_weak_learners):\n",
    "    \"\"\"\n",
    "    Train AdaBoost with given number of estimators.\n",
    "    Returns: list of stumps, list of alphas\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    sample_weights = initialise_weights(n_samples)\n",
    "    stumps = []\n",
    "    alphas = []\n",
    "\n",
    "    for _ in range(n_weak_learners):\n",
    "        stump, error, predictions = find_best_stump(X, y, sample_weights)\n",
    "        alpha = compute_alpha(error)\n",
    "        sample_weights = update_weights(sample_weights, alpha, y, predictions)\n",
    "        stumps.append(stump)\n",
    "        alphas.append(alpha)\n",
    "    return stumps, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af173c1e",
   "metadata": {},
   "source": [
    "## 8. Prediction\n",
    "The following function makes predictions on input data $X$ using a single decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "34c07b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stump_predict(X, stump):\n",
    "    \"\"\"\n",
    "    Predict labels for X using a given decision stump.\n",
    "    \"\"\"\n",
    "    feature_values = X[:, stump[\"feature_index\"]]\n",
    "    predictions = np.ones(X.shape[0])\n",
    "    if stump[\"polarity\"] == 1:\n",
    "        predictions[feature_values < stump[\"threshold\"]] = -1\n",
    "    else:\n",
    "        predictions[feature_values > stump[\"threshold\"]] = -1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99726e68",
   "metadata": {},
   "source": [
    "Then the `predict()` function combines the predictions from all decision stumps in the AdaBoost ensemble using their respective weights $\\alpha$ to produce the final prediction for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3671e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, stumps, alphas):\n",
    "    \"\"\"\n",
    "    Aggregate predictions from all stumps using their alphas.\n",
    "    \"\"\"\n",
    "    final_pred = np.zeros(X.shape[0])\n",
    "    for stump, alpha in zip(stumps, alphas):\n",
    "        pred = stump_predict(X, stump)\n",
    "        final_pred += alpha * pred\n",
    "    return np.sign(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9153c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Training): 0.97\n"
     ]
    }
   ],
   "source": [
    "# Train AdaBoost\n",
    "n_weak_learners = 10\n",
    "stumps, alphas = adaboost_train(X, y, n_weak_learners)\n",
    "\n",
    "# Predict\n",
    "y_pred = predict(X, stumps, alphas)\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(f\"Accuracy (Training): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c609c",
   "metadata": {},
   "source": [
    "## 9. Encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ffd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DecisionStump:\n",
    "    \"\"\"\n",
    "    A simple decision stump (one-level decision tree) used as a weak learner.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        feature_column = X[:, self.feature_index]\n",
    "        predictions = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            predictions[feature_column < self.threshold] = -1\n",
    "        else:\n",
    "            predictions[feature_column > self.threshold] = -1\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class CustomAdaBoost:\n",
    "    \"\"\"\n",
    "    AdaBoost ensemble classifier using decision stumps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_weak_learners=5):\n",
    "        self.n_weak_learners = n_weak_learners\n",
    "        self.classifiers = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialise weights to 1/N\n",
    "        sample_weights = np.full(n_samples, 1 / n_samples)\n",
    "        self.classifiers = []\n",
    "\n",
    "        for _ in range(self.n_weak_learners):\n",
    "            stump = DecisionStump()\n",
    "            min_error = float('inf')\n",
    "\n",
    "            # Find the best decision stump\n",
    "            for feature_index in range(n_features):\n",
    "                feature_column = X[:, feature_index]\n",
    "                thresholds = np.unique(feature_column)\n",
    "                for threshold in thresholds:\n",
    "                    polarity = 1\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[feature_column < threshold] = -1\n",
    "\n",
    "                    # Calculate weighted error\n",
    "                    error = np.sum(sample_weights[y != predictions])\n",
    "\n",
    "                    # If error > 0.5, flip polarity\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        polarity = -1\n",
    "\n",
    "                    if error < min_error:\n",
    "                        stump.polarity = polarity\n",
    "                        stump.threshold = threshold\n",
    "                        stump.feature_index = feature_index\n",
    "                        min_error = error\n",
    "\n",
    "            # Compute alpha (learner weight)\n",
    "            c = 1e-10  # to avoid division by zero\n",
    "            stump.alpha = 0.5 * np.log((1.0 - min_error + c) / (min_error + c))\n",
    "\n",
    "            # Update weights\n",
    "            predictions = stump.predict(X)\n",
    "            sample_weights *= np.exp(-stump.alpha * y * predictions)\n",
    "            sample_weights /= np.sum(sample_weights)  # Normalise\n",
    "\n",
    "            self.classifiers.append(stump)\n",
    "\n",
    "    def predict(self, X):\n",
    "        weighted_preds = [clf.alpha *\n",
    "                          clf.predict(X) for clf in self.classifiers]\n",
    "        y_pred = np.sum(weighted_preds, axis=0)\n",
    "        return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Custom): 0.9912\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train AdaBoost\n",
    "model = CustomAdaBoost(n_weak_learners=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.mean(y_test == y_pred)\n",
    "print(f'Test Accuracy (Custom): {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8c5cf",
   "metadata": {},
   "source": [
    "## 10. Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "16009685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (SK): 0.9649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialise AdaBoost with decision stumps\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "ada = AdaBoostClassifier(estimator=base_estimator,\n",
    "                         n_estimators=10, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = ada.predict(X_test)\n",
    "print(f'Test Accuracy (SK): {accuracy_score(y_test, y_pred):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
