{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66f2663",
   "metadata": {},
   "source": [
    "# Adaptive Boosting from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1587216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from numpy.typing import NDArray\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29553c26",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Adaptive Boosting (AdaBoost) is a foundational ensemble learning algorithm designed to improve the accuracy of machine learning models by combining multiple **weak classifiers** (often decision stumps - decision trees with a single split) into a single **strong classifier**. Althought AdaBoost is primarily for binary classification, it has been extended to handle multiclass problems and regression tasks in some variants. However, its core mechanism and main use case remain in binary classification.\n",
    "\n",
    "### Advantages:\n",
    "- Turn weak models into a strong classifier.\n",
    "- Less overfitting.\n",
    "- No need for parameter tuning.\n",
    "\n",
    "### Limitations:\n",
    "- Sensitive to outliers as misclassified samples get higher weights.\n",
    "- Primarily for binary classification.\n",
    "\n",
    "### Steps:\n",
    "1. Initialise weights.\n",
    "2. For each boosting round (M iterations),\n",
    "    - Train a weak lerner (decision stump).\n",
    "    - Compute weighted error.\n",
    "    - Calculate lerner weights $\\alpha$.\n",
    "    - Update sample weights.\n",
    "    - Repeat for the maximum number of iterations or until weighted error is sufficiently low.\n",
    "3. Predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1022b",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "74e4048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "y = np.where(y == 0, -1, 1)     # AdaBoost expects labels as -1 and +1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394850d",
   "metadata": {},
   "source": [
    "## 3. Initialising Weights\n",
    "All training samples are initialised with equal weight:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_i = \\dfrac{1}{N}\n",
    "\\end{align*}\n",
    "\n",
    "where $N$ is the number of samples. For $N = 5$, the initial weights of the sample will be:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_i = \\dfrac{1}{5} = 0.2\n",
    "\\end{align*}\n",
    "\n",
    "The `np.full` function from NumPy library can generate an array of the specified length with every entry set to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0075d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_weights(n_samples: int) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Initialise all sample weights equally.\n",
    "    \"\"\"\n",
    "    return np.full(n_samples, 1 / n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3676a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For N = 5: [0.2 0.2 0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(f'For N = 5: {initialise_weights(5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f604e6",
   "metadata": {},
   "source": [
    "## 4. Finding the Best Stump\n",
    "<!-- The following `find_best_stump` function searches all features and possible thresholds, and for each, tries both polarities (direction of the inequality). It predicts labels, computes the weighted error, and keeps the stump with the lowest error. -->\n",
    "The following `find_best_stump` function implements the decision stump: It exhaustively searches for the best one-level split across all features and possible thresholds consdering both directions (polarities), and selects the split that minimises the weighted classification error.\n",
    "\n",
    "1. Initialise variables.\n",
    "2. Loop over all features and thresholds (unique values).\n",
    "3. Loop over both polarities: $[1, -1]$.\n",
    "4. Make predictions.\n",
    "    - Initialise all predictions to $+1$.\n",
    "    - For polarity $1$: set to $-1$ if $\\text{value} < \\text{threshold}$.\n",
    "    - Otherwise: set to $+1$.\n",
    "5. Calculate weighted error.\n",
    "\\begin{align*}\n",
    "    \\epsilon_m = \\dfrac{\\sum^{N}_{i=1} w_i \\cdot \\mathbb{I}(h_m(x_i) \\neq y_i)}{\\sum^{N}_{i=1}w_i}\n",
    "\\end{align*}\n",
    "\n",
    "    where:\n",
    "    - $h_m$: $m$-th weak learner.\n",
    "    - $y_i$: True label.\n",
    "    - $\\mathbb{I}$: Indicator function.\n",
    "\n",
    "    In fact, weighted error is just a sum of weights for misclassified samples.\n",
    "6. If the error rate is smaller than `min_error`, update the value (`min_error = error`), best stump and best prediction.\n",
    "7. Return `best_stump`, `min_error`, and `best_predictions` with the least error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_stump(X, y, sample_weights):\n",
    "    \"\"\"\n",
    "    Find the decision stump (feature, threshold, polarity) that minimises weighted error.\n",
    "    Returns: feature_index, threshold, polarity, min_error, predictions\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    min_error = float('inf')\n",
    "    best_stump = {}\n",
    "    best_predictions = None\n",
    "\n",
    "    for feature_i in range(n_features):  # Each feature\n",
    "        feature_vals = X[:, feature_i]  # All values in the selected features\n",
    "        thresholds = np.unique(feature_vals)  # Unique values in feature_vals\n",
    "        for threshold in thresholds:\n",
    "            for polarity in [1, -1]:\n",
    "                # Predict: 1 if (polarity * feature) < (polarity * threshold), else -1\n",
    "                predictions = np.ones(n_samples)\n",
    "                if polarity == 1:\n",
    "                    predictions[feature_vals < threshold] = -1\n",
    "                else:\n",
    "                    predictions[feature_vals > threshold] = -1\n",
    "\n",
    "                # Calculate weighted error\n",
    "                misclassified = predictions != y\n",
    "                error = np.sum(sample_weights[misclassified])\n",
    "\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    best_stump = {\n",
    "                        \"feature_index\": feature_i,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"polarity\": polarity\n",
    "                    }\n",
    "                    best_predictions = predictions.copy()\n",
    "    return best_stump, min_error, best_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28b618",
   "metadata": {},
   "source": [
    "## 5. Learner Weights\n",
    "For the current learner $m$, the new weight $\\alpha_m$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\alpha_m = \\dfrac{1}{2} \\text{ln} \\left( \\dfrac{1-\\epsilon_m + \\text{c}}{\\epsilon_m + \\text{c}} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\epsilon_m$: Error rate calculated inside the `find_best_stump()` function.\n",
    "- $c$: Small constant added to avoid division by zero. Set to $1 \\times 10^{-10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c5922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(error):\n",
    "    \"\"\"\n",
    "    Compute the weight of the weak learner (alpha).\n",
    "    \"\"\"\n",
    "    c = 1e-10  # constant\n",
    "    return 0.5 * np.log((1 - error + c) / (error + c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
