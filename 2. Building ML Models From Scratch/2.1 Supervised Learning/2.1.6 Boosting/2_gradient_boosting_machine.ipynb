{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29e0db9",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "    - [Advantages](#advantages)\n",
    "    - [Limitations](#limitations)\n",
    "    - [Steps](#steps)\n",
    "2. [Loading Data](#2-loading-data)\n",
    "3. [Loss Function](#3-loss-function)\n",
    "    - [Regression](#regression)\n",
    "    - [Binary Classification](#binary-classification)\n",
    "    - [Multi-class Classification](#multi-class-classification)\n",
    "4. [Initialising Model](#4-initialising-model)\n",
    "5. [Finding the Best Split](#5-finding-the-best-split)\n",
    "6. [Building Trees](#6-building-trees)\n",
    "7. [Predictions (Trees)](#7-predictions-trees)\n",
    "8. [Training Model](#8-training-model)\n",
    "9. [Final Predictions](#9-final-predictions)\n",
    "10. [Evaluation Metrics](#10-evaluation-metrics)\n",
    "    - [Mean Squared Error (MSE)](#mean-squared-error-mse)\n",
    "    - [Root Mean Squared Error (RMSE)](#root-mean-squared-error-rmse)\n",
    "    - [Mean Absolute Error (MAE)](#mean-absolute-error-mae)\n",
    "    - [R-Squared ($R^2$)](#r-squared)\n",
    "11. [Encapsulation](#11-encapsulation)\n",
    "12. [Comparison with Scikit-Learn](#12-comparison-with-scikit-learn)\n",
    "13. [References](#13-references)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677e0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d6649c",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "**Gradient Boosting Machine (GBM)** is an ensemble machine learning model that builds a strong predictive model by sequentially combining multiple weak models (typically decision trees) in a stage-wise manner. The core idea is to iteratively add new models that correct the errors made by the existing ensemble, thereby improving overall predictive accuracy.\n",
    "\n",
    "Suppose we have a dataset ${(x_i, y_i)}^n_{i=1}$ where $x_i$ are the features and $y_i$ are the target values. The goal of gradient boosting is to find a function $F(x)$ that minimises a given differentiable loss function $L(y, F(x))$:\n",
    "\n",
    "\\begin{align*}\n",
    "    F(x) = F_0(x) + \\sum^{M}_{m=1}\\gamma_m h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial model (e.g., the mean of $y$).\n",
    "- $\\gamma_i$: Weight (step size) for the $m$-th weak learner, typically determined by minimising the loss function along the direction of $h_m(x)$.\n",
    "- $M$: Number of boosting iterations (e.g., the number of weak learners).\n",
    "- $h_m$: prediction from the $m$-th weak learner (e.g., a decision tree).\n",
    "\n",
    "\n",
    "GBM can be applied to both regression and classification tasks:\n",
    "- For **regression**, gradient boosting typically minimises a loss function such as Mean Squared Error(MSE), producing continuous output values.\n",
    "- For **classification**, the method is adapted to minimise a classification-specific loss function (e.g., logistic loss), yielding class probabilities or class labels.\n",
    "\n",
    "### Advantages\n",
    "- High predictive accuracy, even with non-linear relationships.\n",
    "- Flexible for both regression and classification tasks.\n",
    "- Can handle numerical and categorical values, as well as missing data without extensive preprocessing.\n",
    "\n",
    "### Limitations\n",
    "- Highly prone to overfitting.\n",
    "- Computationally expensive and time-consuming for large datasets or a large number of trees.\n",
    "- Sensitive to hypermarameters (e.g., learning rate, number of trees, tree depth, regularisation parameters).\n",
    "- Poor at handling extrapolation, struggles to predict outcomes outside the range of the training data.\n",
    "\n",
    "### Steps\n",
    "1. Initialise the model:\n",
    "    - Start with a simple model, typically a constant value:\n",
    "        - For regression: the mean of the target variable.\n",
    "        - For binary classification: the log-odds of the positive classes.\n",
    "1. Calculate residuals (Negative Gradients):\n",
    "    - For each iteration, compute the residuals, which are the negative gradients of the loss function with respect to the current predictions.\n",
    "    - This step can be generalised to any differentiable loss function, not just the mean squared error.\n",
    "1. Fit a new weak model to predict the residuals:\n",
    "    - Train a weak learner (typically a shallow decision tree) to predict the rediduals.\n",
    "    - The weak learner focuses on correcting the errors made by the current ensemble.\n",
    "1. Update the model:\n",
    "    - Add the predictions from the new weak learners to the current model's predictions, scaled by a learning rate (shrinkage parameter).\n",
    "    - This update moves the ensemble closer to the true values by correcting previous errors.\n",
    "1. Repeat steps 2-4 for a pre-defined number of iterations.\n",
    "1. Final prediction:\n",
    "    - The sum of the initial prediction and the scaled outputs of all weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c8795c",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "Retrieved from [GitHub - YBI Foundation](https://github.com/YBI-Foundation/Dataset/blob/main/Admission%20Chance.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef6e9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Serial No",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GRE Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOEFL Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "University Rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " SOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOR ",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CGPA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chance of Admit ",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "da888471-efe1-4446-a205-956968bb54ef",
       "rows": [
        [
         "0",
         "1",
         "337",
         "118",
         "4",
         "4.5",
         "4.5",
         "9.65",
         "1",
         "0.92"
        ],
        [
         "1",
         "2",
         "324",
         "107",
         "4",
         "4.0",
         "4.5",
         "8.87",
         "1",
         "0.76"
        ],
        [
         "2",
         "3",
         "316",
         "104",
         "3",
         "3.0",
         "3.5",
         "8.0",
         "1",
         "0.72"
        ],
        [
         "3",
         "4",
         "322",
         "110",
         "3",
         "3.5",
         "2.5",
         "8.67",
         "1",
         "0.8"
        ],
        [
         "4",
         "5",
         "314",
         "103",
         "2",
         "2.0",
         "3.0",
         "8.21",
         "0",
         "0.65"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No  GRE Score  TOEFL Score  University Rating   SOP  LOR   CGPA  \\\n",
       "0          1        337          118                  4   4.5   4.5  9.65   \n",
       "1          2        324          107                  4   4.0   4.5  8.87   \n",
       "2          3        316          104                  3   3.0   3.5  8.00   \n",
       "3          4        322          110                  3   3.5   2.5  8.67   \n",
       "4          5        314          103                  2   2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/YBI-Foundation/Dataset/refs/heads/main/Admission%20Chance.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50879113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (400, 8)\n",
      "Target shape: (400,)\n",
      "Features: \n",
      "['Serial No', 'GRE Score', 'TOEFL Score', 'University Rating', ' SOP', 'LOR ', 'CGPA', 'Research']\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "feature_names = df.columns[:-1].tolist()  # All columns except the last one\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "print(f'Features: \\n{feature_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8361805",
   "metadata": {},
   "source": [
    "## 3. Loss Function\n",
    "For regression, gradient boosting minimises the Mean Squared Error (MSE) by iteratively fitting new models to the negative gradient of the loss function with respect to the current predictions. Although there is no explicit call to MSE, it is integrated inside the gradient calculation.\n",
    "\n",
    "### Regression\n",
    "The most common loss function for regression is the mean squared error (MSE):\n",
    "\n",
    "\\begin{align*}\n",
    "    L(y, \\hat y) = \\dfrac{1}{n}\\sum^{n}_{i=1}(y_i - \\hat y_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True value.\n",
    "- $\\hat y_i$: Predicted value.\n",
    "- $n$: Number of samples.\n",
    "\n",
    "### Binary Classification\n",
    "The standard loss function for binary classification in XGBoost is the binary cross-entropy (log loss):\n",
    "\n",
    "\\begin{align*}\n",
    "    L(y, \\hat p) = - \\dfrac{1}{n} \\sum^{n}_{i=1}\\left[y_i \\log{(\\hat p_i) + (1-y_i) \\log{(1- \\hat p_i)}} \\right]\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True label ($0$ or $1$).\n",
    "- $\\hat p_i$: Predicted probability for class 1 (after applying the sigmoid function to the raw score).\n",
    "- $n$: Number of samples.\n",
    "\n",
    "### Multi-class Classification\n",
    "For multi-class classification (with $K$ classes), XGBoost uses the multi-class cross-entropy (also called softmax loss):\n",
    "\n",
    "\\begin{align*}\n",
    "    L(y, \\hat P) = - \\dfrac{1}{n} \\sum^{n}_{i=1} \\sum^{K}_{k=1} \\mathbb{I}(y_i = k) \\log{(\\hat p_{ik})}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True class label for sample $i$.\n",
    "- $\\hat p_i$: Predicted probability that sample $i$ belongs to class $k$ (output of the softmax function).\n",
    "- $\\mathbb{I}(y_i = k)$: Indicator function, equal to $1$ if $y_i = k$ and $0$ otherwise.\n",
    "- $n$: Number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be4833",
   "metadata": {},
   "source": [
    "## 4. Initialising Model\n",
    "First, we need to initialise the model with a constant function that minimises the loss (initial predictions).\n",
    "\n",
    "\\begin{align*}\n",
    "    F_0(x) = \\arg \\min_{\\gamma}\\sum^{n}_{i=1}L(y_i, \\gamma)\n",
    "\\end{align*}\n",
    "\n",
    "For squared error, the best constant is the mean of the target values. Thus,\n",
    "\n",
    "\\begin{align*}\n",
    "    F_0(x) = \\bar y = \\dfrac{1}{n}\\sum^{n}_{i=1}y_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202d2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(y: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Initialise predictions with the mean of the target values.\n",
    "\n",
    "    Parameters:\n",
    "        y: Target values, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Array of initial predictions, each set to mean of y.\n",
    "    \"\"\"\n",
    "    return np.full_like(y, np.mean(y), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae3d1d",
   "metadata": {},
   "source": [
    "## 5. Finding the Best Split\n",
    "The following functions search for the best feature and threshold to split data into two, such that the variance of the resulting child nodes is minimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c528ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(y: NDArray[np.float64]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the variance of the target values.\n",
    "\n",
    "    Args:\n",
    "        y: Target values.\n",
    "\n",
    "    Returns:\n",
    "        float: Variance of y.\n",
    "    \"\"\"\n",
    "    return np.var(y)\n",
    "\n",
    "\n",
    "def split_dataset(X: NDArray[np.float64], y: NDArray[np.float64], feature_index: int,\n",
    "                  threshold: float) -> Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Splits the dataset based on a feature and threshold.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        y: Target values, shape (n_samples,).\n",
    "        feature_index: Index of the feature to split on.\n",
    "        threshold: Threshold value for the split.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing X_left, y_left, X_right, y_right after the split.\n",
    "    \"\"\"\n",
    "    left_mask = X[:, feature_index] < threshold\n",
    "    right_mask = ~left_mask\n",
    "    return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "\n",
    "def best_split(X: NDArray[np.float64], y: NDArray[np.float64], min_samples_leaf: int) -> Tuple[int | None, float | None]:\n",
    "    \"\"\"\n",
    "    Find the best feature and threshold to split the dataset, minimising weighted variance.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        y: Target values.\n",
    "        min_samples_leaf: Minimum number of samples required at a leaf node.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (best_feature, best_threshold). Returns (None, None) if no valid split is found.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    best_feature, best_threshold, best_var = None, None, float('inf')\n",
    "    for feature in range(n):\n",
    "        thresholds = np.unique(X[:, feature])\n",
    "        for threshold in thresholds:\n",
    "            _, y_left, _, y_right = split_dataset(X, y, feature, threshold)\n",
    "            if len(y_left) < min_samples_leaf or len(y_right) < min_samples_leaf:\n",
    "                continue\n",
    "            var_left = variance(y_left)\n",
    "            var_right = variance(y_right)\n",
    "            var_split = (len(y_left) * var_left + len(y_right) * var_right) / m\n",
    "            if var_split < best_var:\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_var = var_split\n",
    "    return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56c31d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature(index): 6 with its best threshold: 8.74\n"
     ]
    }
   ],
   "source": [
    "best_f, best_th = best_split(X, y, 3)\n",
    "print(f'Best feature(index): {best_f} with its best threshold: {best_th}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428d88c",
   "metadata": {},
   "source": [
    "## 6. Building Trees\n",
    "This function recursively constructs a regression tree by splitting the data at each node using the best split found, until stopping criteria are met. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f9a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X: NDArray[np.float64], y: NDArray[np.float64], max_depth: int, min_samples_leaf: int, depth: int = 0) -> Dict[str, Any] | float:\n",
    "    \"\"\"\n",
    "    Recursively build a regression tree.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix.\n",
    "        y: Target values.\n",
    "        max_depth: Maximum depth of the tree.\n",
    "        min_samples_leaf: Minimum samples required at a leaf node.\n",
    "        depth: Current depth of the tree (default is 0).\n",
    "\n",
    "    Returns:\n",
    "        Tree as a nested dictionary, or a float if a leaf node.\n",
    "    \"\"\"\n",
    "    if depth >= max_depth or len(y) <= min_samples_leaf:\n",
    "        return np.mean(y)\n",
    "    feature, threshold = best_split(X, y, min_samples_leaf)\n",
    "    if feature is None:\n",
    "        return np.mean(y)\n",
    "    X_left, y_left, X_right, y_right = split_dataset(X, y, feature, threshold)\n",
    "    return {\n",
    "        'feature': feature,\n",
    "        'threshold': threshold,\n",
    "        'left': build_tree(X_left, y_left, max_depth, min_samples_leaf, depth + 1),\n",
    "        'right': build_tree(X_right, y_right, max_depth, min_samples_leaf, depth + 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc4393b",
   "metadata": {},
   "source": [
    "## 7. Predictions (Trees)\n",
    "The two functions `predict_tree()` and `predict_tree_batch()` are used to make predictions with a regression decision tree represented as a nested dictionary.\n",
    "\n",
    "- `predict_tree()`: Predicts the output for a single data sample $x$ by traversing the decision tree.\n",
    "- `predict_tree_batch()`: Generates predictions for a batch of samples by applying the `predict_tree` function to each sample in the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e81c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree(tree: Dict[str, Any] | float, x: NDArray[np.float64]) -> float:\n",
    "    \"\"\"\n",
    "    Predict the target value for a single sample using the regression tree.\n",
    "\n",
    "    Args:\n",
    "        tree: The regression tree or a leaf value.\n",
    "        x: Feature vector for a single sample.\n",
    "\n",
    "    Returns:\n",
    "        float: Predicted value.\n",
    "    \"\"\"\n",
    "    while isinstance(tree, dict):\n",
    "        if x[tree['feature']] < tree['threshold']:\n",
    "            tree = tree['left']\n",
    "        else:\n",
    "            tree = tree['right']\n",
    "    return tree\n",
    "\n",
    "\n",
    "def predict_tree_batch(tree: Dict[str, Any] | float, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Predict target values for a batch of samples using the regression tree.\n",
    "\n",
    "    Args:\n",
    "        tree: The regression tree or a leaf value.\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        Predicted values for all samples.\n",
    "    \"\"\"\n",
    "    return np.array([predict_tree(tree, x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905235d",
   "metadata": {},
   "source": [
    "## 8. Training Model\n",
    "During the training process of gradient boosting:\n",
    "1. Initialise model predictions as $F_0(x) = \\bar y$.\n",
    "2. Boosting loop from $m=1$ to $M$ (number of boosting rounds `n_estimators`):\n",
    "    - Compute residuals (negative gradients of the MSE loss function).<br><br>\n",
    "    \\begin{align*}\n",
    "    r_{im} &= - \\left[ \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1(x)}} \\\\\n",
    "     &= - \\frac{\\partial}{\\partial \\hat y_i} \\dfrac{1}{2}(y_i - \\hat y_i)^2 = +(y_i - \\hat y_i)\n",
    "    \\end{align*}\n",
    "        Therefore, $r_{im} = y_i - \\hat y_i$ where $\\hat y$ is the current prediction for sample.\n",
    "\n",
    "    - Fit tree to residuals.\n",
    "    - Append the fitted tree.\n",
    "    - Update predictions.<br><br>\n",
    "    For each boosting iteration, after fitting the weak learner $h_m(x)$ to the pseudo-residuals, the optimal $\\gamma_m$ can be found by solving:\n",
    "    \\begin{align*}\n",
    "        \\gamma_m = \\arg \\min_{\\gamma}\\sum^{n}_{i=1}L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\n",
    "    \\end{align*}\n",
    "    However, in practice, $\\gamma_m$ is often absorbed into the learning rate (denoted as $\\eta$) or set to 1 for simplicity. Thus the update is written as:\n",
    "    \\begin{align*}\n",
    "        \\hat y_i \\leftarrow \\hat y_i + \\eta \\cdot h_m(x_i)\n",
    "    \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "122a605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_fit(X: NDArray[np.float64], y: NDArray[np.float64],\n",
    "                          n_estimators: int = 10, learning_rate: float = 0.1,\n",
    "                          max_depth: int = 3, min_samples_leaf: int = 1) -> Tuple[float, List[Dict[str, Any] | float], float]:\n",
    "    \"\"\"\n",
    "    Fit a gradient boosting regressor.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        y: Target values.\n",
    "        n_estimators: Number of boosting rounds (default is 10).\n",
    "        learning_rate: Learning rate (default is 0.1).\n",
    "        max_depth: Maximum depth of each tree (default is 3).\n",
    "        min_samples_leaf: Minimum samples per leaf (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - Initial prediction (mean of y).\n",
    "            - List of fitted trees.\n",
    "            - Learning rate.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    predictions = initialise_model(y)\n",
    "    for m in range(n_estimators):\n",
    "        residuals = y - predictions\n",
    "        tree = build_tree(X, residuals, max_depth, min_samples_leaf)\n",
    "        models.append(tree)\n",
    "        update = predict_tree_batch(tree, X)\n",
    "        predictions += learning_rate * update\n",
    "    return np.mean(y), models, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c56cc",
   "metadata": {},
   "source": [
    "## 9. Final Predictions\n",
    "The final predictions after $M$ trees is:\n",
    "\\begin{align*}\n",
    "    F_M(x) = F_0(x) + \\sum^{M}_{m=1} \\eta \\cdot h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial prediction (mean of $y$).\n",
    "- $h_m(x)$: Prediction of the $m$-th tree.\n",
    "- $\\eta$: Learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9266c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_predict(X: NDArray[np.float64], initial_prediction: float, models,\n",
    "                              learning_rate: float) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Predicts using the fitted gradient boosting regressor.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        initial_prediction: Initial prediction (mean of y from training).\n",
    "        models: List of fitted regression trees.\n",
    "        learning_rate: Learning rate used during training.\n",
    "\n",
    "    Returns:\n",
    "        Predicted values for all samples.\n",
    "    \"\"\"\n",
    "    y_pred = np.full(X.shape[0], initial_prediction, dtype=float)\n",
    "    for tree in models:\n",
    "        y_pred += learning_rate * predict_tree_batch(tree, X)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b89e0",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metrics\n",
    "### Mean Squared Error (MSE)\n",
    "Mean Squared Error measures the average squared difference between predicted ($\\hat y$) and actual ($y$) values. Large errors are penalised heavily. Smaller MSE indicates better predictions.\n",
    "\n",
    "\\begin{align*}\n",
    "MSE = \\dfrac{1}{n} \\sum_{i=1}^{n}(\\hat y_{i} = y_{i})^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f15f4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e03c11",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "Square root of MSE. It provides error in the same unit as the target variable ($y$) and easier to interpret.\n",
    "\n",
    "\\begin{align*}\n",
    "RMSE = \\sqrt{(MSE)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac798d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3bc03",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "Mean Absolute Error measures the average absolute difference between predicted ($\\hat y$) and actual ($y$) values. It is less sensitive to outliers than MSE. Smaller MAE indicates better predictions.\n",
    "\n",
    "\\begin{align*}\n",
    "MAE = \\dfrac{1}{n} \\sum_{i=1}^{n}|\\hat y_{i} = y_{i}|\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "006e41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a72f3",
   "metadata": {},
   "source": [
    "<a id=\"r-squared\"></a>\n",
    "### R-Squared($R^2$)\n",
    "\n",
    "R-squared indicated the proportion of variance in the dependent variable that is predictable from the independent variables. Value ranges from 0 to 1. Closer to 1 indicates a better fit.\n",
    "\n",
    "\n",
    "\n",
    "Residual Sum of Squares ($SS_{residual}$): \n",
    "\\begin{align*}\n",
    "SS_{residual} = \\sum_{i=1}^{n} (y_{i} - \\hat y_{i})^{2}\n",
    "\\end{align*}\n",
    "\n",
    "Total Sum of Squares ($SS_{total}$): \n",
    "\\begin{align*}\n",
    "SS_{total} = \\sum_{i=1}^{n} (y_{i} - \\bar y_{i})^{2}\n",
    "\\end{align*}\n",
    "\n",
    "$R^2$ is computed as:\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "R^2 = 1 - \\dfrac{SS_{residual}}{SS_{total}} = 1 - \\dfrac{\\sum_{i=1}^{n} (y_{i} - \\hat y_{i})^{2}}{\\sum_{i=1}^{n} (y_{i} - \\bar y_{i})^{2}}\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "$y$: Actual target values.\n",
    "\n",
    "$\\bar y$: Mean of the actual target values.\n",
    "\n",
    "$\\hat y$: Precicted target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8cb0ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "170dfed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate and return evaluation metrics for a regression model, including MSE, RMSE, MAE, and R-squared.\n",
    "\n",
    "     Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "    Returns:\n",
    "        - mse: Mean Squared Error (MSE), indicating the average of the squared differences between predicted and true values.\n",
    "        - rmse: Root Mean Squared Error (RMSE), indicating the standard deviation of the residuals.\n",
    "        - mae: Mean Absolute Error (MAE), representing the average absolute difference between predicted and true values.\n",
    "        - r2: R-squared (coefficient of determination), showing the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n",
    "    \"\"\"\n",
    "    mse = calculate_MSE(y_true, y_pred)\n",
    "    rmse = calculate_RMSE(y_true, y_pred)\n",
    "    mae = calculate_MAE(y_true, y_pred)\n",
    "    r2 = calculate_r2(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456b70d",
   "metadata": {},
   "source": [
    "## 11. Encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e573e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGBRegressor:\n",
    "    \"\"\"\n",
    "    Custom Gradient Boosting for regression with decision trees.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 3,\n",
    "        learning_rate: float = 0.1,\n",
    "        max_depth: int = 3,\n",
    "        min_samples_leaf: int = 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialises the CustomGBRegressor.\n",
    "\n",
    "        Args:\n",
    "            n_estimators: Number of boosting rounds.\n",
    "            learning_rate: Learning rate (shrinkage).\n",
    "            max_depth: Maximum depth of each tree.\n",
    "            min_samples_leaf: Minimum samples per leaf.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.models: List[Dict[str, Any] | float] = []\n",
    "        self.initial_prediction: float = 0.0\n",
    "\n",
    "    def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -> None:\n",
    "        \"\"\"\n",
    "        Fit the gradient boosting regressor to the data.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "            y: Target values, shape (n_samples,).\n",
    "        \"\"\"\n",
    "        self.models = []\n",
    "        self.initial_prediction = float(np.mean(y))\n",
    "        predictions = np.full_like(y, self.initial_prediction, dtype=float)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "            tree = self._build_tree(\n",
    "                X, residuals, self.max_depth, self.min_samples_leaf)\n",
    "            self.models.append(tree)\n",
    "            update = self._predict_tree_batch(tree, X)\n",
    "            predictions += self.learning_rate * update\n",
    "\n",
    "    def predict(self, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Predict target values for given feature matrix.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted values.\n",
    "        \"\"\"\n",
    "        y_pred = np.full(X.shape[0], self.initial_prediction, dtype=np.float64)\n",
    "        for tree in self.models:\n",
    "            y_pred += self.learning_rate * self._predict_tree_batch(tree, X)\n",
    "        return y_pred\n",
    "\n",
    "    def _variance(self, y: NDArray[np.float64]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the variance of the target values.\n",
    "\n",
    "        Args:\n",
    "            y: Target values.\n",
    "\n",
    "        Returns:\n",
    "            float: Variance of y.\n",
    "        \"\"\"\n",
    "        return np.var(y)\n",
    "\n",
    "    def _split_dataset(self, X: NDArray[np.float64],\n",
    "                       y: NDArray[np.float64], feature_index: int,\n",
    "                       threshold: float) -> Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]:\n",
    "        \"\"\"\n",
    "        Splits the dataset based on a feature and threshold.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "            y: Target values, shape (n_samples,).\n",
    "            feature_index: Index of the feature to split on.\n",
    "            threshold: Threshold value for the split.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing X_left, y_left, X_right, y_right after the split.\n",
    "        \"\"\"\n",
    "        left_mask = X[:, feature_index] < threshold\n",
    "        right_mask = ~left_mask\n",
    "        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
    "\n",
    "    def _best_split(self, X: NDArray[np.float64],\n",
    "                    y: NDArray[np.float64], min_samples_leaf: int) -> Tuple[int | None, float | None]:\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split the dataset, minimising weighted variance.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "            y: Target values.\n",
    "            min_samples_leaf: Minimum number of samples required at a leaf node.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (best_feature, best_threshold). Returns (None, None) if no valid split is found.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        best_feature, best_threshold, best_var = None, None, float('inf')\n",
    "        for feature in range(n):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                _, y_left, _, y_right = self._split_dataset(\n",
    "                    X, y, feature, threshold)\n",
    "                if len(y_left) < min_samples_leaf or len(y_right) < min_samples_leaf:\n",
    "                    continue\n",
    "                var_left = self._variance(y_left)\n",
    "                var_right = self._variance(y_right)\n",
    "                var_split = (len(y_left) * var_left +\n",
    "                             len(y_right) * var_right) / m\n",
    "                if var_split < best_var:\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    best_var = var_split\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _build_tree(self, X: NDArray[np.float64], y: NDArray[np.float64],\n",
    "                    max_depth: int, min_samples_leaf: int, depth: int = 0) -> Dict[str, Any] | float:\n",
    "        \"\"\"\n",
    "        Recursively build a regression tree.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix.\n",
    "            y: Target values.\n",
    "            max_depth: Maximum depth of the tree.\n",
    "            min_samples_leaf: Minimum samples required at a leaf node.\n",
    "            depth: Current depth of the tree (default is 0).\n",
    "\n",
    "        Returns:\n",
    "            Tree as a nested dictionary, or a float if a leaf node.\n",
    "        \"\"\"\n",
    "        if depth >= max_depth or len(y) <= min_samples_leaf:\n",
    "            return float(np.mean(y))\n",
    "        feature, threshold = self._best_split(X, y, min_samples_leaf)\n",
    "        if feature is None:\n",
    "            return float(np.mean(y))\n",
    "        X_left, y_left, X_right, y_right = self._split_dataset(\n",
    "            X, y, feature, threshold)\n",
    "        return {\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'left': self._build_tree(X_left, y_left, max_depth, min_samples_leaf, depth + 1),\n",
    "            'right': self._build_tree(X_right, y_right, max_depth, min_samples_leaf, depth + 1)\n",
    "        }\n",
    "\n",
    "    def _predict_tree(self, tree: Dict[str, Any] | float, x: NDArray[np.float64]) -> float:\n",
    "        \"\"\"\n",
    "        Predict the target value for a single sample using the regression tree.\n",
    "\n",
    "        Args:\n",
    "            tree: The regression tree or a leaf value.\n",
    "            x: Feature vector for a single sample.\n",
    "\n",
    "        Returns:\n",
    "            float: Predicted value.\n",
    "        \"\"\"\n",
    "        while isinstance(tree, dict):\n",
    "            if x[tree['feature']] < tree['threshold']:\n",
    "                tree = tree['left']\n",
    "            else:\n",
    "                tree = tree['right']\n",
    "        return float(tree)\n",
    "\n",
    "    def _predict_tree_batch(self, tree: Dict[str, Any] | float, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Predict target values for a batch of samples using the regression tree.\n",
    "\n",
    "        Args:\n",
    "            tree: The regression tree or a leaf value.\n",
    "            X: Feature matrix, shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted values for all samples.\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_tree(tree, x) for x in X], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30335a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Custom): 0.0038\n",
      "RMSE (Custom): 0.0614\n",
      "MAE (Custom): 0.0427\n",
      "R-Squared (Custom): 0.8541\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model_custom = CustomGBRegressor(n_estimators=200, learning_rate=0.1,\n",
    "                                 max_depth=2, min_samples_leaf=1)\n",
    "model_custom.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_custom = model_custom.predict(X_test)\n",
    "mse_custom, rmse_custom, mae_custom, r2_custom = evaluate(\n",
    "    y_test, y_pred_custom)\n",
    "print(f'MSE (Custom): {mse_custom:.4f}')\n",
    "print(f'RMSE (Custom): {rmse_custom:.4f}')\n",
    "print(f'MAE (Custom): {mae_custom:.4f}')\n",
    "print(f'R-Squared (Custom): {r2_custom:.4f}')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4f341",
   "metadata": {},
   "source": [
    "## 12. Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25ccdaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (SK): 0.0039\n",
      "RMSE (SK): 0.0625\n",
      "MAE (SK): 0.0449\n",
      "R-Squared (SK): 0.8489\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Instantiate and fit the sklearn model\n",
    "sklearn_gbm = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "sklearn_gbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_sklearn = sklearn_gbm.predict(X_test)\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "rmse_sklearn = root_mean_squared_error(y_test, y_pred_sklearn)\n",
    "mae_sklearn = mean_absolute_error(y_test, y_pred_sklearn)\n",
    "r2_sklearn = r2_score(y_test, y_pred_sklearn)\n",
    "print(f'MSE (SK): {mse_sklearn:.4f}')\n",
    "print(f'RMSE (SK): {rmse_sklearn:.4f}')\n",
    "print(f'MAE (SK): {mae_sklearn:.4f}')\n",
    "print(f'R-Squared (SK): {r2_sklearn:.4f}')\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20112bb8",
   "metadata": {},
   "source": [
    "## 13. References\n",
    "1. Andreas Mueller. (2020). *Applied ML 2020 - 08 - Gradient Boosting.* <br>\n",
    "https://www.youtube.com/watch?v=yrTW5YTmFjw\n",
    "\n",
    "1. Bex Tuychiev. (2023). *A Guide to The Gradient Boosting Algorithm.* <br>\n",
    "https://www.datacamp.com/tutorial/guide-to-the-gradient-boosting-algorithm\n",
    "\n",
    "1. DataMListic. (2023). *Gradient Boosting with Regression Trees Explained* [YouTube Video]. <br>\n",
    "https://youtu.be/lOwsMpdjxog\n",
    "\n",
    "1. scikit-learn. (n.d.). *GradientBoostingRegressor — scikit-learn API Reference.* <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n",
    "1. scikit-learn. (n.d.). *Gradient Boosting Regression — scikit-learn Examples.* <br>\n",
    "https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *Gradient Boost Part 1 (of 4): Regression Main Ideas* [YouTube Video]. <br>\n",
    "https://youtu.be/3CC4N4z3GJc\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *Gradient Boost Part 2 (of 4): Regression Details* [YouTube Video]. <br>\n",
    "https://youtu.be/2xudPOBz-vs\n",
    "\n",
    "1. Terence Parr and Jeremy Howard. (n.d.). *How to explain gradient boosting.* <br>\n",
    "https://explained.ai/gradient-boosting/index.html\n",
    "\n",
    "1. Tomonori Masui. (2022). *All You Need to Know about Gradient Boosting Algorithm − Part 1. Regression.* <br>\n",
    "https://medium.com/data-science/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
