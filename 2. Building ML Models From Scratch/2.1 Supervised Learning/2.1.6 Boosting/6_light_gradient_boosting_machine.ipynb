{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf80cfa0",
   "metadata": {},
   "source": [
    "# Light Gradient Boosting Machine from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "    - [Advantages](#advantages)\n",
    "    - [Limitations](#limitations)\n",
    "    - [Steps](#steps)\n",
    "1. [Loading Data](#2-loading-data)\n",
    "1. [Loss Function](#3-loss-function)\n",
    "    - [Regression](#regression)\n",
    "    - [Binary Classification](#binary-classification)\n",
    "    - [Multi-class Classification](#multi-class-classification)\n",
    "1. [Initialising Model](#4-initialising-model)\n",
    "1. [Gradient and Hessian Computation](#5-gradient-and-hessian-computation)\n",
    "1. [Histogram Binning](#6-histogram-binning)\n",
    "1. [Finding the Best Split](#7-finding-the-best-split)\n",
    "    - [Regularised Objective Function](#regularised-objective-function)\n",
    "    - [Second-Order Taylor Expansion](#second-order-taylor-expansion)\n",
    "    - [Regularisation Term](#regularisation-term)\n",
    "    - [Total Objective Function](#total-objective-function)\n",
    "    - [Optimising Leaf Weights](#optimising-leaf-weights)\n",
    "    - [Histogram-Based Split Finding](#histogram-based-split-finding)\n",
    "1. [Building Trees](#8-building-trees)\n",
    "1. [Predictions (Trees)](#9-predictions-trees)\n",
    "1. [Training Model](#10-training-model)\n",
    "1. [Final Predictions](#11-final-predictions)\n",
    "1. [Evaluation Metrics](#12-evaluation-metrics)\n",
    "    - [Binary Confusion Matrix](#binary-confusion-matrix)\n",
    "    - [Multi-Class Confusion Matrix](#multi-class-confusion-matrix)\n",
    "    - [Accuracy](#accuracy)\n",
    "    - [Precision](#precision)\n",
    "    - [Recall](#recall)\n",
    "    - [F1-Score](#f1-score)\n",
    "1. [Comparison with LightBGM](#13-comparison-with-lightgbm)\n",
    "1. [References](#14-references)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2650c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced3dbd",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "**Light Gradient Boosting Machine (LightGBM)** is a gradient boosting framework optimised for speed and scalability, making it highly suitable for regression and classification tasks involving large datasets. LightGBM uses **gradient-based one-sided sampling (GOSS)** and **exclusive feature bundling (EFB)** to speed up training and handle high-dimensional data efficiently. Traditional gradient boosting and XGBoost grow trees level-wise (depth-wise), whereas LightGBM grows trees leaf-wise. At each step, LightGBM splits the leaf with the largest loss reduction, which can lead to deeper, more complex trees and often higher accuracy, but also a higher risk of overfitting if not properly regularised.\n",
    "\n",
    "\n",
    "**Gradient Boosting Machine (GBM)** is an ensemble machine learning model that builds a strong predictive model by sequentially combining multiple weak models (typically decision trees) in a stage-wise manner. The core idea is to iteratively add new models that correct the errors made by the existing ensemble, thereby improving overall predictive accuracy.\n",
    "\n",
    "Suppose we have a dataset ${(x_i, y_i)}^n_{i=1}$ where $x_i$ are the features and $y_i$ are the target values. The goal of gradient boosting is to find a function $F(x)$ that minimises a given differentiable loss function $L(y, F(x))$:\n",
    "\n",
    "\\begin{align*}\n",
    "    F(x) = F_0(x) + \\sum^{M}_{m=1}\\gamma_m h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial model (e.g., the mean of $y$).\n",
    "- $\\gamma_i$: Weight (step size) for the $m$-th weak learner, typically determined by minimising the loss function along the direction of $h_m(x)$.\n",
    "- $M$: Number of boosting iterations (e.g., the number of weak learners).\n",
    "- $h_m$: prediction from the $m$-th weak learner (e.g., a decision tree).\n",
    "\n",
    "\n",
    "### Advantages\n",
    "- The histogram-based approach and leaf-wise growth strategy make LightGBM significantly faster and more memory-efficient, especially on large datasets.\n",
    "- The leaf-wise spliting often leads to better accuracy, as it focuses on reducing the largest errors at each iteration.\n",
    "- Efficient binning and feature bundling reduce memory usage.\n",
    "\n",
    "### Limitations\n",
    "- Overgitting risk due to the leaf-wise tree growth.\n",
    "- Sensitive to hypermarameters (e.g., learning rate, number of leaves, regularisation parameter).\n",
    "\n",
    "### Steps\n",
    "1. Initialise the model:\n",
    "    - Start with a simple model, typically a constant value:\n",
    "        - For regression: the mean of the target variable.\n",
    "        - For binary classification: the log-odds of the positive classes.\n",
    "1. Calculate residuals (Negative Gradients) and Hessian:\n",
    "    - For each iteration, compute the the negative gradients (and Hessian for second-order methods) of the loss function with respect to the current predictions.\n",
    "1. Fit a new weak model to predict the residuals:\n",
    "    - Train a weak learner (decision tree) to predict the rediduals.\n",
    "    - LightGBM grows the tree by splitting the leaf with the largest loss reduction (max delta loss), rather than level-wise as in traditional boosting.\n",
    "1. Update the model:\n",
    "    - Add the predictions from the new weak learners to the current model's predictions, scaled by a learning rate (shrinkage parameter).\n",
    "    - The update corrects the errors made by the current ensemble\n",
    "1. Repeat steps 2-4 for a pre-defined number of iterations.\n",
    "1. Final prediction:\n",
    "    - The sum of the initial prediction and the scaled outputs of all weak learners forms the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd277b4",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "Retrieved from [GitHub - YBI Foundation](https://github.com/YBI-Foundation/Dataset/blob/main/Admission%20Chance.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9880c5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Serial No",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GRE Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOEFL Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "University Rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " SOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOR ",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CGPA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chance of Admit ",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7746aec7-3e3e-41d0-a216-28fc70785c5f",
       "rows": [
        [
         "0",
         "1",
         "337",
         "118",
         "4",
         "4.5",
         "4.5",
         "9.65",
         "1",
         "0.92"
        ],
        [
         "1",
         "2",
         "324",
         "107",
         "4",
         "4.0",
         "4.5",
         "8.87",
         "1",
         "0.76"
        ],
        [
         "2",
         "3",
         "316",
         "104",
         "3",
         "3.0",
         "3.5",
         "8.0",
         "1",
         "0.72"
        ],
        [
         "3",
         "4",
         "322",
         "110",
         "3",
         "3.5",
         "2.5",
         "8.67",
         "1",
         "0.8"
        ],
        [
         "4",
         "5",
         "314",
         "103",
         "2",
         "2.0",
         "3.0",
         "8.21",
         "0",
         "0.65"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No  GRE Score  TOEFL Score  University Rating   SOP  LOR   CGPA  \\\n",
       "0          1        337          118                  4   4.5   4.5  9.65   \n",
       "1          2        324          107                  4   4.0   4.5  8.87   \n",
       "2          3        316          104                  3   3.0   3.5  8.00   \n",
       "3          4        322          110                  3   3.5   2.5  8.67   \n",
       "4          5        314          103                  2   2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/YBI-Foundation/Dataset/refs/heads/main/Admission%20Chance.csv\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e64e12dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (400, 8)\n",
      "Target shape: (400,)\n",
      "Features: \n",
      "['Serial No', 'GRE Score', 'TOEFL Score', 'University Rating', ' SOP', 'LOR ', 'CGPA', 'Research']\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "feature_names = df.columns[:-1].tolist()  # All columns except the last one\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Features: \\n{feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d4f92",
   "metadata": {},
   "source": [
    "## 3. Loss Function\n",
    "In LightGBM, the loss function is used to compute the training loss, which is then combined with regularisation terms to construct the overall objective function that is minimised during training. The gradients and Hessians used to fit new trees are calculated with respect to the training loss, but the final optimisation process incorporates both the loss and the regularisation components.\n",
    "\n",
    "### Regression\n",
    "The most common loss function for regression is the mean squared error (MSE):\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y, \\hat y) = \\dfrac{1}{n}\\sum^{n}_{i=1}(y_i - \\hat y_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True value.\n",
    "- $\\hat y_i$: Predicted value.\n",
    "- $n$: Number of samples.\n",
    "\n",
    "### Binary Classification\n",
    "The standard loss function for binary classification in XGBoost is the binary cross-entropy (log loss):\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y, \\hat p) = - \\dfrac{1}{n} \\sum^{n}_{i=1}\\left[y_i \\log{(\\hat p_i) + (1-y_i) \\log{(1- \\hat p_i)}} \\right]\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True label ($0$ or $1$).\n",
    "- $\\hat p_i$: Predicted probability for class 1 (after applying the sigmoid function to the raw score).\n",
    "- $n$: Number of samples.\n",
    "\n",
    "### Multi-class Classification\n",
    "For multi-class classification (with $K$ classes), XGBoost uses the multi-class cross-entropy (also called softmax loss):\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y, \\hat P) = - \\dfrac{1}{n} \\sum^{n}_{i=1} \\sum^{K}_{k=1} \\mathbb{I}(y_i = k) \\log{(\\hat p_{ik})}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True class label for sample $i$.\n",
    "- $\\hat p_i$: Predicted probability that sample $i$ belongs to class $k$ (output of the softmax function).\n",
    "- $\\mathbb{I}(y_i = k)$: Indicator function, equal to $1$ if $y_i = k$ and $0$ otherwise.\n",
    "- $n$: Number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446c2d1",
   "metadata": {},
   "source": [
    "## 4. Initialising Model\n",
    "First, we need to initialise the model with a constant function that minimises the loss (initial predictions).\n",
    "\n",
    "\\begin{align*}\n",
    "    F_0(x) = \\arg \\min_{\\gamma}\\sum^{n}_{i=1}L(y_i, \\gamma)\n",
    "\\end{align*}\n",
    "\n",
    "For squared error, the best constant is the mean of the target values. Thus,\n",
    "\n",
    "\\begin{align*}\n",
    "    F_0(x) = \\bar y = \\dfrac{1}{n}\\sum^{n}_{i=1}y_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "573c55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(y: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Initialise predictions with the mean of the target values.\n",
    "\n",
    "    Parameters:\n",
    "        y: Target values, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Array of initial predictions, each set to mean of y.\n",
    "    \"\"\"\n",
    "    return np.full_like(y, np.mean(y), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac49a1",
   "metadata": {},
   "source": [
    "## 5. Gradient and Hessian Computation\n",
    "We will need the first derivative (gradient) and the second derivative (Hessian) of the loss function for each sample $i$. They will be used later in the algorithm.\n",
    "\n",
    "- $g_i = \\dfrac{\\partial l(y_i, \\hat y_i)}{\\partial \\hat y_i} = \\dfrac{\\partial}{\\partial \\hat y_i} \\dfrac{1}{2}(y_i-\\hat y_i)^2 = \\hat y_i - y_i$\n",
    "- $h_i = \\dfrac{\\partial^2 l(y_i, \\hat y_i)}{\\partial \\hat y_i ^2} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4febe988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_and_hessians(\n",
    "    y_true: NDArray[np.float64], y_pred: NDArray[np.float64]\n",
    ") -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Compute gradients and Hessians for squared error loss.\n",
    "\n",
    "    Args:\n",
    "        y_true: True target values, shape (n_samples,).\n",
    "        y_pred: Predicted values, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        A tuple with gradients and Hessians, both of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    gradients = y_pred - y_true\n",
    "    hessians = np.ones_like(y_true)\n",
    "    return gradients, hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cd21c",
   "metadata": {},
   "source": [
    "## 6. Histogram Binning\n",
    "The `bin_features()` function discretises continuous feature values in a dataset into a fixed number of bins using a process called **histogram binning**. This is a key step in histogram-based gradient boosting algorithms such as LightGBM, which use binned features to accelerate split finding and reduce memory usage.\n",
    "\n",
    "For each feature (column) $j$:\n",
    "- Extract the column: $\\text{col} = X \\left[:, j\\right]$\n",
    "\n",
    "- Compute bin edges:\n",
    "    - `np.linspace()` creates `n_bins` equally spaced intervals (bins) between the minimum and maximum values of the feature. Mathematically, for feature $j$, the bin edges are:\n",
    "    $$\n",
    "    b_0 = \\min{(x_{:, j})}, b_{\\text{n\\textunderscore bins}} = \\max{(x_{:, j})}\n",
    "    $$\n",
    "\n",
    "- Store bin edges:\n",
    "    - The bin edges for each feature are appended to the `bins` list for later use.\n",
    "\n",
    "- Digitise feature values:\n",
    "    - Each value in the feature column is assigned a bin index (from $0$ to $\\text{n\\textunderscore bins} - 1$), indicating which interval it falls into.\n",
    "\n",
    "Suppose we have a sample dataset with two continuous features and five samples:\n",
    "\n",
    "| Sample | Feature 1 | Feature 2 |\n",
    "|--------|-----------|-----------|\n",
    "|   1    |   2.1     |   8.5     |\n",
    "|   2    |   3.4     |   7.3     |\n",
    "|   3    |   1.8     |   6.9     |\n",
    "|   4    |   2.9     |   9.1     |\n",
    "|   5    |   3.0     |   7.8     |\n",
    "\n",
    "Assume we want to bin each feature into **3 bins**.\n",
    "\n",
    "**Step 1: Compute Bin Edges**\n",
    "For each feature, bin edges are calculated using equally spaced intervals between the minimum and maximum values.\n",
    "- Feature 1: $\\min = 1.8$, $\\max = 3.4$\n",
    "Bin edges: $\\left[1.8, 2.3333..., 2.866..., 3.4\\right]$\n",
    "\n",
    "- Feature 2: $\\min = 6.9$, $\\max = 9.1$\n",
    "Bin edges: $\\left[6.9, 7.633..., 8.366..., 9.1\\right]$\n",
    "\n",
    "| Feature   | Min   | Max   | Bin Edges                       |\n",
    "|-----------|-------|-------|---------------------------------|\n",
    "| Feature 1 | 1.8   | 3.4   | 1.8, 2.333..., 2.866..., 3.4    |\n",
    "| Feature 2 | 6.9   | 9.1   | 6.9, 7.633..., 8.366..., 9.1    |\n",
    "\n",
    "\n",
    "\n",
    "**Step 2: Assign Bin Indices**\n",
    "Each value is assigned a bin index ($0$, $1$, or $2$) based on which interval it falls into.\n",
    "\n",
    "| Sample | Feature 1 | Bin Index (F1) | Feature 2 | Bin Index (F2) |\n",
    "|--------|-----------|----------------|-----------|----------------|\n",
    "|   1    |   2.1     |      0         |   8.5     |      2         |\n",
    "|   2    |   3.4     |      2         |   7.3     |      0         |\n",
    "|   3    |   1.8     |      0         |   6.9     |      0         |\n",
    "|   4    |   2.9     |      1         |   9.1     |      2         |\n",
    "|   5    |   3.0     |      2         |   7.8     |      1         |\n",
    "\n",
    "\n",
    "**Step 3: Binned Feature Matrix**\n",
    "The resulting binned feature matrix (each value is the bin index):\n",
    "\n",
    "| Sample | Feature 1 (Binned) | Feature 2 (Binned) |\n",
    "|--------|--------------------|--------------------|\n",
    "|   1    |         0          |         2          |\n",
    "|   2    |         2          |         0          |\n",
    "|   3    |         0          |         0          |\n",
    "|   4    |         1          |         2          |\n",
    "|   5    |         2          |         1          |\n",
    "\n",
    "This transformation enables efficient histogram-based split finding as the algorithm only needs to consider splits at bin boundaries rather than every unique feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c83705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_features(\n",
    "    X: np.ndarray, n_bins: int = 255\n",
    ") -> tuple[np.ndarray, list[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Discretise continuous features into bins using histogram binning.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        n_bins: Number of bins to discretise each feature.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - X_binned: Binned feature matrix of same shape as X, with bin indices.\n",
    "            - bins: List of bin edges for each feature.\n",
    "    \"\"\"\n",
    "    bins = []\n",
    "    X_binned = np.zeros_like(X, dtype=np.uint8)\n",
    "    for j in range(X.shape[1]):\n",
    "        col = X[:, j]\n",
    "        bin_edges = np.linspace(col.min(), col.max(), n_bins + 1)\n",
    "        bins.append(bin_edges)\n",
    "        X_binned[:, j] = np.digitize(col, bin_edges) - 1\n",
    "    return X_binned, bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d63f1",
   "metadata": {},
   "source": [
    "## 7. Finding the Best Split\n",
    "In LightGBM, the process of finding the best split is mathematically similar to XGBoost, as both are based on gradient boosting with second-order Taylor expansion. However, LightGBM introduces several algorithmic innovations, particularly the use of histogram-based split finding and a leaf-wise tree growth strategy. Below is a detailed explanation, with a focus on the LightGBM approach.\n",
    "\n",
    "### Regularised Objective Function\n",
    "For a tree at boosting iteration $t$, the regularised objective function is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}^{(t)} = \\sum^{n}_{i=1}l(y_i, \\hat y_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $l$: Loss function (e.g., mean squared error).\n",
    "- $\\hat y_i^{(t-1)}$: Prediction from previous trees.\n",
    "- $f_t$: New tree.\n",
    "- $\\Omega(f_t)$: Regularisation term.\n",
    "\n",
    "### Second-Order Taylor Expansion\n",
    "LightGBM, like XGBoost, uses a second-order Taylor expansion of the loss function around the current prediction:\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y_i, \\hat y_i^{(t-1)} + f_t(x_i)) \\approx l(y_i, \\hat y_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $g_i = \\dfrac{\\partial l(y_i, \\hat y_i)}{\\partial \\hat y_i}$\n",
    "- $h_i = \\dfrac{\\partial^2 l(y_i, \\hat y_i)}{\\partial \\hat y_i ^2}$\n",
    "\n",
    "Assume the new tree $f_t$ assigns a constant score $w_j$ to all samples in leaft $j$:\n",
    "\\begin{align*}\n",
    "    f_t(x_i) = w_{q(x_i)}\n",
    "\\end{align*}\n",
    "\n",
    "where $q(x_i)$ maps sample $i$ to its leaf.\n",
    "\n",
    "### Regularisation Term\n",
    "LightGBM typically uses only L2 regularisation for leaf weights in most practical scenarios:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Omega(f_t) = \\gamma T + \\frac{1}{2} \\lambda \\sum^{T}_{j=1}w_j^2\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $T$: Number of leaves.\n",
    "- $\\gamma$: Penalty for the number of leaves (tree complexity).\n",
    "- $\\lambda$ L2 regularisation parameter.\n",
    "\n",
    "### Total Objective Function\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{\\tilde L}^{(t)} = \\sum^{T}_{j=1} \\left[\n",
    "        G_j w_j + \\frac{1}{2}(H_j+\\lambda)w_j^2\n",
    "        \\right] + \\gamma T\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $G_j = \\sum_{i \\in I_j} g_i$: Sum of gradients in leaf $j$.\n",
    "- $H_j = \\sum_{i \\in I_j} h_i$: Sum of Hessians in leaf $j$.\n",
    "\n",
    "### Optimising Leaf Weights\n",
    "\n",
    "To find the optimal leaf weight, minimise $\\mathcal{\\tilde L^{(t)}}$ with respect to $w_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial \\mathcal{\\tilde L^{(t)}}}{\\partial w_j} = G_j + (H_j + \\lambda)w_j = 0 \\rightarrow w_j^* = - \\dfrac{G_j}{H_j + \\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "Plug $w_j^*$ back into the objective:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{\\tilde L}^{(t)} = - \\dfrac{1}{2} \\sum^{T}_{j=1} \n",
    "        \\dfrac{G_j^2}{H_j + \\lambda}\n",
    "         + \\gamma T\n",
    "\\end{align*}\n",
    "\n",
    "Suppose a node is split into left ($L$) and right ($R$) children, the gain is the reduction in the objective:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{Gain} = \\dfrac{1}{2} \\left( \n",
    "        \\dfrac{G^2_L}{H_L + \\lambda} + \\dfrac{G^2_R}{H_R + \\lambda} - \\dfrac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\n",
    "        \\right) - \\gamma\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $G_L, H_L$: Sums for the left child.\n",
    "- $G_R, H_R$: Sums for the right child.\n",
    "\n",
    "### Histogram-Based Split Finding\n",
    "\n",
    "- **Feature Binning**: LightGBM bins continuous feature values into discrete intervals (bins), greatly reducing the number of possible split points.\n",
    "\n",
    "- **Histogram Construction**: For each feature, LightGBM builds histograms of gradient and Hessian sums for each bin.\n",
    "\n",
    "- **Split Search**: The gain formula above is evaluated only at bin boundaries, making the process much faster and more memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split_histogram(\n",
    "    X_binned: NDArray[np.int64],\n",
    "    gradients: NDArray[np.float64],\n",
    "    hessians: NDArray[np.float64],\n",
    "    min_samples_leaf: int,\n",
    "    lambda_: float,\n",
    "    gamma: float,\n",
    "    n_bins: int = 255,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Find the best split for a node using histogram-based split finding.\n",
    "\n",
    "    Args:\n",
    "        X_binned: Binned feature matrix of shape (n_samples, n_features).\n",
    "        gradients: Gradients for each sample, shape (n_samples,).\n",
    "        hessians: Hessians for each sample, shape (n_samples,).\n",
    "        min_samples_leaf: Minimum number of samples required in a leaf.\n",
    "        lambda_: L2 regularisation parameter.\n",
    "        gamma: Minimum loss reduction required to make a split.\n",
    "        n_bins: Number of bins per feature.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - best_feature: Index of the best feature to split.\n",
    "            - best_bin: Bin index for the best split.\n",
    "            - best_gain: Gain value for the best split.\n",
    "    \"\"\"\n",
    "    m, n = X_binned.shape\n",
    "    best_feature, best_bin, best_gain = None, None, -np.inf\n",
    "    for feature in range(n):\n",
    "        grad_hist = np.zeros(n_bins)\n",
    "        hess_hist = np.zeros(n_bins)\n",
    "        for b in range(n_bins):\n",
    "            mask = X_binned[:, feature] == b\n",
    "            grad_hist[b] = gradients[mask].sum()\n",
    "            hess_hist[b] = hessians[mask].sum()\n",
    "        G_total, H_total = grad_hist.sum(), hess_hist.sum()\n",
    "        G_L, H_L = 0.0, 0.0\n",
    "        for b in range(n_bins - 1):\n",
    "            G_L += grad_hist[b]\n",
    "            H_L += hess_hist[b]\n",
    "            G_R = G_total - G_L\n",
    "            H_R = H_total - H_L\n",
    "            if H_L < min_samples_leaf or H_R < min_samples_leaf:\n",
    "                continue\n",
    "            gain = (\n",
    "                0.5\n",
    "                * (\n",
    "                    G_L**2 / (H_L + lambda_)\n",
    "                    + G_R**2 / (H_R + lambda_)\n",
    "                    - (G_L + G_R) ** 2 / (H_L + H_R + lambda_)\n",
    "                )\n",
    "                - gamma\n",
    "            )\n",
    "            if gain > best_gain:\n",
    "                best_feature, best_bin, best_gain = feature, b, gain\n",
    "    return best_feature, best_bin, best_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e35d56",
   "metadata": {},
   "source": [
    "## 8. Building Trees\n",
    "The following `build_tree_leafwise()` function implements the **leaf-wise tree growth strategy** for constructing a single decision tree in a gradient boosting ensemble. With this strategy, the algorithm splits the leaf with the highest gain, leading to potentially deeper and more complex trees compared to level-wise (depth-wise) growth. The formula for the optimal leaf value is:\n",
    "\n",
    "\\begin{align*}\n",
    "    w^* = - \\dfrac{\\sum g_i}{\\sum h_i + \\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "where $g_i$ and $h_i$ are the gradients and Hessians, and $\\lambda$ is the L2 regularisation parameter.\n",
    "\n",
    "The function relies on binned features and pre-computed histograms for efficiency, only evaluating splits at bin boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_leafwise(\n",
    "    X_binned: NDArray[np.uint8],\n",
    "    gradients: NDArray[np.float64],\n",
    "    hessians: NDArray[np.float64],\n",
    "    max_leaves: int,\n",
    "    min_samples_leaf: int,\n",
    "    lambda_: float,\n",
    "    gamma: float,\n",
    "    n_bins: int = 255,\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Build a decision tree using the leaf-wise growth strategy.\n",
    "\n",
    "    Args:\n",
    "        X_binned: Binned feature matrix of shape (n_samples, n_features).\n",
    "        gradients: Gradients for each sample, shape (n_samples,).\n",
    "        hessians: Hessians for each sample, shape (n_samples,).\n",
    "        max_leaves: Maximum number of leaves in the tree.\n",
    "        min_samples_leaf: Minimum number of samples required in a leaf.\n",
    "        lambda_: L2 regularisation parameter.\n",
    "        gamma: Minimum loss reduction required to make a split.\n",
    "        n_bins: Number of bins per feature.\n",
    "\n",
    "    Returns:\n",
    "        List of leaves representing the tree structure.\n",
    "    \"\"\"\n",
    "    m = X_binned.shape[0]\n",
    "    leaves = [\n",
    "        {\n",
    "            \"indices\": np.arange(m),\n",
    "            \"depth\": 0,\n",
    "            \"parent\": None,\n",
    "            \"gain\": 0.0,\n",
    "            \"value\": -gradients.sum() / (hessians.sum() + lambda_),\n",
    "        }\n",
    "    ]\n",
    "    for _ in range(max_leaves - 1):\n",
    "        best_gain = -np.inf\n",
    "        best_split = None\n",
    "        for leaf_idx, leaf in enumerate(leaves):\n",
    "            idxs = leaf[\"indices\"]\n",
    "            if len(idxs) <= min_samples_leaf or \"split\" in leaf:\n",
    "                continue\n",
    "            feature, bin_idx, gain = best_split_histogram(\n",
    "                X_binned[idxs],\n",
    "                gradients[idxs],\n",
    "                hessians[idxs],\n",
    "                min_samples_leaf,\n",
    "                lambda_,\n",
    "                gamma,\n",
    "                n_bins,\n",
    "            )\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_split = (leaf_idx, feature, bin_idx, gain)\n",
    "        if best_split is None or best_gain <= 0:\n",
    "            break\n",
    "        leaf_idx, feature, bin_idx, gain = best_split\n",
    "        idxs = leaves[leaf_idx][\"indices\"]\n",
    "        left_mask = X_binned[idxs, feature] <= bin_idx\n",
    "        right_mask = ~left_mask\n",
    "        left_indices = idxs[left_mask]\n",
    "        right_indices = idxs[right_mask]\n",
    "        leaves[leaf_idx][\"split\"] = (feature, bin_idx)\n",
    "        leaves[leaf_idx][\"left\"] = len(leaves)\n",
    "        leaves[leaf_idx][\"right\"] = len(leaves) + 1\n",
    "        leaves.append(\n",
    "            {\n",
    "                \"indices\": left_indices,\n",
    "                \"depth\": leaves[leaf_idx][\"depth\"] + 1,\n",
    "                \"parent\": leaf_idx,\n",
    "                \"gain\": gain,\n",
    "                \"value\": -gradients[left_indices].sum()\n",
    "                / (hessians[left_indices].sum() + lambda_),\n",
    "            }\n",
    "        )\n",
    "        leaves.append(\n",
    "            {\n",
    "                \"indices\": right_indices,\n",
    "                \"depth\": leaves[leaf_idx][\"depth\"] + 1,\n",
    "                \"parent\": leaf_idx,\n",
    "                \"gain\": gain,\n",
    "                \"value\": -gradients[right_indices].sum()\n",
    "                / (hessians[right_indices].sum() + lambda_),\n",
    "            }\n",
    "        )\n",
    "    return leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281b987",
   "metadata": {},
   "source": [
    "## 9. Predictions (Trees)\n",
    "The `predict_tree_batch()` function efficiently makes predictions for a batch of samples using a decision tree represented in a specific structure (as constructed by the `build_tree_leafwise()` function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af31332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree_batch(tree: list, X_binned: NDArray[np.int64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Predict outputs for a batch of samples using a decision tree.\n",
    "\n",
    "    Args:\n",
    "        tree: List of leaves representing the tree structure.\n",
    "        X_binned: Binned feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        Predicted values of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # For each sample, traverse the tree to find the leaf value\n",
    "    y_pred = np.zeros(X_binned.shape[0])\n",
    "    for i in range(X_binned.shape[0]):\n",
    "        node = 0\n",
    "        while \"split\" in tree[node]:\n",
    "            feature, bin_idx = tree[node][\"split\"]\n",
    "            if X_binned[i, feature] <= bin_idx:\n",
    "                node = tree[node][\"left\"]\n",
    "            else:\n",
    "                node = tree[node][\"right\"]\n",
    "        y_pred[i] = tree[node][\"value\"]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db92c65",
   "metadata": {},
   "source": [
    "## 10. Training Model\n",
    "During the training process of LightGBM:\n",
    "1. Initialise model predictions as $ F_0(x) = \\bar y $.\n",
    "1. Continuous features in $X$ are discretised into a fixed number of bins using histogram binning.\n",
    "1. Boosting loop from $ m=1 $ to $ M $ (number of boosting rounds `n_estimators`):\n",
    "    - Compute gradients and hessians.\n",
    "    - LightGBM splits the leaf with the highest potential gain at each step, rather than growing trees level-by-level as in XGBoost.\n",
    "    - Fit a tree $ h_m^{(x)} $ to the gradients and hessians, optimising the regularised objective function:\n",
    "\n",
    "      $$\n",
    "      h_m^{(x)} = \\text{Tree}(X, \\{g_i^{(m)}, h_i^{(m)}\\})\n",
    "      $$\n",
    "\n",
    "      For each candidate split (feature and bin), the gain is computed as:\n",
    "\n",
    "      $$\n",
    "      \\text{Gain} = \\frac{1}{2} \\left( \n",
    "          \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\n",
    "          \\right) - \\gamma\n",
    "      $$\n",
    "\n",
    "      where $G_L$, $H_L$ and $G_R$, $H_R$ are the sums of gradients and hessians for the left and right splits respectively.\n",
    "\n",
    "    - Update predictions.\n",
    "\n",
    "      The model is updated additively:\n",
    "\n",
    "      $$\n",
    "      F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\n",
    "      $$\n",
    "\n",
    "      where $\\eta$ is the learning rate.\n",
    "1. The function returns the initial prediction (mean of $y$), the list of fitted trees, the learning rate and the bin edges for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d92447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    X: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    n_estimators: int = 10,\n",
    "    learning_rate: float = 0.1,\n",
    "    max_leaves: int = 31,\n",
    "    min_samples_leaf: int = 20,\n",
    "    lambda_: float = 1.0,\n",
    "    gamma: float = 0.0,\n",
    "    n_bins: int = 255,\n",
    ") -> tuple[float, list[list], float, list[NDArray[np.float64]]]:\n",
    "    \"\"\"\n",
    "    Fit a LightGBM-style gradient boosting model for regression.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        y: Target values, shape (n_samples,).\n",
    "        n_estimators: Number of boosting rounds.\n",
    "        learning_rate: Learning rate.\n",
    "        max_leaves: Maximum number of leaves per tree.\n",
    "        min_samples_leaf: Minimum samples per leaf.\n",
    "        lambda_: L2 regularisation parameter.\n",
    "        gamma: Minimum gain required to split.\n",
    "        n_bins: Number of bins for feature discretisation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - initial_prediction: Initial prediction (mean of y).\n",
    "            - models: List of fitted trees.\n",
    "            - learning_rate: Learning rate used.\n",
    "            - bins: List of bin edges for each feature.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    X_binned, bins = bin_features(X, n_bins)\n",
    "    y_pred = initialise_model(y)\n",
    "    for m in range(n_estimators):\n",
    "        gradients, hessians = compute_gradients_and_hessians(y, y_pred)\n",
    "        tree = build_tree_leafwise(\n",
    "            X_binned,\n",
    "            gradients,\n",
    "            hessians,\n",
    "            max_leaves,\n",
    "            min_samples_leaf,\n",
    "            lambda_,\n",
    "            gamma,\n",
    "            n_bins,\n",
    "        )\n",
    "        update = predict_tree_batch(tree, X_binned)\n",
    "        y_pred += learning_rate * update\n",
    "        models.append(tree)\n",
    "    return np.mean(y), models, learning_rate, bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6aa88",
   "metadata": {},
   "source": [
    "## 11. Final Predictions\n",
    "The final predictions after $M$ trees is:\n",
    "\\begin{align*}\n",
    "    F_M(x) = F_0(x) + \\sum^{M}_{m=1} \\eta \\cdot h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial prediction (mean of $y$).\n",
    "- $h_m(x)$: Prediction of the $m$-th tree.\n",
    "- $\\eta$: Learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58259044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    X: NDArray[np.float64],\n",
    "    initial_prediction: float,\n",
    "    models: list[list],\n",
    "    learning_rate: float,\n",
    "    bins: list[NDArray[np.float64]],\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Predict outputs for a batch of samples using the fitted LightGBM model.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        initial_prediction: Initial prediction (mean of y from training).\n",
    "        models: List of fitted trees.\n",
    "        learning_rate: Learning rate.\n",
    "        bins: List of bin edges for each feature.\n",
    "\n",
    "    Returns:\n",
    "        Predicted values of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    X_binned = np.zeros_like(X, dtype=np.uint8)\n",
    "    for j in range(X.shape[1]):\n",
    "        X_binned[:, j] = np.digitize(X[:, j], bins[j]) - 1\n",
    "    y_pred = np.full(X.shape[0], initial_prediction, dtype=float)\n",
    "    for tree in models:\n",
    "        update = predict_tree_batch(tree, X_binned)\n",
    "        y_pred += learning_rate * update\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7d939d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "init_pred, models, lr, bins = fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_estimators=20,\n",
    "    learning_rate=0.2,\n",
    "    max_leaves=16,\n",
    "    min_samples_leaf=5,\n",
    "    lambda_=1.0,\n",
    "    gamma=0.0,\n",
    "    n_bins=64,\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = predict(X_test, init_pred, models, lr, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe73ce",
   "metadata": {},
   "source": [
    "## 12. Evaluation Metrics\n",
    "### Mean Squared Error (MSE)\n",
    "Mean Squared Error measures the average squared difference between predicted ($\\hat y$) and actual ($y$) values. Large errors are penalised heavily. Smaller MSE indicates better predictions.\n",
    "\n",
    "\\begin{align*}\n",
    "MSE = \\dfrac{1}{n} \\sum_{i=1}^{n}(\\hat y_{i} - y_{i})^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9c728ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584540da",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "Square root of MSE. It provides error in the same unit as the target variable ($y$) and easier to interpret.\n",
    "\n",
    "\\begin{align*}\n",
    "RMSE = \\sqrt{(MSE)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "714759c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff45ec5",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "Mean Absolute Error measures the average absolute difference between predicted ($\\hat y$) and actual ($y$) values. It is less sensitive to outliers than MSE. Smaller MAE indicates better predictions.\n",
    "\n",
    "\\begin{align*}\n",
    "MAE = \\dfrac{1}{n} \\sum_{i=1}^{n}|\\hat y_{i} = y_{i}|\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90674162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fc466",
   "metadata": {},
   "source": [
    "<a id=\"r-squared\"></a>\n",
    "### R-Squared($R^2$)\n",
    "\n",
    "R-squared indicated the proportion of variance in the dependent variable that is predictable from the independent variables. Value ranges from 0 to 1. Closer to 1 indicates a better fit.\n",
    "\n",
    "\n",
    "\n",
    "Residual Sum of Squares ($SS_{residual}$): \n",
    "\\begin{align*}\n",
    "SS_{residual} = \\sum_{i=1}^{n} (y_{i} - \\hat y_{i})^{2}\n",
    "\\end{align*}\n",
    "\n",
    "Total Sum of Squares ($SS_{total}$): \n",
    "\\begin{align*}\n",
    "SS_{total} = \\sum_{i=1}^{n} (y_{i} - \\bar y_{i})^{2}\n",
    "\\end{align*}\n",
    "\n",
    "$R^2$ is computed as:\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "R^2 = 1 - \\dfrac{SS_{residual}}{SS_{total}} = 1 - \\dfrac{\\sum_{i=1}^{n} (y_{i} - \\hat y_{i})^{2}}{\\sum_{i=1}^{n} (y_{i} - \\bar y_{i})^{2}}\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "$y$: Actual target values.\n",
    "\n",
    "$\\bar y$: Mean of the actual target values.\n",
    "\n",
    "$\\hat y$: Precicted target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad09c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    y_true: NDArray[np.float64], y_pred: NDArray[np.float64]\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate and return evaluation metrics for a regression model, including MSE, RMSE, MAE, and R-squared.\n",
    "\n",
    "     Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "    Returns:\n",
    "        - mse: Mean Squared Error (MSE), indicating the average of the squared differences between predicted and true values.\n",
    "        - rmse: Root Mean Squared Error (RMSE), indicating the standard deviation of the residuals.\n",
    "        - mae: Mean Absolute Error (MAE), representing the average absolute difference between predicted and true values.\n",
    "        - r2: R-squared (coefficient of determination), showing the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n",
    "    \"\"\"\n",
    "    mse = calculate_MSE(y_true, y_pred)\n",
    "    rmse = calculate_RMSE(y_true, y_pred)\n",
    "    mae = calculate_MAE(y_true, y_pred)\n",
    "    r2 = calculate_r2(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLightGBM:\n",
    "    \"\"\"\n",
    "    LightGBM-style Gradient Boosting Regressor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators: int = 10,\n",
    "        learning_rate: float = 0.1,\n",
    "        max_leaves: int = 31,\n",
    "        min_samples_leaf: int = 20,\n",
    "        lambda_: float = 1.0,\n",
    "        gamma: float = 0.0,\n",
    "        n_bins: int = 255,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise the CustomLightGBM regressor.\n",
    "\n",
    "        Args:\n",
    "            n_estimators: Number of boosting rounds.\n",
    "            learning_rate: Learning rate.\n",
    "            max_leaves: Maximum number of leaves per tree.\n",
    "            min_samples_leaf: Minimum samples per leaf.\n",
    "            lambda_: L2 regularisation parameter.\n",
    "            gamma: Minimum gain required to split.\n",
    "            n_bins: Number of bins for feature discretisation.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_leaves = max_leaves\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "        self.n_bins = n_bins\n",
    "        self.models: list[list] = []\n",
    "        self.bins: list[NDArray[np.float64]] = []\n",
    "        self.initial_prediction: float = 0.0\n",
    "\n",
    "    def _bin_features(\n",
    "        self, X: NDArray[np.float64]\n",
    "    ) -> tuple[NDArray[np.uint8], list[NDArray[np.float64]]]:\n",
    "        \"\"\"\n",
    "        Discretise continuous features into bins using histogram binning.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - X_binned: Binned feature matrix of same shape as X, with bin indices.\n",
    "                - bins: List of bin edges for each feature.\n",
    "        \"\"\"\n",
    "        bins = []\n",
    "        X_binned = np.zeros_like(X, dtype=np.uint8)\n",
    "        for j in range(X.shape[1]):\n",
    "            col = X[:, j]\n",
    "            bin_edges = np.linspace(col.min(), col.max(), self.n_bins + 1)\n",
    "            bins.append(bin_edges)\n",
    "            X_binned[:, j] = np.digitize(col, bin_edges) - 1\n",
    "        return X_binned, bins\n",
    "\n",
    "    def _best_split_histogram(\n",
    "        self,\n",
    "        X_binned: NDArray[np.uint8],\n",
    "        gradients: NDArray[np.float64],\n",
    "        hessians: NDArray[np.float64],\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Find the best split for a node using histogram-based split finding.\n",
    "\n",
    "        Args:\n",
    "            X_binned: Binned feature matrix of shape (n_samples, n_features).\n",
    "            gradients: Gradients for each sample, shape (n_samples,).\n",
    "            hessians: Hessians for each sample, shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "                - best_feature: Index of the best feature to split.\n",
    "                - best_bin: Bin index for the best split.\n",
    "                - best_gain: Gain value for the best split.\n",
    "        \"\"\"\n",
    "        m, n = X_binned.shape\n",
    "        best_feature, best_bin, best_gain = None, None, -np.inf\n",
    "        for feature in range(n):\n",
    "            grad_hist = np.zeros(self.n_bins)\n",
    "            hess_hist = np.zeros(self.n_bins)\n",
    "            for b in range(self.n_bins):\n",
    "                mask = X_binned[:, feature] == b\n",
    "                grad_hist[b] = gradients[mask].sum()\n",
    "                hess_hist[b] = hessians[mask].sum()\n",
    "            G_total, H_total = grad_hist.sum(), hess_hist.sum()\n",
    "            G_L, H_L = 0.0, 0.0\n",
    "            for b in range(self.n_bins - 1):\n",
    "                G_L += grad_hist[b]\n",
    "                H_L += hess_hist[b]\n",
    "                G_R = G_total - G_L\n",
    "                H_R = H_total - H_L\n",
    "                if H_L < self.min_samples_leaf or H_R < self.min_samples_leaf:\n",
    "                    continue\n",
    "                gain = (\n",
    "                    0.5\n",
    "                    * (\n",
    "                        G_L**2 / (H_L + self.lambda_)\n",
    "                        + G_R**2 / (H_R + self.lambda_)\n",
    "                        - (G_L + G_R) ** 2 / (H_L + H_R + self.lambda_)\n",
    "                    )\n",
    "                    - self.gamma\n",
    "                )\n",
    "                if gain > best_gain:\n",
    "                    best_feature, best_bin, best_gain = feature, b, gain\n",
    "        return best_feature, best_bin, best_gain\n",
    "\n",
    "    def _build_tree_leafwise(\n",
    "        self,\n",
    "        X_binned: NDArray[np.uint8],\n",
    "        gradients: NDArray[np.float64],\n",
    "        hessians: NDArray[np.float64],\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        Build a decision tree using the leaf-wise growth strategy.\n",
    "\n",
    "        Args:\n",
    "            X_binned: Binned feature matrix of shape (n_samples, n_features).\n",
    "            gradients: Gradients for each sample, shape (n_samples,).\n",
    "            hessians: Hessians for each sample, shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            List of leaves representing the tree structure.\n",
    "        \"\"\"\n",
    "        m = X_binned.shape[0]\n",
    "        leaves = [\n",
    "            {\n",
    "                \"indices\": np.arange(m),\n",
    "                \"depth\": 0,\n",
    "                \"parent\": None,\n",
    "                \"gain\": 0.0,\n",
    "                \"value\": -gradients.sum() / (hessians.sum() + self.lambda_),\n",
    "            }\n",
    "        ]\n",
    "        for _ in range(self.max_leaves - 1):\n",
    "            best_gain = -np.inf\n",
    "            best_split = None\n",
    "            for leaf_idx, leaf in enumerate(leaves):\n",
    "                idxs = leaf[\"indices\"]\n",
    "                if len(idxs) <= self.min_samples_leaf or \"split\" in leaf:\n",
    "                    continue\n",
    "                feature, bin_idx, gain = self._best_split_histogram(\n",
    "                    X_binned[idxs], gradients[idxs], hessians[idxs]\n",
    "                )\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = (leaf_idx, feature, bin_idx, gain)\n",
    "            if best_split is None or best_gain <= 0:\n",
    "                break\n",
    "            leaf_idx, feature, bin_idx, gain = best_split\n",
    "            idxs = leaves[leaf_idx][\"indices\"]\n",
    "            left_mask = X_binned[idxs, feature] <= bin_idx\n",
    "            right_mask = ~left_mask\n",
    "            left_indices = idxs[left_mask]\n",
    "            right_indices = idxs[right_mask]\n",
    "            leaves[leaf_idx][\"split\"] = (feature, bin_idx)\n",
    "            leaves[leaf_idx][\"left\"] = len(leaves)\n",
    "            leaves[leaf_idx][\"right\"] = len(leaves) + 1\n",
    "            leaves.append(\n",
    "                {\n",
    "                    \"indices\": left_indices,\n",
    "                    \"depth\": leaves[leaf_idx][\"depth\"] + 1,\n",
    "                    \"parent\": leaf_idx,\n",
    "                    \"gain\": gain,\n",
    "                    \"value\": -gradients[left_indices].sum()\n",
    "                    / (hessians[left_indices].sum() + self.lambda_),\n",
    "                }\n",
    "            )\n",
    "            leaves.append(\n",
    "                {\n",
    "                    \"indices\": right_indices,\n",
    "                    \"depth\": leaves[leaf_idx][\"depth\"] + 1,\n",
    "                    \"parent\": leaf_idx,\n",
    "                    \"gain\": gain,\n",
    "                    \"value\": -gradients[right_indices].sum()\n",
    "                    / (hessians[right_indices].sum() + self.lambda_),\n",
    "                }\n",
    "            )\n",
    "        return leaves\n",
    "\n",
    "    def _predict_tree_batch(\n",
    "        self, tree: list, X_binned: NDArray[np.uint8]\n",
    "    ) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Predict outputs for a batch of samples using a decision tree.\n",
    "\n",
    "        Args:\n",
    "            tree: List of leaves representing the tree structure.\n",
    "            X_binned: Binned feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted values of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X_binned.shape[0])\n",
    "        for i in range(X_binned.shape[0]):\n",
    "            node = 0\n",
    "            while \"split\" in tree[node]:\n",
    "                feature, bin_idx = tree[node][\"split\"]\n",
    "                if X_binned[i, feature] <= bin_idx:\n",
    "                    node = tree[node][\"left\"]\n",
    "                else:\n",
    "                    node = tree[node][\"right\"]\n",
    "            y_pred[i] = tree[node][\"value\"]\n",
    "        return y_pred\n",
    "\n",
    "    def _initialise_model(self, y: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"Initialises predictions with the mean of the target values.\n",
    "\n",
    "        Args:\n",
    "            y: Target values of shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            Array of initial predictions, each set to mean of y.\n",
    "        \"\"\"\n",
    "        return np.full_like(y, np.mean(y), dtype=float)\n",
    "\n",
    "    def _compute_gradients_and_hessians(\n",
    "        self, y_true: NDArray[np.float64], y_pred: NDArray[np.float64]\n",
    "    ) -> tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "        \"\"\"Computes gradients and Hessians for squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y_true: True target values, shape (n_samples,).\n",
    "            y_pred: Predicted values, shape (n_samples,).\n",
    "\n",
    "        Returns:\n",
    "            Tuple of gradients and Hessians, both of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        gradients = y_pred - y_true\n",
    "        hessians = np.ones_like(y_true)\n",
    "        return gradients, hessians\n",
    "\n",
    "    def fit(self, X: NDArray[np.float64], y: NDArray[np.float64]) -> None:\n",
    "        \"\"\"Fits the CustomLightGBM model to the training data.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "            y: Target values, shape (n_samples,).\n",
    "        \"\"\"\n",
    "        self.models = []\n",
    "        X_binned, self.bins = self._bin_features(X)\n",
    "        y_pred = self._initialise_model(y)\n",
    "        self.initial_prediction = float(np.mean(y))\n",
    "        for m in range(self.n_estimators):\n",
    "            gradients, hessians = self._compute_gradients_and_hessians(y, y_pred)\n",
    "            tree = self._build_tree_leafwise(X_binned, gradients, hessians)\n",
    "            update = self._predict_tree_batch(tree, X_binned)\n",
    "            y_pred += self.learning_rate * update\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"Predicts outputs for a batch of samples using the fitted model.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted values of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        X_binned = np.zeros_like(X, dtype=np.uint8)\n",
    "        for j in range(X.shape[1]):\n",
    "            X_binned[:, j] = np.digitize(X[:, j], self.bins[j]) - 1\n",
    "        y_pred = np.full(X.shape[0], self.initial_prediction, dtype=float)\n",
    "        for tree in self.models:\n",
    "            update = self._predict_tree_batch(tree, X_binned)\n",
    "            y_pred += self.learning_rate * update\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6aa193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Custom): 0.0049\n",
      "RMSE (Custom): 0.0699\n",
      "MAE (Custom): 0.0469\n",
      "R-Squared (Custom): 0.8106\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model_custom = CustomLightGBM(\n",
    "    n_estimators=100, learning_rate=0.1, max_leaves=20, lambda_=0.01\n",
    ")\n",
    "model_custom.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_custom = model_custom.predict(X_test)\n",
    "mse_custom, rmse_custom, mae_custom, r2_custom = evaluate(y_test, y_pred_custom)\n",
    "print(f\"MSE (Custom): {mse_custom:.4f}\")\n",
    "print(f\"RMSE (Custom): {rmse_custom:.4f}\")\n",
    "print(f\"MAE (Custom): {mae_custom:.4f}\")\n",
    "print(f\"R-Squared (Custom): {r2_custom:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1281ad",
   "metadata": {},
   "source": [
    "## 13. Comparison with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Custom): 0.0049\n",
      "MSE (lightgbm): 0.0045\n",
      "RMSE (Custom): 0.0699\n",
      "RMSE (lightgbm): 0.0669\n",
      "MAE (Custom): 0.0469\n",
      "MAE (lightgbm): 0.0452\n",
      "R-Squared (Custom): 0.8106\n",
      "R-Squared (lightgbm): 0.8264\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"num_leaves\": 31,\n",
    "    \"verbose\": -1,  # Suppress LightGBM internal logs\n",
    "}\n",
    "model_lgb = lgb.train(params, train_data, num_boost_round=100)\n",
    "y_pred_lgb = model_lgb.predict(X_test)\n",
    "\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "rmse_lgb = root_mean_squared_error(y_test, y_pred_lgb)\n",
    "mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred_lgb)\n",
    "print(f\"MSE (Custom): {mse_custom:.4f}\")\n",
    "print(f\"MAE (Custom): {mae_custom:.4f}\")\n",
    "print(f\"RMSE (Custom): {rmse_custom:.4f}\")\n",
    "print(f\"R-Squared (Custom): {r2_custom:.4f}\")\n",
    "print(\"----------\")\n",
    "print(f\"MSE (lightgbm): {mse_lgb:.4f}\")\n",
    "print(f\"MAE (lightgbm): {mae_lgb:.4f}\")\n",
    "print(f\"RMSE (lightgbm): {rmse_lgb:.4f}\")\n",
    "print(f\"R-Squared (lightgbm): {r2_lgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638972d",
   "metadata": {},
   "source": [
    "## 14. References\n",
    "\n",
    "1. Andreas Mueller. (2020). *Applied ML 2020 - 08 - Gradient Boosting.* <br>\n",
    "https://www.youtube.com/watch?v=yrTW5YTmFjw\n",
    "\n",
    "1. Bex Tuychiev. (2023). *A Guide to The Gradient Boosting Algorithm.* <br>\n",
    "https://www.datacamp.com/tutorial/guide-to-the-gradient-boosting-algorithm\n",
    "\n",
    "1. DataMListic. (2023). *Gradient Boosting with Regression Trees Explained* [YouTube Video]. <br>\n",
    "https://youtu.be/lOwsMpdjxog\n",
    "\n",
    "1. DMLC XGBOOST. (2022). *Introduction to Boosted Trees*. <br>\n",
    "https://xgboost.readthedocs.io/en/stable/tutorials/model.html\n",
    "\n",
    "1. GeeksforGeeks. (2025). *LightGBM (Light Gradient Boosting Machine)*. <br>\n",
    "https://www.geeksforgeeks.org/machine-learning/lightgbm-light-gradient-boosting-machine/\n",
    "\n",
    "1. GeeksforGeeks. (2025). *XGBoost*. <br>\n",
    "https://www.geeksforgeeks.org/machine-learning/xgboost/\n",
    "\n",
    "1. IBM. (2024). *What is XGBoost?* <br>\n",
    "https://www.ibm.com/think/topics/xgboost\n",
    "\n",
    "1. Jason Brownlee. (2021). *How to Develop Your First XGBoost Model in Python*.<br>\n",
    "https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "\n",
    "1. M Iqbal. (2025). *LightGBM Explained: Fast, Accurate, and Lightweight Machine Learning*. <br>\n",
    "https://youtu.be/DJYjUOtEHCE\n",
    "\n",
    "1. Nilimesh Halder. (2023). *Unpacking XGBoost: A Comprehensive Guide to Enhanced Gradient Boosting in Machine Learning*. <br>\n",
    "https://blog.gopenai.com/unpacking-xgboost-a-comprehensive-guide-to-enhanced-gradient-boosting-in-machine-learning-c145acec09fc\n",
    "\n",
    "1. nVIDIA. (n.d.). *What is XGBoost?* <br>\n",
    "https://www.nvidia.com/en-us/glossary/xgboost/\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *Gradient Boost Part 1 (of 4): Regression Main Ideas* [YouTube Video]. <br>\n",
    "https://youtu.be/3CC4N4z3GJc\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *Gradient Boost Part 2 (of 4): Regression Details* [YouTube Video]. <br>\n",
    "https://youtu.be/2xudPOBz-vs\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *XGBoost Part 1 (of 4): Regression* [YouTube Video]. <br>\n",
    "https://youtu.be/OtD8wVaFm6E\n",
    "\n",
    "1. Terence Parr and Jeremy Howard. (n.d.). *How to explain gradient boosting.* <br>\n",
    "https://explained.ai/gradient-boosting/index.html\n",
    "\n",
    "1. The IoT Academy. (2024). *What is the XGBoost Algorithm in ML  Explained With Steps*. <br>\n",
    "https://www.theiotacademy.co/blog/xgboost-algorithm/\n",
    "\n",
    "1. Tomonori Masui. (2022). *All You Need to Know about Gradient Boosting Algorithm  Part 1. Regression.* <br>\n",
    "https://medium.com/data-science/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502\n",
    "\n",
    "1. ultralytics. (n.d.). *LightGBM*. <br>\n",
    "https://www.ultralytics.com/glossary/lightgbm\n",
    "\n",
    "1. Unfold Data Science. (2022). *LightGBM algorithm explained | Lightgbm vs xgboost | lightGBM regression| LightGBM model*. <br>\n",
    "https://youtu.be/9uxWzeLglr0\n",
    "\n",
    "1. Wiens, M., Verone-Boyle, A., Henscheid, N., Podichetty, J. T., & Burton, J. (2025). A Tutorial and Use Case Example of the eXtreme Gradient Boosting (XGBoost) Artificial Intelligence Algorithm for Drug Development Applications. Clinical and translational science, 18(3), e70172. <br>\n",
    "https://doi.org/10.1111/cts.70172"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
