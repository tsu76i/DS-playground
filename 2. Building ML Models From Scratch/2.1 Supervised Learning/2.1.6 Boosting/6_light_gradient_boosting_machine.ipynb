{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf80cfa0",
   "metadata": {},
   "source": [
    "# Light Gradient Boosting Machine from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2650c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced3dbd",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "**Light Gradient Boosting Machine (LightGBM)** is a gradient boosting framework optimised for speed and scalability, making it highly suitable for regression and classification tasks involving large datasets. LightGBM uses **gradient-based one-sided sampling (GOSS)** and **exclusive feature bundling (EFB)** to speed up training and handle high-dimensional data efficiently. Traditional gradient boosting and XGBoost grow trees level-wise (depth-wise), whereas LightGBM grows trees leaf-wise. At each step, LightGBM splits the leaf with the largest loss reduction, which can lead to deeper, more complex trees and often higher accuracy, but also a higher risk of overfitting if not properly regularised.\n",
    "\n",
    "\n",
    "**Gradient Boosting Machine (GBM)** is an ensemble machine learning model that builds a strong predictive model by sequentially combining multiple weak models (typically decision trees) in a stage-wise manner. The core idea is to iteratively add new models that correct the errors made by the existing ensemble, thereby improving overall predictive accuracy.\n",
    "\n",
    "Suppose we have a dataset ${(x_i, y_i)}^n_{i=1}$ where $x_i$ are the features and $y_i$ are the target values. The goal of gradient boosting is to find a function $F(x)$ that minimises a given differentiable loss function $L(y, F(x))$:\n",
    "\n",
    "\\begin{align*}\n",
    "    F(x) = F_0(x) + \\sum^{M}_{m=1}\\gamma_m h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial model (e.g., the mean of $y$).\n",
    "- $\\gamma_i$: Weight (step size) for the $m$-th weak learner, typically determined by minimising the loss function along the direction of $h_m(x)$.\n",
    "- $M$: Number of boosting iterations (e.g., the number of weak learners).\n",
    "- $h_m$: prediction from the $m$-th weak learner (e.g., a decision tree).\n",
    "\n",
    "\n",
    "### Advantages\n",
    "- The histogram-based approach and leaf-wise growth strategy make LightGBM significantly faster and more memory-efficient, especially on large datasets.\n",
    "- The leaf-wise spliting often leads to better accuracy, as it focuses on reducing the largest errors at each iteration.\n",
    "- Efficient binning and feature bundling reduce memory usage.\n",
    "\n",
    "### Limitations\n",
    "- Overgitting risk due to the leaf-wise tree growth.\n",
    "- Sensitive to hypermarameters (e.g., learning rate, number of leaves, regularisation parameter).\n",
    "\n",
    "### Steps\n",
    "1. Initialise the model:\n",
    "    - Start with a simple model, typically a constant value:\n",
    "        - For regression: the mean of the target variable.\n",
    "        - For binary classification: the log-odds of the positive classes.\n",
    "1. Calculate residuals (Negative Gradients) and Hessian:\n",
    "    - For each iteration, compute the the negative gradients (and Hessian for second-order methods) of the loss function with respect to the current predictions.\n",
    "1. Fit a new weak model to predict the residuals:\n",
    "    - Train a weak learner (decision tree) to predict the rediduals.\n",
    "    - LightGBM grows the tree by splitting the leaf with the largest loss reduction (max delta loss), rather than level-wise as in traditional boosting.\n",
    "1. Update the model:\n",
    "    - Add the predictions from the new weak learners to the current model's predictions, scaled by a learning rate (shrinkage parameter).\n",
    "    - The update corrects the errors made by the current ensemble\n",
    "1. Repeat steps 2-4 for a pre-defined number of iterations.\n",
    "1. Final prediction:\n",
    "    - The sum of the initial prediction and the scaled outputs of all weak learners forms the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd277b4",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "Retrieved from [GitHub - YBI Foundation](https://github.com/YBI-Foundation/Dataset/blob/main/Admission%20Chance.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9880c5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Serial No",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "GRE Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TOEFL Score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "University Rating",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": " SOP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LOR ",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CGPA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Research",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Chance of Admit ",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "df5b88dd-35f7-4419-877f-b23d4ff9b38d",
       "rows": [
        [
         "0",
         "1",
         "337",
         "118",
         "4",
         "4.5",
         "4.5",
         "9.65",
         "1",
         "0.92"
        ],
        [
         "1",
         "2",
         "324",
         "107",
         "4",
         "4.0",
         "4.5",
         "8.87",
         "1",
         "0.76"
        ],
        [
         "2",
         "3",
         "316",
         "104",
         "3",
         "3.0",
         "3.5",
         "8.0",
         "1",
         "0.72"
        ],
        [
         "3",
         "4",
         "322",
         "110",
         "3",
         "3.5",
         "2.5",
         "8.67",
         "1",
         "0.8"
        ],
        [
         "4",
         "5",
         "314",
         "103",
         "2",
         "2.0",
         "3.0",
         "8.21",
         "0",
         "0.65"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No  GRE Score  TOEFL Score  University Rating   SOP  LOR   CGPA  \\\n",
       "0          1        337          118                  4   4.5   4.5  9.65   \n",
       "1          2        324          107                  4   4.0   4.5  8.87   \n",
       "2          3        316          104                  3   3.0   3.5  8.00   \n",
       "3          4        322          110                  3   3.5   2.5  8.67   \n",
       "4          5        314          103                  2   2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/YBI-Foundation/Dataset/refs/heads/main/Admission%20Chance.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64e12dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (400, 8)\n",
      "Target shape: (400,)\n",
      "Features: \n",
      "['Serial No', 'GRE Score', 'TOEFL Score', 'University Rating', ' SOP', 'LOR ', 'CGPA', 'Research']\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "feature_names = df.columns[:-1].tolist()  # All columns except the last one\n",
    "\n",
    "# Check the shape of the data\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')\n",
    "print(f'Features: \\n{feature_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d4f92",
   "metadata": {},
   "source": [
    "## 3. Loss Function\n",
    "In LightGBM, the loss function is used to compute the training loss, which is then combined with regularisation terms to construct the overall objective function that is minimised during training. The gradients and Hessians used to fit new trees are calculated with respect to the training loss, but the final optimisation process incorporates both the loss and the regularisation components.\n",
    "\n",
    "### Regression\n",
    "The most common loss function for regression is the mean squared error (MSE):\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y, \\hat y) = \\dfrac{1}{n}\\sum^{n}_{i=1}(y_i - \\hat y_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True value.\n",
    "- $\\hat y_i$: Predicted value.\n",
    "- $n$: Number of samples.\n",
    "\n",
    "### Binary Classification\n",
    "The standard loss function for binary classification in XGBoost is the binary cross-entropy (log loss):\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y, \\hat p) = - \\dfrac{1}{n} \\sum^{n}_{i=1}\\left[y_i \\log{(\\hat p_i) + (1-y_i) \\log{(1- \\hat p_i)}} \\right]\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True label ($0$ or $1$).\n",
    "- $\\hat p_i$: Predicted probability for class 1 (after applying the sigmoid function to the raw score).\n",
    "- $n$: Number of samples.\n",
    "\n",
    "### Multi-class Classification\n",
    "For multi-class classification (with $K$ classes), XGBoost uses the multi-class cross-entropy (also called softmax loss):\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y, \\hat P) = - \\dfrac{1}{n} \\sum^{n}_{i=1} \\sum^{K}_{k=1} \\mathbb{I}(y_i = k) \\log{(\\hat p_{ik})}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_i$: True class label for sample $i$.\n",
    "- $\\hat p_i$: Predicted probability that sample $i$ belongs to class $k$ (output of the softmax function).\n",
    "- $\\mathbb{I}(y_i = k)$: Indicator function, equal to $1$ if $y_i = k$ and $0$ otherwise.\n",
    "- $n$: Number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446c2d1",
   "metadata": {},
   "source": [
    "## 4. Initialising Model\n",
    "First, we need to initialise the model with a constant function that minimises the loss (initial predictions).\n",
    "\n",
    "\\begin{align*}\n",
    "    F_0(x) = \\arg \\min_{\\gamma}\\sum^{n}_{i=1}L(y_i, \\gamma)\n",
    "\\end{align*}\n",
    "\n",
    "For squared error, the best constant is the mean of the target values. Thus,\n",
    "\n",
    "\\begin{align*}\n",
    "    F_0(x) = \\bar y = \\dfrac{1}{n}\\sum^{n}_{i=1}y_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573c55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(y: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Initialise predictions with the mean of the target values.\n",
    "\n",
    "    Parameters:\n",
    "        y: Target values, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Array of initial predictions, each set to mean of y.\n",
    "    \"\"\"\n",
    "    return np.full_like(y, np.mean(y), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac49a1",
   "metadata": {},
   "source": [
    "## 5. Gradient and Hessian Computation\n",
    "We will need the first derivative (gradient) and the second derivative (Hessian) of the loss function for each sample $i$. They will be used later in the algorithm.\n",
    "\n",
    "- $g_i = \\dfrac{\\partial l(y_i, \\hat y_i)}{\\partial \\hat y_i} = \\dfrac{\\partial}{\\partial \\hat y_i} \\dfrac{1}{2}(y_i-\\hat y_i)^2 = \\hat y_i - y_i$\n",
    "- $h_i = \\dfrac{\\partial^2 l(y_i, \\hat y_i)}{\\partial \\hat y_i ^2} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4febe988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_and_hessians(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> Tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Compute gradients and Hessians for squared error loss.\n",
    "\n",
    "    Args:\n",
    "        y_true: True target values, shape (n_samples,).\n",
    "        y_pred: Predicted values, shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        A tuple with gradients and Hessians, both of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    gradients = y_pred - y_true\n",
    "    hessians = np.ones_like(y_true)\n",
    "    return gradients, hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cd21c",
   "metadata": {},
   "source": [
    "## 6. Histogram Binning\n",
    "The `bin_features()` function discretises continuous feature values in a dataset into a fixed number of bins using a process called **histogram binning**. This is a key step in histogram-based gradient boosting algorithms such as LightGBM, which use binned features to accelerate split finding and reduce memory usage.\n",
    "\n",
    "For each feature (column) $j$:\n",
    "- Extract the column: $\\text{col} = X \\left[:, j\\right]$\n",
    "\n",
    "- Compute bin edges:\n",
    "    - `np.linspace()` creates `n_bins` equally spaced intervals (bins) between the minimum and maximum values of the feature. Mathematically, for feature $j$, the bin edges are:\n",
    "    $$\n",
    "    b_0 = \\min{(x_{:, j})}, b_{\\text{n\\textunderscore bins}} = \\max{(x_{:, j})}\n",
    "    $$\n",
    "\n",
    "- Store bin edges:\n",
    "    - The bin edges for each feature are appended to the `bins` list for later use.\n",
    "\n",
    "- Digitise feature values:\n",
    "    - Each value in the feature column is assigned a bin index (from $0$ to $\\text{n\\textunderscore bins} - 1$), indicating which interval it falls into.\n",
    "\n",
    "Suppose we have a sample dataset with two continuous features and five samples:\n",
    "\n",
    "| Sample | Feature 1 | Feature 2 |\n",
    "|--------|-----------|-----------|\n",
    "|   1    |   2.1     |   8.5     |\n",
    "|   2    |   3.4     |   7.3     |\n",
    "|   3    |   1.8     |   6.9     |\n",
    "|   4    |   2.9     |   9.1     |\n",
    "|   5    |   3.0     |   7.8     |\n",
    "\n",
    "Assume we want to bin each feature into **3 bins**.\n",
    "\n",
    "**Step 1: Compute Bin Edges**\n",
    "For each feature, bin edges are calculated using equally spaced intervals between the minimum and maximum values.\n",
    "- Feature 1: $\\min = 1.8$, $\\max = 3.4$\n",
    "Bin edges: $\\left[1.8, 2.3333..., 2.866..., 3.4\\right]$\n",
    "\n",
    "- Feature 2: $\\min = 6.9$, $\\max = 9.1$\n",
    "Bin edges: $\\left[6.9, 7.633..., 8.366..., 9.1\\right]$\n",
    "\n",
    "| Feature   | Min   | Max   | Bin Edges                       |\n",
    "|-----------|-------|-------|---------------------------------|\n",
    "| Feature 1 | 1.8   | 3.4   | 1.8, 2.333..., 2.866..., 3.4    |\n",
    "| Feature 2 | 6.9   | 9.1   | 6.9, 7.633..., 8.366..., 9.1    |\n",
    "\n",
    "\n",
    "\n",
    "**Step 2: Assign Bin Indices**\n",
    "Each value is assigned a bin index ($0$, $1$, or $2$) based on which interval it falls into.\n",
    "\n",
    "| Sample | Feature 1 | Bin Index (F1) | Feature 2 | Bin Index (F2) |\n",
    "|--------|-----------|----------------|-----------|----------------|\n",
    "|   1    |   2.1     |      0         |   8.5     |      2         |\n",
    "|   2    |   3.4     |      2         |   7.3     |      0         |\n",
    "|   3    |   1.8     |      0         |   6.9     |      0         |\n",
    "|   4    |   2.9     |      1         |   9.1     |      2         |\n",
    "|   5    |   3.0     |      2         |   7.8     |      1         |\n",
    "\n",
    "\n",
    "**Step 3: Binned Feature Matrix**\n",
    "The resulting binned feature matrix (each value is the bin index):\n",
    "\n",
    "| Sample | Feature 1 (Binned) | Feature 2 (Binned) |\n",
    "|--------|--------------------|--------------------|\n",
    "|   1    |         0          |         2          |\n",
    "|   2    |         2          |         0          |\n",
    "|   3    |         0          |         0          |\n",
    "|   4    |         1          |         2          |\n",
    "|   5    |         2          |         1          |\n",
    "\n",
    "This transformation enables efficient histogram-based split finding as the algorithm only needs to consider splits at bin boundaries rather than every unique feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c83705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_features(X: np.ndarray, n_bins: int = 255) -> Tuple[np.ndarray, List[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Discretise continuous features into bins using histogram binning.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix, shape (n_samples, n_features).\n",
    "        n_bins: Number of bins to discretise each feature.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - X_binned: Binned feature matrix of same shape as X, with bin indices.\n",
    "            - bins: List of bin edges for each feature.\n",
    "    \"\"\"\n",
    "    bins = []\n",
    "    X_binned = np.zeros_like(X, dtype=np.uint8)\n",
    "    for j in range(X.shape[1]):\n",
    "        col = X[:, j]\n",
    "        bin_edges = np.linspace(col.min(), col.max(), n_bins + 1)\n",
    "        bins.append(bin_edges)\n",
    "        X_binned[:, j] = np.digitize(col, bin_edges) - 1\n",
    "    return X_binned, bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d63f1",
   "metadata": {},
   "source": [
    "## 7. Finding the Best Split\n",
    "In LightGBM, the process of finding the best split is mathematically similar to XGBoost, as both are based on gradient boosting with second-order Taylor expansion. However, LightGBM introduces several algorithmic innovations, particularly the use of histogram-based split finding and a leaf-wise tree growth strategy. Below is a detailed explanation, with a focus on the LightGBM approach.\n",
    "\n",
    "### Regularised Objective Function\n",
    "For a tree at boosting iteration $t$, the regularised objective function is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}^{(t)} = \\sum^{n}_{i=1}l(y_i, \\hat y_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $l$: Loss function (e.g., mean squared error).\n",
    "- $\\hat y_i^{(t-1)}$: Prediction from previous trees.\n",
    "- $f_t$: New tree.\n",
    "- $\\Omega(f_t)$: Regularisation term.\n",
    "\n",
    "### Second-Order Taylor Expansion\n",
    "LightGBM, like XGBoost, uses a second-order Taylor expansion of the loss function around the current prediction:\n",
    "\n",
    "\\begin{align*}\n",
    "    l(y_i, \\hat y_i^{(t-1)} + f_t(x_i)) \\approx l(y_i, \\hat y_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $g_i = \\dfrac{\\partial l(y_i, \\hat y_i)}{\\partial \\hat y_i}$\n",
    "- $h_i = \\dfrac{\\partial^2 l(y_i, \\hat y_i)}{\\partial \\hat y_i ^2}$\n",
    "\n",
    "Assume the new tree $f_t$ assigns a constant score $w_j$ to all samples in leaft $j$:\n",
    "\\begin{align*}\n",
    "    f_t(x_i) = w_{q(x_i)}\n",
    "\\end{align*}\n",
    "\n",
    "where $q(x_i)$ maps sample $i$ to its leaf.\n",
    "\n",
    "### Regularisation Term\n",
    "LightGBM typically uses only L2 regularisation for leaf weights in most practical scenarios:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\Omega(f_t) = \\gamma T + \\frac{1}{2} \\lambda \\sum^{T}_{j=1}w_j^2\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $T$: Number of leaves.\n",
    "- $\\gamma$: Penalty for the number of leaves (tree complexity).\n",
    "- $\\lambda$ L2 regularisation parameter.\n",
    "\n",
    "### Total Objective Function\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{\\tilde L}^{(t)} = \\sum^{T}_{j=1} \\left[\n",
    "        G_j w_j + \\frac{1}{2}(H_j+\\lambda)w_j^2\n",
    "        \\right] + \\gamma T\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $G_j = \\sum_{i \\in I_j} g_i$: Sum of gradients in leaf $j$.\n",
    "- $H_j = \\sum_{i \\in I_j} h_i$: Sum of Hessians in leaf $j$.\n",
    "\n",
    "### Optimising Leaf Weights\n",
    "\n",
    "To find the optimal leaf weight, minimise $\\mathcal{\\tilde L^{(t)}}$ with respect to $w_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial \\mathcal{\\tilde L^{(t)}}}{\\partial w_j} = G_j + (H_j + \\lambda)w_j = 0 \\rightarrow w_j^* = - \\dfrac{G_j}{H_j + \\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "Plug $w_j^*$ back into the objective:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{\\tilde L}^{(t)} = - \\dfrac{1}{2} \\sum^{T}_{j=1} \n",
    "        \\dfrac{G_j^2}{H_j + \\lambda}\n",
    "         + \\gamma T\n",
    "\\end{align*}\n",
    "\n",
    "Suppose a node is split into left ($L$) and right ($R$) children, the gain is the reduction in the objective:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{Gain} = \\dfrac{1}{2} \\left( \n",
    "        \\dfrac{G^2_L}{H_L + \\lambda} + \\dfrac{G^2_R}{H_R + \\lambda} - \\dfrac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\n",
    "        \\right) - \\gamma\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $G_L, H_L$: Sums for the left child.\n",
    "- $G_R, H_R$: Sums for the right child.\n",
    "\n",
    "### Histogram-based Split Finding\n",
    "\n",
    "- **Feature Binning**: LightGBM bins continuous feature values into discrete intervals (bins), greatly reducing the number of possible split points.\n",
    "\n",
    "- **Histogram Construction**: For each feature, LightGBM builds histograms of gradient and Hessian sums for each bin.\n",
    "\n",
    "- **Split Search**: The gain formula above is evaluated only at bin boundaries, making the process much faster and more memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split_histogram(X_binned: NDArray[np.int64], gradients: NDArray[np.float64], hessians: NDArray[np.float64],\n",
    "                         min_samples_leaf: int, lambda_: float, gamma: float, n_bins: int = 255) -> Tuple[Any, Any, float]:\n",
    "    \"\"\"\n",
    "    Find the best split for a node using histogram-based split finding.\n",
    "\n",
    "    Args:\n",
    "        X_binned: Binned feature matrix of shape (n_samples, n_features).\n",
    "        gradients: Gradients for each sample, shape (n_samples,).\n",
    "        hessians: Hessians for each sample, shape (n_samples,).\n",
    "        min_samples_leaf: Minimum number of samples required in a leaf.\n",
    "        lambda_: L2 regularisation parameter.\n",
    "        gamma: Minimum loss reduction required to make a split.\n",
    "        n_bins: Number of bins per feature.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - best_feature: Index of the best feature to split.\n",
    "            - best_bin: Bin index for the best split.\n",
    "            - best_gain: Gain value for the best split.\n",
    "    \"\"\"\n",
    "    m, n = X_binned.shape\n",
    "    best_feature, best_bin, best_gain = None, None, -np.inf\n",
    "    for feature in range(n):\n",
    "        grad_hist = np.zeros(n_bins)\n",
    "        hess_hist = np.zeros(n_bins)\n",
    "        for b in range(n_bins):\n",
    "            mask = X_binned[:, feature] == b\n",
    "            grad_hist[b] = gradients[mask].sum()\n",
    "            hess_hist[b] = hessians[mask].sum()\n",
    "        G_total, H_total = grad_hist.sum(), hess_hist.sum()\n",
    "        G_L, H_L = 0.0, 0.0\n",
    "        for b in range(n_bins - 1):\n",
    "            G_L += grad_hist[b]\n",
    "            H_L += hess_hist[b]\n",
    "            G_R = G_total - G_L\n",
    "            H_R = H_total - H_L\n",
    "            if H_L < min_samples_leaf or H_R < min_samples_leaf:\n",
    "                continue\n",
    "            gain = 0.5 * (G_L**2 / (H_L + lambda_) + G_R**2 / (H_R + lambda_) -\n",
    "                          (G_L + G_R)**2 / (H_L + H_R + lambda_)) - gamma\n",
    "            if gain > best_gain:\n",
    "                best_feature, best_bin, best_gain = feature, b, gain\n",
    "    return best_feature, best_bin, best_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e35d56",
   "metadata": {},
   "source": [
    "## 8. Building Trees\n",
    "The following `build_tree_leafwise()` function implements the **leaf-wise tree growth strategy** for constructing a single decision tree in a gradient boosting ensemble. With this strategy, the algorithm splits the leaf with the highest gain, leading to potentially deeper and more complex trees compared to level-wise (depth-wise) growth. The formula for the optimal leaf value is:\n",
    "\n",
    "\\begin{align*}\n",
    "    w^* = - \\dfrac{\\sum g_i}{\\sum h_i + \\lambda}\n",
    "\\end{align*}\n",
    "\n",
    "where $g_i$ and $h_i$ are the gradients and Hessians, and $\\lambda$ is the L2 regularisation parameter.\n",
    "\n",
    "The function relies on binned features and pre-computed histograms for efficiency, only evaluating splits at bin boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_leafwise(X_binned: NDArray[np.uint8], gradients: NDArray[np.float64], hessians: NDArray[np.float64],\n",
    "                        max_leaves: int, min_samples_leaf: int, lambda_: float, gamma: float, n_bins: int = 255) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Build a decision tree using the leaf-wise growth strategy.\n",
    "\n",
    "    Args:\n",
    "        X_binned: Binned feature matrix of shape (n_samples, n_features).\n",
    "        gradients: Gradients for each sample, shape (n_samples,).\n",
    "        hessians: Hessians for each sample, shape (n_samples,).\n",
    "        max_leaves: Maximum number of leaves in the tree.\n",
    "        min_samples_leaf: Minimum number of samples required in a leaf.\n",
    "        lambda_: L2 regularisation parameter.\n",
    "        gamma: Minimum loss reduction required to make a split.\n",
    "        n_bins: Number of bins per feature.\n",
    "\n",
    "    Returns:\n",
    "        List of leaves representing the tree structure.\n",
    "    \"\"\"\n",
    "    m = X_binned.shape[0]\n",
    "    leaves = [{\n",
    "        'indices': np.arange(m),\n",
    "        'depth': 0,\n",
    "        'parent': None,\n",
    "        'gain': 0.0,\n",
    "        'value': -gradients.sum() / (hessians.sum() + lambda_)\n",
    "    }]\n",
    "    split_info = []\n",
    "    for _ in range(max_leaves - 1):\n",
    "        best_gain = -np.inf\n",
    "        best_split = None\n",
    "        for leaf_idx, leaf in enumerate(leaves):\n",
    "            idxs = leaf['indices']\n",
    "            if len(idxs) <= min_samples_leaf or 'split' in leaf:\n",
    "                continue\n",
    "            feature, bin_idx, gain = best_split_histogram(\n",
    "                X_binned[idxs], gradients[idxs], hessians[idxs],\n",
    "                min_samples_leaf, lambda_, gamma, n_bins\n",
    "            )\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_split = (leaf_idx, feature, bin_idx, gain)\n",
    "        if best_split is None or best_gain <= 0:\n",
    "            break\n",
    "        leaf_idx, feature, bin_idx, gain = best_split\n",
    "        idxs = leaves[leaf_idx]['indices']\n",
    "        left_mask = X_binned[idxs, feature] <= bin_idx\n",
    "        right_mask = ~left_mask\n",
    "        left_indices = idxs[left_mask]\n",
    "        right_indices = idxs[right_mask]\n",
    "        leaves[leaf_idx]['split'] = (feature, bin_idx)\n",
    "        leaves[leaf_idx]['left'] = len(leaves)\n",
    "        leaves[leaf_idx]['right'] = len(leaves) + 1\n",
    "        leaves.append({\n",
    "            'indices': left_indices,\n",
    "            'depth': leaves[leaf_idx]['depth'] + 1,\n",
    "            'parent': leaf_idx,\n",
    "            'gain': gain,\n",
    "            'value': -gradients[left_indices].sum() / (hessians[left_indices].sum() + lambda_)\n",
    "        })\n",
    "        leaves.append({\n",
    "            'indices': right_indices,\n",
    "            'depth': leaves[leaf_idx]['depth'] + 1,\n",
    "            'parent': leaf_idx,\n",
    "            'gain': gain,\n",
    "            'value': -gradients[right_indices].sum() / (hessians[right_indices].sum() + lambda_)\n",
    "        })\n",
    "    return leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281b987",
   "metadata": {},
   "source": [
    "## 9. Predictions (Trees)\n",
    "The `predict_tree_batch()` function efficiently makes predictions for a batch of samples using a decision tree represented in a specific structure (as constructed by the `build_tree_leafwise()` function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af31332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree_batch(tree: List[Any], X_binned: NDArray[np.int64]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Predict outputs for a batch of samples using a decision tree.\n",
    "\n",
    "    Args:\n",
    "        tree: List of leaves representing the tree structure.\n",
    "        X_binned: Binned feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "    Returns:\n",
    "        Predicted values of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    # For each sample, traverse the tree to find the leaf value\n",
    "    y_pred = np.zeros(X_binned.shape[0])\n",
    "    for i in range(X_binned.shape[0]):\n",
    "        node = 0\n",
    "        while 'split' in tree[node]:\n",
    "            feature, bin_idx = tree[node]['split']\n",
    "            if X_binned[i, feature] <= bin_idx:\n",
    "                node = tree[node]['left']\n",
    "            else:\n",
    "                node = tree[node]['right']\n",
    "        y_pred[i] = tree[node]['value']\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db92c65",
   "metadata": {},
   "source": [
    "## 10. Training Model\n",
    "During the training process of LightGBM:\n",
    "1. Initialise model predictions as $ F_0(x) = \\bar y $.\n",
    "1. Continuous features in $X$ are discretised into a fixed number of bins using histogram binning.\n",
    "1. Boosting loop from $ m=1 $ to $ M $ (number of boosting rounds `n_estimators`):\n",
    "    - Compute gradients and hessians.\n",
    "    - LightGBM splits the leaf with the highest potential gain at each step, rather than growing trees level-by-level as in XGBoost.\n",
    "    - Fit a tree $ h_m^{(x)} $ to the gradients and hessians, optimising the regularised objective function:\n",
    "\n",
    "      $$\n",
    "      h_m^{(x)} = \\text{Tree}(X, \\{g_i^{(m)}, h_i^{(m)}\\})\n",
    "      $$\n",
    "\n",
    "      For each candidate split (feature and bin), the gain is computed as:\n",
    "\n",
    "      $$\n",
    "      \\text{Gain} = \\frac{1}{2} \\left( \n",
    "          \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\n",
    "          \\right) - \\gamma\n",
    "      $$\n",
    "\n",
    "      where $G_L$, $H_L$ and $G_R$, $H_R$ are the sums of gradients and hessians for the left and right splits respectively.\n",
    "\n",
    "    - Update predictions.\n",
    "\n",
    "      The model is updated additively:\n",
    "\n",
    "      $$\n",
    "      F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\n",
    "      $$\n",
    "\n",
    "      where $\\eta$ is the learning rate.\n",
    "1. The function returns the initial prediction (mean of $y$), the list of fitted trees, the learning rate and the bin edges for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d92447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X: NDArray[np.float64], y: NDArray[np.float64], n_estimators: int = 10,\n",
    "        learning_rate: float = 0.1, max_leaves: int = 31, min_samples_leaf: int = 20, lambda_: float = 1.0,\n",
    "        gamma: float = 0.0, n_bins: int = 255) -> Tuple[float, List[List[Any]], float, List[NDArray[np.float64]]]:\n",
    "    \"\"\"\n",
    "    Fit a LightGBM-style gradient boosting model for regression.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        y: Target values, shape (n_samples,).\n",
    "        n_estimators: Number of boosting rounds.\n",
    "        learning_rate: Learning rate.\n",
    "        max_leaves: Maximum number of leaves per tree.\n",
    "        min_samples_leaf: Minimum samples per leaf.\n",
    "        lambda_: L2 regularisation parameter.\n",
    "        gamma: Minimum gain required to split.\n",
    "        n_bins: Number of bins for feature discretisation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - initial_prediction: Initial prediction (mean of y).\n",
    "            - models: List of fitted trees.\n",
    "            - learning_rate: Learning rate used.\n",
    "            - bins: List of bin edges for each feature.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    X_binned, bins = bin_features(X, n_bins)\n",
    "    y_pred = initialise_model(y)\n",
    "    for m in range(n_estimators):\n",
    "        gradients, hessians = compute_gradients_and_hessians(y, y_pred)\n",
    "        tree = build_tree_leafwise(\n",
    "            X_binned, gradients, hessians, max_leaves, min_samples_leaf, lambda_, gamma, n_bins)\n",
    "        update = predict_tree_batch(tree, X_binned)\n",
    "        y_pred += learning_rate * update\n",
    "        models.append(tree)\n",
    "    return np.mean(y), models, learning_rate, bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6aa88",
   "metadata": {},
   "source": [
    "## 11. Final Predictions\n",
    "The final predictions after $M$ trees is:\n",
    "\\begin{align*}\n",
    "    F_M(x) = F_0(x) + \\sum^{M}_{m=1} \\eta \\cdot h_m(x)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $F_0(x)$: Initial prediction (mean of $y$).\n",
    "- $h_m(x)$: Prediction of the $m$-th tree.\n",
    "- $\\eta$: Learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58259044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: NDArray[np.float64], initial_prediction: float, models: List[List[Any]],\n",
    "            learning_rate: float, bins: List[NDArray[np.float64]]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Predict outputs for a batch of samples using the fitted LightGBM model.\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        initial_prediction: Initial prediction (mean of y from training).\n",
    "        models: List of fitted trees.\n",
    "        learning_rate: Learning rate.\n",
    "        bins: List of bin edges for each feature.\n",
    "\n",
    "    Returns:\n",
    "        Predicted values of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    X_binned = np.zeros_like(X, dtype=np.uint8)\n",
    "    for j in range(X.shape[1]):\n",
    "        X_binned[:, j] = np.digitize(X[:, j], bins[j]) - 1\n",
    "    y_pred = np.full(X.shape[0], initial_prediction, dtype=float)\n",
    "    for tree in models:\n",
    "        update = predict_tree_batch(tree, X_binned)\n",
    "        y_pred += learning_rate * update\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d939d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2: 0.8419\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "init_pred, models, lr, bins = fit(\n",
    "    X_train, y_train,\n",
    "    n_estimators=20,\n",
    "    learning_rate=0.2,\n",
    "    max_leaves=16,\n",
    "    min_samples_leaf=5,\n",
    "    lambda_=1.0,\n",
    "    gamma=0.0,\n",
    "    n_bins=64\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred = predict(X_test, init_pred, models, lr, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe73ce",
   "metadata": {},
   "source": [
    "## 12. Evaluation Metrics\n",
    "### Mean Squared Error (MSE)\n",
    "Mean Squared Error measures the average squared difference between predicted ($\\hat y$) and actual ($y$) values. Large errors are penalised heavily. Smaller MSE indicates better predictions.\n",
    "\n",
    "\\begin{align*}\n",
    "MSE = \\dfrac{1}{n} \\sum_{i=1}^{n}(\\hat y_{i} = y_{i})^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c728ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MSE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584540da",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "Square root of MSE. It provides error in the same unit as the target variable ($y$) and easier to interpret.\n",
    "\n",
    "\\begin{align*}\n",
    "RMSE = \\sqrt{(MSE)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714759c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_RMSE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff45ec5",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "Mean Absolute Error measures the average absolute difference between predicted ($\\hat y$) and actual ($y$) values. It is less sensitive to outliers than MSE. Smaller MAE indicates better predictions.\n",
    "\n",
    "\\begin{align*}\n",
    "MAE = \\dfrac{1}{n} \\sum_{i=1}^{n}|\\hat y_{i} = y_{i}|\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90674162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_MAE(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    return np.mean(np.abs(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94fc466",
   "metadata": {},
   "source": [
    "<a id=\"r-squared\"></a>\n",
    "### R-Squared($R^2$)\n",
    "\n",
    "R-squared indicated the proportion of variance in the dependent variable that is predictable from the independent variables. Value ranges from 0 to 1. Closer to 1 indicates a better fit.\n",
    "\n",
    "\n",
    "\n",
    "Residual Sum of Squares ($SS_{residual}$): \n",
    "\\begin{align*}\n",
    "SS_{residual} = \\sum_{i=1}^{n} (y_{i} - \\hat y_{i})^{2}\n",
    "\\end{align*}\n",
    "\n",
    "Total Sum of Squares ($SS_{total}$): \n",
    "\\begin{align*}\n",
    "SS_{total} = \\sum_{i=1}^{n} (y_{i} - \\bar y_{i})^{2}\n",
    "\\end{align*}\n",
    "\n",
    "$R^2$ is computed as:\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "R^2 = 1 - \\dfrac{SS_{residual}}{SS_{total}} = 1 - \\dfrac{\\sum_{i=1}^{n} (y_{i} - \\hat y_{i})^{2}}{\\sum_{i=1}^{n} (y_{i} - \\bar y_{i})^{2}}\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "$y$: Actual target values.\n",
    "\n",
    "$\\bar y$: Mean of the actual target values.\n",
    "\n",
    "$\\hat y$: Precicted target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> float:\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true: NDArray[np.float64], y_pred: NDArray[np.float64]) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate and return evaluation metrics for a regression model, including MSE, RMSE, MAE, and R-squared.\n",
    "\n",
    "     Args:\n",
    "        y_true: True labels.\n",
    "        y_pred: Predicted labels.\n",
    "        class_names: List of class names. Defaults to None.\n",
    "    Returns:\n",
    "        - mse: Mean Squared Error (MSE), indicating the average of the squared differences between predicted and true values.\n",
    "        - rmse: Root Mean Squared Error (RMSE), indicating the standard deviation of the residuals.\n",
    "        - mae: Mean Absolute Error (MAE), representing the average absolute difference between predicted and true values.\n",
    "        - r2: R-squared (coefficient of determination), showing the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n",
    "    \"\"\"\n",
    "    mse = calculate_MSE(y_true, y_pred)\n",
    "    rmse = calculate_RMSE(y_true, y_pred)\n",
    "    mae = calculate_MAE(y_true, y_pred)\n",
    "    r2 = calculate_r2(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
