{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66f2663",
   "metadata": {},
   "source": [
    "# Adaptive Boosting from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "    - [Advantages](#advantages)\n",
    "    - [Limitations](#limitations)\n",
    "    - [Steps](#steps)\n",
    "1. [Loading Data](#2-loading-data)\n",
    "1. [Initialising Weights](#3-initialising-weights)\n",
    "1. [Finding the Best Stump](#4-finding-the-best-stump)\n",
    "1. [Learner Weights](#5-learner-weights)\n",
    "1. [Updating Sample Weights](#6-updating-sample-weights)\n",
    "1. [Training Loop](#7-training-loop)\n",
    "1. [Prediction](#8-prediction)\n",
    "1. [Encapsulation](#9-encapsulation)\n",
    "1. [Comparison with Scikit-Learn](#10-comparison-with-scikit-learn)\n",
    "1. [References](#11-references)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1587216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.typing import NDArray\n",
    "from typing import Tuple, Dict, Any, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29553c26",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Adaptive Boosting (AdaBoost) is a foundational ensemble learning algorithm designed to improve the accuracy of machine learning models by combining multiple **weak classifiers** (often decision stumps - decision trees with a single split) into a single **strong classifier**. Althought AdaBoost is primarily for binary classification, it has been extended to handle multiclass problems and regression tasks in some variants. However, its core mechanism and main use case remain in binary classification.\n",
    "\n",
    "### Advantages\n",
    "- Turn weak models into a strong classifier.\n",
    "- Less overfitting.\n",
    "- No need for parameter tuning.\n",
    "\n",
    "### Limitations\n",
    "- Sensitive to outliers as misclassified samples get higher weights.\n",
    "- Primarily for binary classification.\n",
    "\n",
    "### Steps\n",
    "1. Initialise weights.\n",
    "2. For each boosting round (M iterations),\n",
    "    - Train a weak lerner (decision stump).\n",
    "    - Compute weighted error.\n",
    "    - Calculate lerner weights $\\alpha$.\n",
    "    - Update sample weights.\n",
    "    - Repeat for the maximum number of iterations or until weighted error is sufficiently low.\n",
    "3. Predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1022b",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74e4048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "y = np.where(y == 0, -1, 1)     # AdaBoost expects labels as -1 and +1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394850d",
   "metadata": {},
   "source": [
    "## 3. Initialising Weights\n",
    "All training samples are initialised with equal weight:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_i = \\dfrac{1}{N}\n",
    "\\end{align*}\n",
    "\n",
    "where $N$ is the number of samples. For $N = 5$, the initial weights of the sample will be:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_i = \\dfrac{1}{5} = 0.2\n",
    "\\end{align*}\n",
    "\n",
    "The `np.full` function from NumPy library can generate an array of the specified length with every entry set to the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0075d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_weights(n_samples: int) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Initialise sample weights equally.\n",
    "\n",
    "    Parameters:\n",
    "        n_samples: Number of samples.\n",
    "\n",
    "    Returns:\n",
    "        Initialised sample weights of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    return np.full(n_samples, 1 / n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3676a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For N = 5: [0.2 0.2 0.2 0.2 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(f'For N = 5: {initialise_weights(5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f604e6",
   "metadata": {},
   "source": [
    "## 4. Finding the Best Stump\n",
    "<!-- The following `find_best_stump` function searches all features and possible thresholds, and for each, tries both polarities (direction of the inequality). It predicts labels, computes the weighted error, and keeps the stump with the lowest error. -->\n",
    "The following `find_best_stump` function implements the decision stump: It exhaustively searches for the best one-level split across all features and possible thresholds consdering both directions (polarities), and selects the split that minimises the weighted classification error.\n",
    "\n",
    "1. Initialise variables.\n",
    "2. Loop over all features and thresholds (unique values).\n",
    "3. Loop over both polarities: $[1, -1]$.\n",
    "4. Make predictions.\n",
    "    - Initialise all predictions to $+1$.\n",
    "    - For polarity $1$: set to $-1$ if $\\text{value} < \\text{threshold}$.\n",
    "    - Otherwise: set to $+1$.\n",
    "5. Calculate weighted error.\n",
    "\\begin{align*}\n",
    "    \\epsilon_m = \\dfrac{\\sum^{N}_{i=1} w_i \\cdot \\mathbb{I}(h_m(x_i) \\neq y_i)}{\\sum^{N}_{i=1}w_i}\n",
    "\\end{align*}\n",
    "\n",
    "    where:\n",
    "    - $h_m$: $m$-th weak learner.\n",
    "    - $y_i$: True label.\n",
    "    - $\\mathbb{I}$: Indicator function.\n",
    "\n",
    "    In fact, weighted error is just a sum of weights for misclassified samples.\n",
    "6. If the error rate is smaller than `min_error`, update the value (`min_error = error`), best stump and best prediction.\n",
    "7. Return `best_stump`, `min_error`, and `best_predictions` with the least error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8b9fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_stump(X: NDArray[np.float64], y: NDArray[np.int8],\n",
    "                    sample_weights: NDArray[np.float64]) -> Tuple[Dict[str, Any], float, NDArray[np.int8]]:\n",
    "    \"\"\"\n",
    "    Find the best decision stump that minimises weighted classification error.\n",
    "\n",
    "    Parameters:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        y: Labels array of shape (n_samples,), with values -1 or 1.\n",
    "        sample_weights: Sample weights of shape (n_samples,).\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - best_stump: Dictionary with keys 'feature_index', 'threshold', and 'polarity'.\n",
    "            - min_error: Minimum weighted classification error.\n",
    "            - best_predictions: Predictions of the best stump on X.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    min_error = float('inf')\n",
    "    best_stump = {}\n",
    "    best_predictions = None\n",
    "\n",
    "    for feature_i in range(n_features):  # Each feature\n",
    "        feature_vals = X[:, feature_i]  # All values in the selected features\n",
    "        thresholds = np.unique(feature_vals)  # Unique values in feature_vals\n",
    "        for threshold in thresholds:\n",
    "            for polarity in [1, -1]:\n",
    "                # Predict: 1 if (polarity * feature) < (polarity * threshold), else -1\n",
    "                predictions = np.ones(n_samples)\n",
    "                if polarity == 1:\n",
    "                    predictions[feature_vals < threshold] = -1\n",
    "                else:\n",
    "                    predictions[feature_vals > threshold] = -1\n",
    "\n",
    "                # Calculate weighted error\n",
    "                misclassified = predictions != y\n",
    "                error = np.sum(sample_weights[misclassified])\n",
    "\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    best_stump = {\n",
    "                        \"feature_index\": feature_i,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"polarity\": polarity\n",
    "                    }\n",
    "                    best_predictions = predictions.copy()\n",
    "    return best_stump, min_error, best_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28b618",
   "metadata": {},
   "source": [
    "## 5. Learner Weights\n",
    "For the current learner $m$, the learner weight $\\alpha_m$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\alpha_m = \\dfrac{1}{2} \\text{ln} \\left( \\dfrac{1-\\epsilon_m + \\text{c}}{\\epsilon_m + \\text{c}} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $\\epsilon_m$: Error rate calculated inside the `find_best_stump()` function.\n",
    "- $c$: Small constant added to avoid division by zero. Set to $1 \\times 10^{-10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c5922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(error: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute the weight of the weak learner (alpha).\n",
    "\n",
    "    Parameters:\n",
    "        error: Weighted classification error of the weak learner.\n",
    "\n",
    "    Returns:\n",
    "        Weight of the weak learner.\n",
    "    \"\"\"\n",
    "    c = 1e-10  # constant\n",
    "    return 0.5 * np.log((1 - error + c) / (error + c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081afa49",
   "metadata": {},
   "source": [
    "## 6. Updating Sample Weights\n",
    "After calculating the learner weight $\\alpha_m$, we update the old weight $w_m$ such that:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_i \\leftarrow w_i \\cdot \\text{e}^{-\\alpha_m y_i h_m(x_i)}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "$w_i$: Current weight of sample $i$.\n",
    "$\\alpha_m$: Weight of the weak learner $m$. \n",
    "$h_m(x_i)$: Prediction for sample $i$ ($-1$ or $+1$).\n",
    "\n",
    "The weights are increased for misclassified samples, and are decreased for correctly classified ones:\n",
    "- If the prediction is **correct** $(y_i = h_m(x_i))$, then $y_i \\cdot h_m(x_i) = 1$, so the weight is **decreased**:\n",
    "\\begin{align*}\n",
    "    w_i \\leftarrow w_i \\cdot \\text{e}^{-\\alpha}\n",
    "\\end{align*}\n",
    "\n",
    "- If the prediction is **incorrect** $(y_i \\neq h_m(x_i))$, then $y_i \\cdot h_m(x_i) = -1$, so the weight is **increased**:\n",
    "\\begin{align*}\n",
    "    w_i \\leftarrow w_i \\cdot \\text{e}^{\\alpha}\n",
    "\\end{align*}\n",
    "\n",
    "The function returns the normalised weights (all sample weights sum to 1) for the next AdaBoost iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4ca197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(sample_weights: NDArray[np.float64], alpha: float,\n",
    "                   y: NDArray[np.int8], predictions: NDArray[np.int8]) -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Update sample weights: increase for misclassified, decrease for correct.\n",
    "\n",
    "    Parameters:\n",
    "        sample_weights: Current sample weights.\n",
    "        alpha: Weight of the weak learner.\n",
    "        y: True labels.\n",
    "        predictions: Predictions from the weak learner.\n",
    "\n",
    "    Returns:\n",
    "        Updated and normalised sample weights.\n",
    "    \"\"\"\n",
    "    sample_weights *= np.exp(-alpha * y * predictions)\n",
    "    sample_weights /= np.sum(sample_weights)  # Normalisation\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1785e13",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "The training loop runs for the specified number of weak learners `n_weak_learners`. After all iterations, it returns a list of all trained stumps `stumps` with their parameters, and a list of the corresponding weights for each stump `alphas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e87126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_train(X: NDArray[np.float64], y: NDArray[np.int8],\n",
    "                   n_weak_learners: int) -> Tuple[List[Dict[str, Any]], List[float]]:\n",
    "    \"\"\"\n",
    "    Train AdaBoost ensemble with decision stumps.\n",
    "\n",
    "    Parameters:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        y: Labels array of shape (n_samples,), with values -1 or 1.\n",
    "        n_weak_learners: Number of weak learners to train.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - stumps: List of decision stump dictionaries.\n",
    "            - alphas: List of weak learner weights.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    sample_weights = initialise_weights(n_samples)\n",
    "    stumps = []\n",
    "    alphas = []\n",
    "\n",
    "    for _ in range(n_weak_learners):\n",
    "        stump, error, predictions = find_best_stump(X, y, sample_weights)\n",
    "        alpha = compute_alpha(error)\n",
    "        sample_weights = update_weights(sample_weights, alpha, y, predictions)\n",
    "        stumps.append(stump)\n",
    "        alphas.append(alpha)\n",
    "    return stumps, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af173c1e",
   "metadata": {},
   "source": [
    "## 8. Prediction\n",
    "The following function makes predictions on input data $X$ using a single decision stump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c07b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stump_predict(X: NDArray[np.float64], stump: Dict[str, Any]) -> NDArray[np.int8]:\n",
    "    \"\"\"\n",
    "    Predict labels for X using a given decision stump.\n",
    "\n",
    "    Parameters:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        stump: Decision stump parameters.\n",
    "\n",
    "    Returns:\n",
    "        Predicted labels (-1 or 1) of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    feature_values = X[:, stump[\"feature_index\"]]\n",
    "    predictions = np.ones(X.shape[0])\n",
    "    if stump[\"polarity\"] == 1:\n",
    "        predictions[feature_values < stump[\"threshold\"]] = -1\n",
    "    else:\n",
    "        predictions[feature_values > stump[\"threshold\"]] = -1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99726e68",
   "metadata": {},
   "source": [
    "Then the `predict()` function combines the predictions from all decision stumps in the AdaBoost ensemble using their respective weights $\\alpha$ to produce the final prediction for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3671e29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: NDArray[np.float64], stumps: List[Dict[str, Any]],\n",
    "            alphas: List[float]) -> NDArray[np.int8]:\n",
    "    \"\"\"\n",
    "    Aggregate predictions from all stumps using their alphas.\n",
    "\n",
    "    Parameters:\n",
    "        X: Feature matrix of shape (n_samples, n_features).\n",
    "        stumps: List of decision stump dictionaries.\n",
    "        alphas: List of weak learner weights.\n",
    "\n",
    "    Returns:\n",
    "        Final predicted labels (-1 or 1) of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    final_pred = np.zeros(X.shape[0])\n",
    "    for stump, alpha in zip(stumps, alphas):\n",
    "        pred = stump_predict(X, stump)\n",
    "        final_pred += alpha * pred\n",
    "    return np.sign(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9153c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Training): 0.9736\n"
     ]
    }
   ],
   "source": [
    "# Train AdaBoost\n",
    "n_weak_learners = 10\n",
    "stumps, alphas = adaboost_train(X, y, n_weak_learners)\n",
    "\n",
    "# Predict\n",
    "y_pred = predict(X, stumps, alphas)\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(f\"Accuracy (Training): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c609c",
   "metadata": {},
   "source": [
    "## 9. Encapsulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ffd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump:\n",
    "    \"\"\"\n",
    "    A simple decision stump (one-level decision tree) used as a weak learner.\n",
    "\n",
    "    Attributes:\n",
    "        polarity: The direction of the inequality for the split.\n",
    "        feature_index: The index of the feature used for splitting.\n",
    "        threshold: The threshold value for the split.\n",
    "        alpha: The weight of this stump in the ensemble.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the decision stump with default values.\n",
    "        \"\"\"\n",
    "        self.polarity: int = 1\n",
    "        self.feature_index: Optional[int] = None\n",
    "        self.threshold: Optional[float] = None\n",
    "        self.alpha: Optional[float] = None\n",
    "\n",
    "    def predict(self, X: NDArray[np.float64]) -> NDArray[np.int8]:\n",
    "        \"\"\"\n",
    "        Predicts class labels for samples in X using the decision stump.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted class labels (+1 or -1) of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        feature_column = X[:, self.feature_index]\n",
    "        predictions = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            predictions[feature_column < self.threshold] = -1\n",
    "        else:\n",
    "            predictions[feature_column > self.threshold] = -1\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class CustomAdaBoost:\n",
    "    \"\"\"\n",
    "    AdaBoost ensemble classifier using decision stumps.\n",
    "\n",
    "    Attributes:\n",
    "        n_weak_learners: Number of weak learners (decision stumps) to use.\n",
    "        classifiers: List of fitted decision stumps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_weak_learners: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Initialise the AdaBoost classifier.\n",
    "\n",
    "        Args:\n",
    "            n_weak_learners: Number of weak learners (decision stumps) to use. Defaults to 5.\n",
    "        \"\"\"\n",
    "        self.n_weak_learners = n_weak_learners\n",
    "        self.classifiers = []\n",
    "\n",
    "    def fit(self, X: NDArray[np.float64], y: NDArray[np.int8]) -> None:\n",
    "        \"\"\"\n",
    "        Fit the AdaBoost classifier on the training data.\n",
    "\n",
    "        Args:\n",
    "            X: Training feature matrix of shape (n_samples, n_features).\n",
    "            y: Training labels (+1 or -1) of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialise weights to 1/N\n",
    "        sample_weights = np.full(n_samples, 1 / n_samples)\n",
    "        self.classifiers = []\n",
    "\n",
    "        for _ in range(self.n_weak_learners):\n",
    "            stump = DecisionStump()\n",
    "            min_error = float('inf')\n",
    "\n",
    "            # Find the best decision stump\n",
    "            for feature_index in range(n_features):\n",
    "                feature_column = X[:, feature_index]\n",
    "                thresholds = np.unique(feature_column)\n",
    "                for threshold in thresholds:\n",
    "                    polarity = 1\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    predictions[feature_column < threshold] = -1\n",
    "\n",
    "                    # Calculate weighted error\n",
    "                    error = np.sum(sample_weights[y != predictions])\n",
    "\n",
    "                    # If error > 0.5, flip polarity\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        polarity = -1\n",
    "\n",
    "                    if error < min_error:\n",
    "                        stump.polarity = polarity\n",
    "                        stump.threshold = threshold\n",
    "                        stump.feature_index = feature_index\n",
    "                        min_error = error\n",
    "\n",
    "            # Compute alpha (learner weight)\n",
    "            c = 1e-10  # to avoid division by zero\n",
    "            stump.alpha = 0.5 * np.log((1.0 - min_error + c) / (min_error + c))\n",
    "\n",
    "            # Update weights\n",
    "            predictions = stump.predict(X)\n",
    "            sample_weights *= np.exp(-stump.alpha * y * predictions)\n",
    "            sample_weights /= np.sum(sample_weights)  # Normalise\n",
    "\n",
    "            self.classifiers.append(stump)\n",
    "\n",
    "    def predict(self, X: NDArray[np.float64]) -> NDArray[np.int8]:\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X using the trained AdaBoost ensemble.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "            Predicted class labels (+1 or -1) of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        weighted_preds = [clf.alpha *\n",
    "                          clf.predict(X) for clf in self.classifiers]\n",
    "        y_pred = np.sum(weighted_preds, axis=0)\n",
    "        return np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2588525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (Custom): 0.9912\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train AdaBoost\n",
    "model = CustomAdaBoost(n_weak_learners=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.mean(y_test == y_pred)\n",
    "print(f'Test Accuracy (Custom): {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8c5cf",
   "metadata": {},
   "source": [
    "## 10. Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16009685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (SK): 0.9649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialise AdaBoost with decision stumps\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "ada = AdaBoostClassifier(estimator=base_estimator,\n",
    "                         n_estimators=10, random_state=42)\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = ada.predict(X_test)\n",
    "print(f'Test Accuracy (SK): {accuracy_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a7525",
   "metadata": {},
   "source": [
    "## 11. References\n",
    "1. Data Science Wizards. (2023). *Understanding the AdaBoost Algorithm.* <br>\n",
    "https://medium.com/@datasciencewizards/understanding-the-adaboost-algorithm-2e9344d83d9b\n",
    "\n",
    "1. GeeksforGeeks. (2025). *Boosting in Machine Learning | Boosting and AdaBoost.*<br>\n",
    "https://www.geeksforgeeks.org/machine-learning/boosting-in-machine-learning-boosting-and-adaboost/\n",
    "\n",
    "1. GeeksforGeeks. (2025). *Implementing the AdaBoost Algorithm From Scratch.*<br>\n",
    "https://www.geeksforgeeks.org/machine-learning/implementing-the-adaboost-algorithm-from-scratch/\n",
    "\n",
    "1. Patrick Loeber. (2020). *AdaBoost in Python - Machine Learning From Scratch 13 - Python Tutorial*. <br>\n",
    "https://youtu.be/wF5t4Mmv5us\n",
    "\n",
    "1. scikit-learn. (n.d.). *AdaBoostClassifier — scikit-learn API Reference.* <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "1. scikit-learn. (n.d.). *1.11.7. AdaBoost — scikit-learn User Guide.* <br>\n",
    "https://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "\n",
    "1. StatQuest with Josh Starmer. (2019). *AdaBoost, Clearly Explained*. <br>\n",
    "https://youtu.be/LsK-xG1cLYA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
