{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4836be",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a39a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict\n",
    "from numpy.typing import NDArray\n",
    "from matplotlib import pyplot as plt\n",
    "from math import cos, sin, atan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85bda8",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "A Vanila Neural Network(VNN), also known as a feedforward neural network, is the most basic form of artificial neural network and serves as athe foundation for more advanced architectures. It is called *vanilla* because it lacks the additional features or complexities found in specialised neural networks, such as Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN). \n",
    "\n",
    "\n",
    "### Layers\n",
    "A vanilla neural network is composed of one input layer, one or more hidden layer(s) and one output layer.\n",
    "\n",
    "- **Input Layer**: Receives the data input.\n",
    "- **Hidden Layer(s)**: Layer(s) where data is processed through weighted connections. These layers allow the network to learn complex patterns.\n",
    "- **Output Layer**: Procudes the final output (e.g., a class label or a predicted value).\n",
    "\n",
    "### Neurons\n",
    "Each layer consists of units called **neurons**. Each neuron receives input, processes it, and passes the output to the next layer.\n",
    "\n",
    "### Weights and Biases\n",
    "- Weights ($W$) determine the strength of connections between neurons\n",
    "- Biases ($b$) allow the model to shift the activation function\n",
    "\n",
    "### Activation Functions\n",
    "Activation functions are non-linear functions that allow models to learn complex patterns.\n",
    "\n",
    "## 2. Loading Data\n",
    "The XOR dataset is a simple dataset based on the exclusive OR (XOR) logical operation. It involves two binary inputs (either 0 or 1) and one binary output. The output is $1$ if exactly one of the inputs is $1$, and $0$ otherwise. \n",
    "\n",
    "| Input A | Input B | Output (A XOR B) |\n",
    "|---------|---------|------------------|\n",
    "|    0    |    0    |        0         |\n",
    "|    0    |    1    |        1         |\n",
    "|    1    |    0    |        1         |\n",
    "|    1    |    1    |        0         |\n",
    "\n",
    "The XOR problem is not linearly separable, thus no linear function can divide the classes in the input space. This exercise will demonstrate how to solve this problem by introducing hidden layers and non-linear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e981382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset (inputs and outputs)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceaa89e",
   "metadata": {},
   "source": [
    "## 3. Parameter Initialisation\n",
    "Each neuron in a neural network involves two main steps: a weighted sum (linear combination) of the inputs plus a bias, followed by an activation function that introduces non-linearity. A single neuron is expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "    y = \\sigma \\left( \\sum_{i=1}^{n} w_{i}x_{i} + b \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $x_{i}$: Inputs to the neuron.\n",
    "- $w_{i}$: Corresponding weights.\n",
    "- $b$: Bias.\n",
    "- $\\sigma$: Activation function (e.g., sigmoid, ReLU, tanh).\n",
    "\n",
    "\n",
    "Or, in vector notation:\n",
    "\n",
    "\\begin{align*}\n",
    "    y = \\sigma \\left(Z \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $Z = W^{T}X + b$\n",
    "- $X$: Input vector.\n",
    "- $W$: Weight vector.\n",
    "- $b$: Bias scalar.\n",
    "- $\\sigma$: Activation function.\n",
    "\n",
    "Firstly, we need to initialise $W$ and $b$ with random values for each neuron. In this example, let's take small values (e.g., Gaussian distribution)for weights to break symmetry, and 0s for biases. Note that W1 & b1 are for hidden layer, and W2 & b2 are for output layer. For example:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{cases}\n",
    "    z_1 = x_1 w_{11} + x_2 w_{12} + \\cdots + x_m w_{1m} + b_1 \\\\\n",
    "    z_2 = x_1 w_{21} + x_2 w_{22} + \\cdots + x_m w_{2m} + b_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    z_n = x_1 w_{n1} + x_2 w_{n2} + \\cdots + x_m w_{nm} + b_n \\\\\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "In a vector form:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{bmatrix}\n",
    "        z_1 \\\\\n",
    "        z_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        z_n\n",
    "        \\end{bmatrix}_{n \\times 1}\n",
    "        =\n",
    "        \\begin{bmatrix}\n",
    "        w_{11} & w_{12} & \\cdots & w_{1m} \\\\\n",
    "        w_{21} & w_{22} & \\cdots & w_{2m} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w_{n1} & w_{n2} & \\cdots & w_{nm}\n",
    "        \\end{bmatrix}_{n \\times m}\n",
    "        \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_m\n",
    "        \\end{bmatrix}_{m \\times 1}\n",
    "        +\n",
    "        \\begin{bmatrix}\n",
    "        b_1 \\\\\n",
    "        b_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        b_n\n",
    "    \\end{bmatrix}_{n \\times 1}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "With an input $X = \\left[x_1, x_2\\right]$, hidden layer is consist of:\n",
    "\n",
    "\\begin{align*}\n",
    "    h_1 = \\sigma\\left(x_1 w_{11} + x_2 w_{12} + b_{1}\\right) \\\\\n",
    "    h_2 = \\sigma\\left(x_2 w_{21} + x_2 w_{22} + b_{2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "and output layer will be:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat y = \\sigma\\left(h_1 w_{1} + h_2 w_{2} + b\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97011a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons, hidden_neurons, output_neurons = 2, 2, 1\n",
    "W1 = np.random.randn(input_neurons, hidden_neurons)\n",
    "b1 = np.zeros((1, hidden_neurons))\n",
    "W2 = np.random.randn(hidden_neurons, output_neurons)\n",
    "b2 = np.zeros((1, output_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6abca634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: \n",
      "[[ 0.92849333  0.16699119]\n",
      " [ 1.15186567 -0.26194181]]\n",
      "b1: \n",
      "[[0. 0.]]\n",
      "W2: \n",
      "[[ 1.05437164]\n",
      " [-0.47252378]]\n",
      "b2: \n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f'W1: \\n{W1}')\n",
    "print(f'b1: \\n{b1}')\n",
    "print(f'W2: \\n{W2}')\n",
    "print(f'b2: \\n{b2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae581b",
   "metadata": {},
   "source": [
    "## 4. Activation Functions\n",
    "Activation functions introduce non-linearity to neural networks, which enables them to learn complex patterns. Considering $Z = W^{T}X + b$, some popular activation functions and their derivatives are as follows:\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(Z) = \\dfrac{1}{1+e^{-Z}}\n",
    "\\end{align*}\n",
    "\n",
    "- Output range: (0, 1).\n",
    "- Smooth gradient (avoiding abrupt jumps).\n",
    "- Popular for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e6a7c",
   "metadata": {},
   "source": [
    "### Sigmoid Derivative\n",
    "\n",
    "\\begin{align*}\n",
    "\\dfrac{d}{dZ}\\sigma(Z) = \\sigma(Z) \\cdot (1 - \\sigma(Z))\n",
    "\\end{align*}\n",
    "\n",
    "The derivative function accepts the output of the sigmoid function, not raw input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8131132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return Z * (1 - Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0e32e",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU)\n",
    "ReLU is computationally efficient and it mitigates vanishing gradient, but it may cause 'Dying ReLU' problem where neurons can get stuck at 0.\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(Z) = \\text{max}(0, Z)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### ReLU Derivative\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sigma'(Z) =\n",
    "    \\begin{cases}\n",
    "    1 & \\text{if } Z > 0 \\\\\n",
    "    0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe1672a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dfdb2a",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "Leaky ReLU solves 'Dying ReLU' problem by allowing small negative outputs.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sigma(Z) =\n",
    "    \\begin{cases}\n",
    "    Z & \\text{if } Z > 0 \\\\\n",
    "    \\alpha Z & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\alpha$ is typically 0.01.\n",
    "\n",
    "### Leaky ReLU Derivative\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sigma'(Z) =\n",
    "    \\begin{cases}\n",
    "    1 & \\text{if } Z > 0 \\\\\n",
    "    \\alpha & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3a7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "\n",
    "def leaky_relu_derivative(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, 1, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f657b43e",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent (tanh)\n",
    "By applying hyperbolic tangent, output is centered at 0 (ranging from -1 to +1), giving a stronger gradient than sigmoid.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{tanh}(Z) =\n",
    "    \\dfrac{e^{Z}-e^{-Z}}{e^{Z}+e^{-Z}}\n",
    "\\end{align*}\n",
    "\n",
    "### Tanh derivative\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{d}{dZ} \\text{tanh}(Z) =\n",
    "    1 - \\text{tanh}^2(Z)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe0d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b15381",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Softmax is used for classification. \n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{softmax}(Z_{i}) =\n",
    "    \\dfrac{e^{Z_{i}}}{\\sum_{j=1}^{K}e^{Z_{j}}}\n",
    "\\end{align*}\n",
    "\n",
    "### Softmax Derivative\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial \\text{softmax}(Z_{i})}{\\partial Z_{j}} = \n",
    "    \\text{softmax}(Z_{i}) (\\delta_{ij} - \\text{softmax}(Z_{i}))\n",
    "\\end{align*}\n",
    "\n",
    "where $\\delta_{ij}$ is Kronecker delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1579612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2473be0b",
   "metadata": {},
   "source": [
    "| Function   | Best For                          | Gradient Behavior             |\n",
    "|------------|-----------------------------------|-------------------------------|\n",
    "| Sigmoid    | Output layer (binary class)       | Vanishes at extremes          |\n",
    "| ReLU       | Hidden layers (most cases)        | Simple, fast computation      |\n",
    "| Leaky ReLU | Deep networks (prevents dead neurons) | Avoids zero gradients    |\n",
    "| tanh       | Hidden layers (stronger gradient) | Vanishes less than sigmoid    |\n",
    "| Softmax    | Output layer (multi-class)        | Normalizes probabilities      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f411c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = X @ W1 + b1            # [1x2] = [1x2] @ [2x2] + [1x2]\n",
    "h = sigmoid(Z1)             # [1x2]\n",
    "Z2 = h @ W2 + b2            # [1x1] = [1x2] @ [2x1] + [1x1]\n",
    "y_hat = sigmoid(Z2)         # [1x1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5de924",
   "metadata": {},
   "source": [
    "## 5. Forward Propagation\n",
    "Forward propagation is the process where input data flows through the neural network to produce predictions.\n",
    "\n",
    "We have our hidden layer $Z_{1} = XW_{1} + b_{1}$. Applying the activation function (sigmoid for instance) to it, we get:\n",
    "\n",
    "\\begin{align*}\n",
    "    h = \\sigma(Z_{1})\n",
    "\\end{align*}\n",
    "\n",
    "Then we compute the output layer:\n",
    "\n",
    "\\begin{align*}\n",
    "    Z_{2} = hW_{2}+b_{2}\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we obtain the output:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat y = \\sigma(Z_{2})\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36c2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    # Hidden layer computation\n",
    "    Z1 = np.dot(X, W1) + b1    # Matrix multiplication + bias\n",
    "    h = sigmoid(Z1)             # Apply activation\n",
    "\n",
    "    # Output layer computation\n",
    "    Z2 = np.dot(h, W2) + b2     # Matrix multiplication + bias\n",
    "    y_hat = sigmoid(Z2)         # Apply activation\n",
    "\n",
    "    return h, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c370ab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57222231]\n",
      " [0.64466544]\n",
      " [0.62241196]\n",
      " [0.67090057]]\n"
     ]
    }
   ],
   "source": [
    "h, y_hat = forward(X, W1, b1, W2, b2)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a85ed",
   "metadata": {},
   "source": [
    "## 6. Loss Functions\n",
    "A loss function (or error function) is a mathematical formula that quantifies how far off a neural network’s predictions are from the actual target values. It provides a single scalar value representing the aggregate difference between predicted and true outputs. The central goal of training a neural network is to minimise this loss, which directly improves the predictive accuracy of the model. \n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "Mean Squared Error penalises larger errors due to its squared term.\n",
    "\\begin{align*}\n",
    "    MSE = \\dfrac{1}{N} \\sum_{i=1}^{N} (y_{i} - \\hat y_{i})^2\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $y_{i}$: True value.\n",
    "- $\\hat y_{i}$: Predicted value.\n",
    "- $N$: Number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac8e01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_hat):\n",
    "    return np.mean((y - y_hat) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c7fe84",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "Mean Absolute Error measures average magnitude of errors. It is less sensitive to outliers than MSE.\n",
    "\n",
    "\\begin{align*}\n",
    "    MAE = \\dfrac{1}{N} \\sum_{i=1}^{N} |y_{i} - \\hat y_{i}|\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f406ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y, y_hat):\n",
    "    return np.mean(np.abs(y - y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f429253",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "Root Mean Squared Error uses the same units as target variable, thus it is easier to interpret in general.\n",
    "\n",
    "\\begin{align*}\n",
    "    RMSE = \\sqrt{\\dfrac{1}{N} \\sum_{i=1}^{N} (y_{i} - \\hat y_{i})^2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc7dbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_hat):\n",
    "    return np.sqrt(np.mean((y - y_hat) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078f1b5",
   "metadata": {},
   "source": [
    "... and many more. The choice of loss function depends on our specific task (regression, classification, etc.), and understanding its properties is crucial for building effective neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a1630",
   "metadata": {},
   "source": [
    "## 7. Backward Propagation\n",
    "Backward Propagation (also called Backpropagation) is the algorithm used to train neural networks by minimising the loss function. It calculates gradients of the loss function with respect to weights and biases by propagating prediction errors backward through the network, then updates them using an optimisation algorithm such as Gradient Descent. The steps are as follows:\n",
    "\n",
    "1. Forward Pass\n",
    "- Compute the activations and outputs for each layer.\n",
    "- Use a loss function to quantify the error.\n",
    "2. Backward Pass\n",
    "- For each layer, compute gradients of the loss function with respect to activations, pre-activation values, weights and biases by applying the chain rule.\n",
    "3. Update Parameters\n",
    "- Update weights and biases using Gradient Descent.\n",
    "\n",
    "Using the forward pass notations provided:\n",
    "\n",
    "- $Z_1 = XW_1 + b_1$ (hidden layer input)\n",
    "- $h = \\sigma(Z_1)$ (hidden layer activation)\n",
    "- $Z_2 = hW_2 + b_2$ (output layer input)\n",
    "- $\\hat{y} = \\sigma(Z_2)$ (predicted output)\n",
    "\n",
    "We will derive backpropagation for Mean Squared Error loss: $C = \\frac{1}{2}(\\hat{y} - y)^2$\n",
    "\n",
    "### 1. Error Propagation\n",
    "- Output Layer Error ($\\delta_2$):\n",
    "\\begin{align*}\n",
    "    \\delta_2 = \\dfrac{\\partial C}{\\partial Z_2} = \\dfrac{\\partial C}{\\partial \\hat y} \\cdot \\dfrac{\\partial \\hat y}{\\partial Z_2} = (\\hat y - y) \\odot \\hat y(1 - \\hat y) = \\text{(Loss gradient)} \\odot \\text{(Activation derivative)}\n",
    "\\end{align*}\n",
    "\n",
    "- Hidden Layer Error ($\\delta_1$):\n",
    "\\begin{align*}\n",
    "    \\delta_1 = \\dfrac{\\partial C}{\\partial Z_1} = \\dfrac{\\partial C}{\\partial h} \\cdot \\dfrac{\\partial h}{\\partial Z_1} = (\\delta_2 W_{2}^{T}) \\odot h(1 - h) = \\text{(From output layer)} \\odot \\text{(Activation derivative)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### 2. Output Layer Gradients\n",
    "With respect to $W_{2}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial C}{\\partial W_{2}} = \\dfrac{\\partial C}{\\partial \\hat y} \\cdot \\dfrac{\\partial \\hat y}{\\partial Z_{2}} \\cdot \\dfrac{\\partial Z_{2}}{\\partial W_{2}}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\dfrac{\\partial C}{\\partial \\hat y} = \\hat y - y \\text{ (Loss Derivative)}$\n",
    "- $\\dfrac{\\partial \\hat y}{\\partial Z_{2}} = \\sigma'{Z_{2}} = \\hat y(1-\\hat y) \\text{ (Sigmoid Derivative)}$\n",
    "- $\\dfrac{\\partial Z_{2}}{\\partial W_{2}} = h \\text{ (Linear operation derivative)}$\n",
    "\n",
    "Thus:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial C}{\\partial W_{2}} = (\\hat y - y) \\cdot \\hat y(1 - \\hat y) \\cdot h\n",
    "\\end{align*}\n",
    "\n",
    "Matrix form:\n",
    "\\begin{align*}\n",
    "    \\nabla_{W_2} = h^T \\left[ (\\hat{y} - y) \\odot \\hat{y}(1 - \\hat{y}) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "With respect to $b_{2}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial C}{\\partial b_{2}} &= \\dfrac{\\partial C}{\\partial \\hat y} \\cdot \\dfrac{\\partial \\hat y}{\\partial Z_{2}} \\cdot \\dfrac{\\partial Z_{2}}{\\partial b_{2}} \\\\\n",
    "    &= (\\hat y - y) \\cdot \\hat y(1 - \\hat y) \\cdot 1\n",
    "\\end{align*}\n",
    "\n",
    "Once $\\delta_2$ is calculated, gradients can be written as:\n",
    "\\begin{align*}\n",
    "    \\nabla_{W_{2}} = h^{T} \\delta_2 \\\\\n",
    "    \\nabla_{b_{2}} = \\sum \\delta_2\n",
    "\\end{align*}\n",
    "\n",
    "### 3. Hidden Layer Gradients\n",
    "With respect to $W_{1}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial C}{\\partial W_{1}} = \\dfrac{\\partial C}{\\partial h} \\cdot \\dfrac{\\partial h}{\\partial Z_{1}} \\cdot \\dfrac{\\partial Z_{1}}{\\partial W_{1}}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\dfrac{\\partial C}{\\partial h} = \\dfrac{\\partial C}{\\partial Z_{2}} \\cdot \\dfrac{\\partial Z_{2}}{\\partial h}$\n",
    "- $\\dfrac{\\partial C}{\\partial Z_{2}} = (\\hat y - y) \\cdot \\hat y(1 - \\hat y) \\text{ (From output layer)}$\n",
    "- $\\dfrac{\\partial Z_{2}}{\\partial h} = W_{2}$\n",
    "- $\\dfrac{\\partial h}{\\partial Z_{1}} = \\sigma'(Z_{1}) = h(1-h)$\n",
    "- $\\dfrac{\\partial Z_{1}}{\\partial W_{1}} = X$\n",
    "\n",
    "Thus:\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial C}{\\partial W_{1}} = [(\\hat y - y) \\cdot \\hat y(1 - \\hat y) \\cdot W^{T}_{2}] \\odot h(1-h) \\cdot X\n",
    "\\end{align*}\n",
    "\n",
    "Matrix form:\n",
    "\\begin{align*}\n",
    "    \\nabla_{W_1} = X^T \\left[ \\left( (\\hat{y} - y) \\odot \\hat{y}(1 - \\hat{y}) \\right) W_2^T \\odot h(1 - h) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "With respect to $b_{1}$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial C}{\\partial b_{1}} &= \\dfrac{\\partial C}{\\partial h} \\cdot \\dfrac{\\partial h}{\\partial Z_{1}} \\cdot \\dfrac{\\partial Z_{1}}{\\partial b_{1}} \\\\\n",
    "    &= [(\\hat y - y) \\cdot \\hat y(1 - \\hat y) \\cdot W^{T}_{2}] \\odot h(1-h) \\cdot 1\n",
    "\\end{align*}\n",
    "\n",
    "Once $\\delta_1$ is calculated, gradients can be written as:\n",
    "\\begin{align*}\n",
    "    \\nabla_{W_{1}} = h^{T} \\delta_1 \\\\\n",
    "    \\nabla_{b_{1}} = \\sum \\delta_1\n",
    "\\end{align*}\n",
    "\n",
    "### 4. Update Rules\n",
    "For learning rate $\\eta$:\n",
    "\\begin{align*}\n",
    "    W_2 &\\leftarrow W_2 - \\eta \\cdot \\nabla_{w_{2}} \\\\\n",
    "    b_2 &\\leftarrow b_2 - \\eta \\cdot \\nabla_{b_{2}} \\\\\n",
    "    W_1 &\\leftarrow W_1 - \\eta \\cdot \\nabla_{w_{1}} \\\\\n",
    "    b_1 &\\leftarrow b_1 - \\eta \\cdot \\nabla_{b_{1}} \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f04585",
   "metadata": {},
   "source": [
    "## 8. Encapsulation\n",
    "During epochs, forward and backward propagations are repeatedly executed to minimise the loss and to keep updating weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc66098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A two-layer neural network implementation.\n",
    "\n",
    "    This neural network consists of:\n",
    "    - Input layer.\n",
    "    - One hidden layer with sigmoid activation.\n",
    "    - Output layer with sigmoid activation.\n",
    "    - Trained using backpropagation with MSE loss.\n",
    "\n",
    "    Attributes:\n",
    "        W1 (NDArray[np.float64]): Weight matrix between input and hidden layer.\n",
    "        b1 (NDArray[np.float64]): Bias vector for hidden layer.\n",
    "        W2 (NDArray[np.float64]): Weight matrix between hidden and output layer.\n",
    "        b2 (NDArray[np.float64]): Bias vector for output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialise weights and biases for the neural network.\n",
    "\n",
    "        Args:\n",
    "            input_size: Number of input features.\n",
    "            hidden_size: Number of neurons in hidden layer.\n",
    "            output_size: Number of neurons in output layer.\n",
    "\n",
    "        Initialisation:\n",
    "            Weights: Random values from standard normal distribution.\n",
    "            Biases: Initialised to zeros.\n",
    "        \"\"\"\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def sigmoid(self, Z: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Compute sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: Input tensor (linear transformation).\n",
    "\n",
    "        Returns:\n",
    "            Sigmoid activation of input, range [0, 1].\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def sigmoid_derivative(self, a: NDArray[np.float64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Compute derivative of sigmoid function.\n",
    "\n",
    "        Note: This function expects the activation output (a), not the raw input (Z).\n",
    "\n",
    "        Args:\n",
    "            a: Output from sigmoid activation (a = σ(Z)).\n",
    "\n",
    "        Returns:\n",
    "            Derivative of sigmoid: a * (1 - a).\n",
    "        \"\"\"\n",
    "\n",
    "        return a * (1 - a)\n",
    "\n",
    "    def forward(self, X: NDArray[np.int64]) -> NDArray[np.float64]:\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the network.\n",
    "\n",
    "        Computes:\n",
    "            hidden_input = X·W1 + b1\n",
    "            hidden_output = σ(hidden_input)\n",
    "            output_input = hidden_output·W2 + b2\n",
    "            prediction = σ(output_input)\n",
    "\n",
    "        Stores intermediate values for use in backpropagation.\n",
    "\n",
    "        Args:\n",
    "            X: Input data, shape (n_samples, input_size)\n",
    "\n",
    "        Returns:\n",
    "            Final predictions, shape (n_samples, output_size)\n",
    "        \"\"\"\n",
    "        self.hidden_input = np.dot(X, self.W1) + self.b1\n",
    "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
    "        self.output_input = np.dot(self.hidden_output, self.W2) + self.b2\n",
    "        self.prediction = self.sigmoid(self.output_input)\n",
    "        return self.prediction\n",
    "\n",
    "    def backward(self, X: NDArray[np.int64], y: NDArray[np.float64], lr: float = 0.01):\n",
    "        \"\"\"\n",
    "        Perform backpropagation and update weights & biases.\n",
    "\n",
    "        Computes gradients using chain rule and updates parameters:\n",
    "            1. Calculate output layer error.\n",
    "            2. Calculate hidden layer error.\n",
    "            3. Update weights and biases using gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X: Input data, shape (n_samples, input_size).\n",
    "            y: Target values, shape (n_samples, output_size).\n",
    "            lr: Learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "\n",
    "        # Output layer error\n",
    "        output_error = y - self.prediction\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.prediction)\n",
    "\n",
    "        # Hidden layer error\n",
    "        hidden_error = output_delta.dot(self.W2.T)\n",
    "        hidden_delta = hidden_error * \\\n",
    "            self.sigmoid_derivative(self.hidden_output)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.W2 += self.hidden_output.T.dot(output_delta) * lr\n",
    "        self.b2 += np.sum(output_delta, axis=0, keepdims=True) * lr\n",
    "        self.W1 += X.T.dot(hidden_delta) * lr\n",
    "        self.b1 += np.sum(hidden_delta, axis=0, keepdims=True) * lr\n",
    "\n",
    "    def mse(self, y:NDArray[np.float64], y_hat:NDArray[np.float64]) -> float:\n",
    "        \"\"\"\n",
    "        Compute mean squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y_true: Ground truth values\n",
    "            y_pred: Predicted values\n",
    "\n",
    "        Returns:\n",
    "            Mean squared error\n",
    "        \"\"\"\n",
    "\n",
    "        return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "    def train(self, X, y, epochs, lr):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            self.backward(X, y, lr)\n",
    "            loss = self.mse(y, y_pred)\n",
    "            if epoch + 1 == 1 or (epoch + 1) % (epochs//10) == 0 or epoch + 1 == epochs:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cb7b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Loss: 0.2652\n",
      "Epoch 1000/10000, Loss: 0.2265\n",
      "Epoch 2000/10000, Loss: 0.1825\n",
      "Epoch 3000/10000, Loss: 0.1335\n",
      "Epoch 4000/10000, Loss: 0.0398\n",
      "Epoch 5000/10000, Loss: 0.0149\n",
      "Epoch 6000/10000, Loss: 0.0083\n",
      "Epoch 7000/10000, Loss: 0.0056\n",
      "Epoch 8000/10000, Loss: 0.0041\n",
      "Epoch 9000/10000, Loss: 0.0033\n",
      "Epoch 10000/10000, Loss: 0.0027\n",
      "\n",
      "Final Predictions:\n",
      "Input: [0 0] -> Output: 0 (Probability: 0.044) | Expected: 0\n",
      "Input: [0 1] -> Output: 1 (Probability: 0.9507) | Expected: 1\n",
      "Input: [1 0] -> Output: 1 (Probability: 0.9509) | Expected: 1\n",
      "Input: [1 1] -> Output: 0 (Probability: 0.0629) | Expected: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialise and train network\n",
    "nn = VanillaNeuralNetwork(input_size=2, hidden_size=2, output_size=1)\n",
    "nn.train(X, y, epochs=10000, lr=0.1)\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "for i in range(len(X)):\n",
    "    probability = round(nn.forward(X[i:i+1]).item(), 4)\n",
    "    output = 0 if probability <= 0.5 else 1\n",
    "    print(\n",
    "        f\"Input: {X[i]} -> Output: {output} (Probability: {probability}) | Expected: {y[i][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
