{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4836be",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b3a39a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict\n",
    "from numpy.typing import NDArray\n",
    "from matplotlib import pyplot as plt\n",
    "from math import cos, sin, atan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85bda8",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "A Vanila Neural Network(VNN), also known as a feedforward neural network, is the most basic form of artificial neural network and serves as athe foundation for more advanced architectures. It is called *vanilla* because it lacks the additional features or complexities found in specialised neural networks, such as Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN). \n",
    "\n",
    "\n",
    "### Layers\n",
    "A vanilla neural network is composed of one input layer, one or more hidden layer(s) and one output layer.\n",
    "\n",
    "- **Input Layer**: Receives the data input.\n",
    "- **Hidden Layer(s)**: Layer(s) where data is processed through weighted connections. These layers allow the network to learn complex patterns.\n",
    "- **Output Layer**: Procudes the final output (e.g., a class label or a predicted value).\n",
    "\n",
    "### Neurons\n",
    "Each layer consists of units called **neurons**. Each neuron receives input, processes it, and passes the output to the next layer.\n",
    "\n",
    "### Weights and Biases\n",
    "- Weights ($W$) determine the strength of connections between neurons\n",
    "- Biases ($b$) allow the model to shift the activation function\n",
    "\n",
    "### Activation Functions\n",
    "Activation functions are non-linear functions that allow models to learn complex patterns.\n",
    "\n",
    "## 2. Loading Data\n",
    "The XOR dataset is a simple dataset based on the exclusive OR (XOR) logical operation. It involves two binary inputs (either 0 or 1) and one binary output. The output is $1$ if exactly one of the inputs is $1$, and $0$ otherwise. \n",
    "\n",
    "| Input A | Input B | Output (A XOR B) |\n",
    "|---------|---------|------------------|\n",
    "|    0    |    0    |        0         |\n",
    "|    0    |    1    |        1         |\n",
    "|    1    |    0    |        1         |\n",
    "|    1    |    1    |        0         |\n",
    "\n",
    "The XOR problem is not linearly separable, thus no linear function can divide the classes in the input space. This exercise will demonstrate how to solve this problem by introducing hidden layers and non-linear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e981382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset (inputs and outputs)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceaa89e",
   "metadata": {},
   "source": [
    "## 3. Parameter Initialisation\n",
    "Each neuron in a neural network involves two main steps: a weighted sum (linear combination) of the inputs plus a bias, followed by an activation function that introduces non-linearity. A single neuron is expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "    y = \\sigma \\left( \\sum_{i=1}^{n} w_{i}x_{i} + b \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $x_{i}$: Inputs to the neuron.\n",
    "- $w_{i}$: Corresponding weights.\n",
    "- $b$: Bias.\n",
    "- $\\sigma$: Activation function (e.g., sigmoid, ReLU, tanh).\n",
    "\n",
    "\n",
    "Or, in vector notation:\n",
    "\n",
    "\\begin{align*}\n",
    "    y = \\sigma \\left(Z \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $Z = W^{T}X + b$\n",
    "- $X$: Input vector.\n",
    "- $W$: Weight vector.\n",
    "- $b$: Bias scalar.\n",
    "- $\\sigma$: Activation function.\n",
    "\n",
    "Firstly, we need to initialise $W$ and $b$ with random values for each neuron. In this example, let's take small values (e.g., Gaussian distribution)for weights to break symmetry, and 0s for biases. Note that W1 & b1 are for hidden layer, and W2 & b2 are for output layer. For example:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{cases}\n",
    "    z_1 = x_1 w_{11} + x_2 w_{12} + \\cdots + x_m w_{1m} + b_1 \\\\\n",
    "    z_2 = x_1 w_{21} + x_2 w_{22} + \\cdots + x_m w_{2m} + b_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    z_n = x_1 w_{n1} + x_2 w_{n2} + \\cdots + x_m w_{nm} + b_n \\\\\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "In a vector form:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{bmatrix}\n",
    "        z_1 \\\\\n",
    "        z_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        z_n\n",
    "        \\end{bmatrix}_{n \\times 1}\n",
    "        =\n",
    "        \\begin{bmatrix}\n",
    "        w_{11} & w_{12} & \\cdots & w_{1m} \\\\\n",
    "        w_{21} & w_{22} & \\cdots & w_{2m} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w_{n1} & w_{n2} & \\cdots & w_{nm}\n",
    "        \\end{bmatrix}_{n \\times m}\n",
    "        \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_m\n",
    "        \\end{bmatrix}_{m \\times 1}\n",
    "        +\n",
    "        \\begin{bmatrix}\n",
    "        b_1 \\\\\n",
    "        b_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        b_n\n",
    "    \\end{bmatrix}_{n \\times 1}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "With an input $X = \\left[x_1, x_2\\right]$, hidden layer is consist of:\n",
    "\n",
    "\\begin{align*}\n",
    "    h_1 = \\sigma\\left(x_1 w_{11} + x_2 w_{12} + b_{1}\\right) \\\\\n",
    "    h_2 = \\sigma\\left(x_2 w_{21} + x_2 w_{22} + b_{2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "and output layer will be:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat y = \\sigma\\left(h_1 w_{1} + h_2 w_{2} + b\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "97011a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_neurons, hidden_neurons, output_neurons = 2, 2, 1\n",
    "W1 = np.random.randn(input_neurons, hidden_neurons)\n",
    "b1 = np.zeros((1, hidden_neurons))\n",
    "W2 = np.random.randn(hidden_neurons, output_neurons)\n",
    "b2 = np.zeros((1, output_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6abca634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: \n",
      "[[-0.54438272  0.11092259]\n",
      " [-1.15099358  0.37569802]]\n",
      "b1: \n",
      "[[0. 0.]]\n",
      "W2: \n",
      "[[-0.60063869]\n",
      " [-0.29169375]]\n",
      "b2: \n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f'W1: \\n{W1}')\n",
    "print(f'b1: \\n{b1}')\n",
    "print(f'W2: \\n{W2}')\n",
    "print(f'b2: \\n{b2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae581b",
   "metadata": {},
   "source": [
    "## 4. Activation Functions\n",
    "Activation functions introduce non-linearity to neural networks, which enables them to learn complex patterns. Considering $Z = W^{T}X + b$, some popular activation functions and their derivatives are as follows:\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(Z) = \\dfrac{1}{1+e^{-Z}}\n",
    "\\end{align*}\n",
    "\n",
    "- Output range: (0, 1).\n",
    "- Smooth gradient (avoiding abrupt jumps).\n",
    "- Popular for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e6a7c",
   "metadata": {},
   "source": [
    "### Sigmoid Derivative\n",
    "\n",
    "\\begin{align*}\n",
    "\\dfrac{d}{dZ}\\sigma(Z) = \\sigma(Z) \\cdot (1 - \\sigma(Z))\n",
    "\\end{align*}\n",
    "\n",
    "The derivative function accepts the output of the sigmoid function, not raw input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8131132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "        return Z * (1 - Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0e32e",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU)\n",
    "ReLU is computationally efficient and it mitigates vanishing gradient, but it may cause 'Dying ReLU' problem where neurons can get stuck at 0.\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(Z) = \\text{max}(0, Z)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### ReLU Derivative\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sigma'(Z) =\n",
    "    \\begin{cases}\n",
    "    1 & \\text{if } Z > 0 \\\\\n",
    "    0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fe1672a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dfdb2a",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "Leaky ReLU solves 'Dying ReLU' problem by allowing small negative outputs.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sigma(Z) =\n",
    "    \\begin{cases}\n",
    "    Z & \\text{if } Z > 0 \\\\\n",
    "    \\alpha Z & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\alpha$ is typically 0.01.\n",
    "\n",
    "### Leaky ReLU Derivative\n",
    "\n",
    "\\begin{align*}\n",
    "    \\sigma'(Z) =\n",
    "    \\begin{cases}\n",
    "    1 & \\text{if } Z > 0 \\\\\n",
    "    \\alpha & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0f3a7375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "def leaky_relu_derivative(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, 1, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f657b43e",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent (tanh)\n",
    "By applying hyperbolic tangent, output is centered at 0 (ranging from -1 to +1), giving a stronger gradient than sigmoid.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{tanh}(Z) =\n",
    "    \\dfrac{e^{Z}-e^{-Z}}{e^{Z}+e^{-Z}}\n",
    "\\end{align*}\n",
    "\n",
    "### Tanh derivative\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{d}{dZ} \\text{tanh}(Z) =\n",
    "    1 - \\text{tanh}^2(Z)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8fe0d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b15381",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Softmax is used for classification. \n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{softmax}(Z_{i}) =\n",
    "    \\dfrac{e^{Z_{i}}}{\\sum_{j=1}^{K}e^{Z_{j}}}\n",
    "\\end{align*}\n",
    "\n",
    "### Softmax Derivative\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dfrac{\\partial \\text{softmax}(Z_{i})}{\\partial Z_{j}} = \n",
    "    \\text{softmax}(Z_{i}) (\\delta_{ij} - \\text{softmax}(Z_{i}))\n",
    "\\end{align*}\n",
    "\n",
    "where $\\delta_{ij}$ is Kronecker delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c1579612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2473be0b",
   "metadata": {},
   "source": [
    "| Function   | Best For                          | Gradient Behavior             |\n",
    "|------------|-----------------------------------|-------------------------------|\n",
    "| Sigmoid    | Output layer (binary class)       | Vanishes at extremes          |\n",
    "| ReLU       | Hidden layers (most cases)        | Simple, fast computation      |\n",
    "| Leaky ReLU | Deep networks (prevents dead neurons) | Avoids zero gradients    |\n",
    "| tanh       | Hidden layers (stronger gradient) | Vanishes less than sigmoid    |\n",
    "| Softmax    | Output layer (multi-class)        | Normalizes probabilities      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f411c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = X @ W1 + b1            # [1x2] = [1x2] @ [2x2] + [1x2]\n",
    "h = sigmoid(Z1)             # [1x2]\n",
    "Z2 = h @ W2 + b2            # [1x1] = [1x2] @ [2x1] + [1x1]\n",
    "y_hat = sigmoid(Z2)         # [1x1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5de924",
   "metadata": {},
   "source": [
    "## 5. Forward Propagation\n",
    "Forward propagation is the process where input data flows through the neural network to produce predictions.\n",
    "\n",
    "We have our hidden layer $Z_{1} = XW_{1} + b_{1}$. Applying the activation function (sigmoid for instance) to it, we get:\n",
    "\n",
    "\\begin{align*}\n",
    "    h = \\sigma(Z_{1})\n",
    "\\end{align*}\n",
    "\n",
    "Then we compute the output layer:\n",
    "\n",
    "\\begin{align*}\n",
    "    Z_{2} = hW_{2}+b_{2}\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we obtain the output:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat y = \\sigma(Z_{2})\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d36c2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    # Hidden layer computation\n",
    "    Z1 = np.dot(X, W1) + b1    # Matrix multiplication + bias\n",
    "    h = sigmoid(Z1)             # Apply activation\n",
    "    \n",
    "    # Output layer computation\n",
    "    Z2 = np.dot(h, W2) + b2     # Matrix multiplication + bias\n",
    "    y_hat = sigmoid(Z2)         # Apply activation\n",
    "    \n",
    "    return h, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c370ab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39027267]\n",
      " [0.42134259]\n",
      " [0.40746301]\n",
      " [0.4319769 ]]\n"
     ]
    }
   ],
   "source": [
    "h, y_hat = forward(X, W1, b1, W2, b2)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66098f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
