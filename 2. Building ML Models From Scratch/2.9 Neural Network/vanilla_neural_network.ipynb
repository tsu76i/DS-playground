{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4836be",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network from Scratch\n",
    "***\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b3a39a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Dict\n",
    "from numpy.typing import NDArray\n",
    "from matplotlib import pyplot as plt\n",
    "from math import cos, sin, atan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85bda8",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "A Vanila Neural Network(VNN), also known as a feedforward neural network, is the most basic form of artificial neural network and serves as athe foundation for more advanced architectures. It is called *vanilla* because it lacks the additional features or complexities found in specialised neural networks, such as Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN). \n",
    "\n",
    "\n",
    "### Layers\n",
    "A vanilla neural network is composed of one input layer, one or more hidden layer(s) and one output layer.\n",
    "\n",
    "- **Input Layer**: Receives the data input.\n",
    "- **Hidden Layer(s)**: Layer(s) where data is processed through weighted connections. These layers allow the network to learn complex patterns.\n",
    "- **Output Layer**: Procudes the final output (e.g., a class label or a predicted value).\n",
    "\n",
    "### Neurons\n",
    "Each layer consists of units called **neurons**. Each neuron receives input, processes it, and passes the output to the next layer.\n",
    "\n",
    "### Weights and Biases\n",
    "- Weights ($W$) determine the strength of connections between neurons\n",
    "- Biases ($b$) allow the model to shift the activation function\n",
    "\n",
    "### Activation Functions\n",
    "Activation functions are non-linear functions that allow models to learn complex patterns.\n",
    "\n",
    "## 2. Loading Data\n",
    "The XOR dataset is a simple dataset based on the exclusive OR (XOR) logical operation. It involves two binary inputs (either 0 or 1) and one binary output. The output is $1$ if exactly one of the inputs is $1$, and $0$ otherwise. \n",
    "\n",
    "| Input A | Input B | Output (A XOR B) |\n",
    "|---------|---------|------------------|\n",
    "|    0    |    0    |        0         |\n",
    "|    0    |    1    |        1         |\n",
    "|    1    |    0    |        1         |\n",
    "|    1    |    1    |        0         |\n",
    "\n",
    "The XOR problem is not linearly separable, thus no linear function can divide the classes in the input space. This exercise will demonstrate how to solve this problem by introducing hidden layers and non-linear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e981382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset (inputs and outputs)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceaa89e",
   "metadata": {},
   "source": [
    "## 3. Parameter Initialisation\n",
    "Each neuron in a neural network involves two main steps: a weighted sum (linear combination) of the inputs plus a bias, followed by an activation function that introduces non-linearity. A single neuron is expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "    y = \\sigma \\left( \\sum_{i=1}^{n} w_{i}x_{i} + b \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $x_{i}$: Inputs to the neuron.\n",
    "- $w_{i}$: Corresponding weights.\n",
    "- $b$: Bias.\n",
    "- $\\sigma$: Activation function (e.g., sigmoid, ReLU, tanh).\n",
    "\n",
    "\n",
    "Or, in vector notation:\n",
    "\n",
    "\\begin{align*}\n",
    "    y = \\sigma \\left(Z \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "- $Z = W^{T}X + b$\n",
    "- $X$: Input vector.\n",
    "- $W$: Weight vector.\n",
    "- $b$: Bias scalar.\n",
    "- $\\sigma$: Activation function.\n",
    "\n",
    "Firstly, we need to initialise $W$ and $b$ with random values for each neuron. In this example, let's take small values (e.g., Gaussian distribution)for weights to break symmetry, and 0s for biases. Note that W1 & b1 are for hidden layer, and W2 & b2 are for output layer. For example:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{cases}\n",
    "    z_1 = x_1 w_{11} + x_2 w_{12} + \\cdots + x_m w_{1m} + b_1 \\\\\n",
    "    z_2 = x_1 w_{21} + x_2 w_{22} + \\cdots + x_m w_{2m} + b_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    z_n = x_1 w_{n1} + x_2 w_{n2} + \\cdots + x_m w_{nm} + b_n \\\\\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "In a vector form:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{bmatrix}\n",
    "        z_1 \\\\\n",
    "        z_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        z_n\n",
    "        \\end{bmatrix}_{n \\times 1}\n",
    "        =\n",
    "        \\begin{bmatrix}\n",
    "        w_{11} & w_{12} & \\cdots & w_{1m} \\\\\n",
    "        w_{21} & w_{22} & \\cdots & w_{2m} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w_{n1} & w_{n2} & \\cdots & w_{nm}\n",
    "        \\end{bmatrix}_{n \\times m}\n",
    "        \\begin{bmatrix}\n",
    "        x_1 \\\\\n",
    "        x_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        x_m\n",
    "        \\end{bmatrix}_{m \\times 1}\n",
    "        +\n",
    "        \\begin{bmatrix}\n",
    "        b_1 \\\\\n",
    "        b_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        b_n\n",
    "    \\end{bmatrix}_{n \\times 1}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "With an input $X = \\left[x_1, x_2\\right]$, hidden layer is consist of:\n",
    "\n",
    "\\begin{align*}\n",
    "    h_1 = \\sigma\\left(x_1 w_{11} + x_2 w_{12} + b_{1}\\right) \\\\\n",
    "    h_2 = \\sigma\\left(x_2 w_{21} + x_2 w_{22} + b_{2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "and output layer will be:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat y = \\sigma\\left(h_1 w_{1} + h_2 w_{2} + b\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "97011a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "input_neurons, hidden_neurons, output_neurons = 2, 2, 1\n",
    "W1 = np.random.randn(input_neurons, hidden_neurons)\n",
    "b1 = np.zeros((1, hidden_neurons))\n",
    "W2 = np.random.randn(hidden_neurons, output_neurons)\n",
    "b2 = np.zeros((1, output_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6abca634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: \n",
      "[[ 0.49671415 -0.1382643 ]\n",
      " [ 0.64768854  1.52302986]]\n",
      "b1: \n",
      "[[0. 0.]]\n",
      "W2: \n",
      "[[-0.23415337]\n",
      " [-0.23413696]]\n",
      "b2: \n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f'W1: \\n{W1}')\n",
    "print(f'b1: \\n{b1}')\n",
    "print(f'W2: \\n{W2}')\n",
    "print(f'b2: \\n{b2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae581b",
   "metadata": {},
   "source": [
    "## 4. Activation Functions\n",
    "Activation functions introduce non-linearity to neural networks, which enables them to learn complex patterns. Considering $Z = W^{T}X + b$, some popular activation functions and their derivatives are as follows:\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma(Z) = \\dfrac{1}{1+e^{-Z}}\n",
    "\\end{align*}\n",
    "\n",
    "- Output range: (0, 1).\n",
    "- Smooth gradient (avoiding abrupt jumps).\n",
    "- Popular for binary classification.\n",
    "\n",
    "\n",
    "### Sigmoid Derivative\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
